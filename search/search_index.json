{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is the documentation of the accompanying code for the paper:</p> <pre><code>@inproceedings{\n    goebler2025gresit,\n    title={Nonlinear Causal Discovery for Grouped Data},\n    author={Goebler, K., Windisch, T., Drton, M.},\n    booktitle={The 41st Conference on Uncertainty in Artificial Intelligence},\n    year={2025},\n}\n</code></pre> <p></p>"},{"location":"algorithms/","title":"Algorithms","text":"<p>This is a short overview of the basis algorithms implemented and benchmarked for the grouped setting. Here, we use the synthetic dataset:</p> <pre><code>from gresit.synthetic_data import GenERData\ndata_gen = GenERData(number_of_nodes=4, group_size=2)\ndata_dict, _ = data_gen.generate_data(num_samples=100)\n</code></pre>"},{"location":"algorithms/#gresit","title":"gRESIT","text":"<pre><code>from gresit.group_resit import GroupResit\nfrom gresit.torch_models import Multioutcome_MLP\nfrom gresit.independence_tests import HSIC\n\nalg = GroupResit(\n    pruning_method='murgs',\n    test=HSIC,\n    regressor=Multioutcome_MLP(),\n)\ngraph = alg.learn_graph(data_dict=data_dict)\n</code></pre> <p>See here for more details on its hyperparameters.</p>"},{"location":"algorithms/#grouppc","title":"GroupPC","text":"<pre><code>from gresit.group_pc import GroupPC\nalg = GroupPC(alpha=0.1)\ngraph = alg.learn_graph(data_dict=data_dict)\n</code></pre>"},{"location":"algorithms/#grouped-grandag","title":"Grouped GraNDAG","text":"<pre><code>from gresit.group_grandag import GroupGraNDAG\nalg = GroupGraNDAG(n_iterations=10, with_group_constraint=True)\ngraph = alg.learn_graph(data_dict)\n</code></pre>"},{"location":"algorithms/#grouped-lingam","title":"Grouped LiNGAM","text":"<pre><code>from gresit.group_lingam import GroupLiNGAM\nalg = GroupLiNGAM()\ngraph = alg.learn_graph(data_dict)\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#regressors","title":"Regressors","text":"<p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"api/#gresit.background_knowledge.BackgroundKnowledge","title":"<code>BackgroundKnowledge</code>","text":"<p>Class to store background knowledge.</p> Source code in <code>gresit/background_knowledge.py</code> <pre><code>class BackgroundKnowledge:\n    \"\"\"Class to store background knowledge.\"\"\"\n\n    def __init__(self, full_data_dict: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Initializes the BK object.\n\n        Args:\n            full_data_dict (dict[str, np.ndarray]): input data without BK.\n        \"\"\"\n        self.final_data = full_data_dict\n\n    def target_overwrite(self, target_dict: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Restrictions on target variable.\"\"\"\n        for target_name, target_values in target_dict.items():\n            self.final_data[target_name] = target_values\n\n    def target_remove_by_index(self, target_dict: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Takes array columns in target key, value pair and removes them.\n\n        Removal is done by (multiple) index array.\n\n        Args:\n            target_dict (dict[str, np.ndarray]): Dict with key equal to groups\n                and values equal to indices.\n        \"\"\"\n        for target_name, target_indices in target_dict.items():\n            self.final_data[target_name] = np.delete(\n                arr=self.final_data[target_name], obj=target_indices, axis=1\n            )\n</code></pre>"},{"location":"api/#gresit.background_knowledge.BackgroundKnowledge.__init__","title":"<code>__init__(full_data_dict)</code>","text":"<p>Initializes the BK object.</p> <p>Parameters:</p> Name Type Description Default <code>full_data_dict</code> <code>dict[str, ndarray]</code> <p>input data without BK.</p> required Source code in <code>gresit/background_knowledge.py</code> <pre><code>def __init__(self, full_data_dict: dict[str, np.ndarray]) -&gt; None:\n    \"\"\"Initializes the BK object.\n\n    Args:\n        full_data_dict (dict[str, np.ndarray]): input data without BK.\n    \"\"\"\n    self.final_data = full_data_dict\n</code></pre>"},{"location":"api/#gresit.background_knowledge.BackgroundKnowledge.target_overwrite","title":"<code>target_overwrite(target_dict)</code>","text":"<p>Restrictions on target variable.</p> Source code in <code>gresit/background_knowledge.py</code> <pre><code>def target_overwrite(self, target_dict: dict[str, np.ndarray]) -&gt; None:\n    \"\"\"Restrictions on target variable.\"\"\"\n    for target_name, target_values in target_dict.items():\n        self.final_data[target_name] = target_values\n</code></pre>"},{"location":"api/#gresit.background_knowledge.BackgroundKnowledge.target_remove_by_index","title":"<code>target_remove_by_index(target_dict)</code>","text":"<p>Takes array columns in target key, value pair and removes them.</p> <p>Removal is done by (multiple) index array.</p> <p>Parameters:</p> Name Type Description Default <code>target_dict</code> <code>dict[str, ndarray]</code> <p>Dict with key equal to groups and values equal to indices.</p> required Source code in <code>gresit/background_knowledge.py</code> <pre><code>def target_remove_by_index(self, target_dict: dict[str, np.ndarray]) -&gt; None:\n    \"\"\"Takes array columns in target key, value pair and removes them.\n\n    Removal is done by (multiple) index array.\n\n    Args:\n        target_dict (dict[str, np.ndarray]): Dict with key equal to groups\n            and values equal to indices.\n    \"\"\"\n    for target_name, target_indices in target_dict.items():\n        self.final_data[target_name] = np.delete(\n            arr=self.final_data[target_name], obj=target_indices, axis=1\n        )\n</code></pre>"},{"location":"api/#gresit.group_pc.GroupPC","title":"<code>GroupPC</code>","text":"<p>               Bases: <code>LearnAlgo</code></p> <p>This class provides tools for causal discovery.</p> <p>Particularly, in the context where data is known to follow a layered structure.</p> Source code in <code>gresit/group_pc.py</code> <pre><code>class GroupPC(LearnAlgo):\n    \"\"\"This class provides tools for causal discovery.\n\n    Particularly, in the context where data is known to follow\n    a layered structure.\n    \"\"\"\n\n    def __init__(self, alpha: float = 0.05, test: type[CItest] = FisherZVec) -&gt; None:\n        \"\"\"Initiates VectorPC.\n\n        Args:\n            alpha (float, optional): Acts as a tuning parameter. The significance\n                threshold for the conditional independence test. The smaller,\n                the sparser the resulting graph. Defaults to 0.05.\n            test (CItest, optional): Which CI test to use.\n        \"\"\"\n        self.layering: dict[str, list[str]] | None = None\n        self.alpha: float = alpha\n        self.pdag: PDAG = PDAG()\n        self.ambiguities: list[tuple[str, str, str]] = []\n        self.skel: pd.DataFrame\n        self.ci_test = test()\n\n    def learn_graph(\n        self,\n        data_dict: dict[str, np.ndarray],\n        threshold: float = 0.5,\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; PDAG:\n        \"\"\"Learns the graph from the given data.\n\n        If layering is provided it is taken to be unambiguous.\n        If layering is not Null, then the separation sets may never\n        contain variables that appear in future layers to the pair of\n        variables considered.\n\n        Args:\n            data_dict (dict | ndarray): relevant data.\n            threshold (float, optional): The majority vote threshold for deciding\n                on ambiguous collider structures. Defaults to 0.5.\n            layering (dict[str, list[str]], optional): The layering of the nodes.\n\n        Returns:\n            PDAG: Graph estimate.\n\n        \"\"\"\n        self.layering = layering\n\n        self._find_skeleton(data=data_dict, alpha=self.alpha, layering=layering)\n        self.maximally_orient(data=data_dict, alpha=self.alpha, threshold=threshold)\n        return self.pdag\n\n    def _find_skeleton(\n        self,\n        data: dict[str, np.ndarray],\n        alpha: float = 0.05,\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; None:\n        \"\"\"First Phase of layered PC (stable) algorithm.\n\n        If layering is not Null, then the separation sets may never\n        contain variables that appear in future layers to the pair of\n        variables considered.\n\n        Args:\n            data (dict[str, np.ndarray]): _description_\n            alpha (float, optional): _description_. Defaults to 0.05.\n            layering (dict[str, list[str]], optional): _description_. Defaults to None.\n        \"\"\"\n        n_features = len(data)\n        node_names = list(data.keys())\n        skeleton = pd.DataFrame(\n            (np.ones((n_features, n_features)) - np.eye(n_features)),\n            columns=node_names,\n            index=node_names,\n        )\n\n        nodes = sorted(node_names)\n        sep_set: dict[int, dict[tuple[str, str], list[str]]] = defaultdict(dict)\n        d = -1\n        node_sets = list(combinations(nodes, 2))\n        node_sets = sorted(node_sets, key=lambda x: x[0])\n\n        while self._adj_size_criterion(skeleton, d):  # until for each adj(C,i)\\{j} &lt; l\n            d += 1\n            c_stable = deepcopy(skeleton)  # needed for stable\n            if d not in sep_set:\n                sep_set[d] = {}\n            for node_i, node_j in node_sets:\n                if skeleton.loc[node_i, node_j] == 0:\n                    continue\n                adj_i = set(c_stable.index[c_stable[node_i] == 1].to_list())\n                z = adj_i - {node_j}  # adj(C, i)\\{j}\n                if len(z) &gt;= d:\n                    # |adj(C, i)\\{j}| &gt;= l\n                    z_list = sorted([*z])\n                    k = sorted([*combinations(z_list, d)], reverse=True)\n                    for subset in k:\n                        sub_z = list(subset)\n                        if layering is not None and sub_z:\n                            max_layer = self._return_max_layer(\n                                layering=layering, node_i=node_i, node_j=node_j\n                            )\n                            allowed_dict = self._remove_pairs_after_key(\n                                layering=layering, max_layer=max_layer\n                            )\n                            sub_z = self._remove_future_nodes(\n                                sep_set=sub_z,\n                                allowed_dict=allowed_dict,\n                            )\n                        if not sub_z:\n                            _, p_value = self.ci_test.test(x_data=data[node_i], y_data=data[node_j])\n                        else:\n                            # find highest layer of node_{i,j} and restrict sep\n                            # set such that no nodes can be in layers further\n                            # than said highest layer.\n                            _, p_value = self.ci_test.test(\n                                x_data=data[node_i],\n                                y_data=data[node_j],\n                                z_data=np.concatenate(\n                                    [dat for node, dat in data.items() if node in sub_z], axis=1\n                                ),\n                            )\n\n                        if p_value &gt;= alpha:\n                            skeleton.loc[node_i, node_j] = skeleton.loc[node_j, node_i] = 0\n                            sep_set[d][(node_i, node_j)] = sub_z\n                            break\n\n        self.skel = skeleton\n        self.pdag = PDAG.from_pandas_adjacency(skeleton)\n\n    def _return_max_layer(self, layering: dict[str, list[str]], node_i: str, node_j: str) -&gt; str:\n        \"\"\"Highest layer of the pair of nodes considered.\n\n        Args:\n            layering (dict[str, list[str]]): given layering\n            node_i (str): first node in the pair\n            node_j (str): second node in the pair\n\n        Returns:\n            str: key corresponding to highest layer.\n        \"\"\"\n        for key, value in reversed(layering.items()):\n            if node_i in value or node_j in value:\n                max_layer = key\n                break\n        return max_layer\n\n    def _remove_pairs_after_key(\n        self, layering: dict[str, list[str]], max_layer: str\n    ) -&gt; dict[str, list[str]]:\n        items = list(layering.items())\n        # Find the index of the target key\n        try:\n            target_index = next(i for i, (key, _) in enumerate(items) if key == max_layer)\n        except StopIteration:\n            # If the target key is not found, return the original dictionary\n            return layering\n\n        # Recreate the dictionary up to and including the target key\n        return dict(items[: target_index + 1])\n\n    def _remove_future_nodes(\n        self, sep_set: list[str], allowed_dict: dict[str, list[str]]\n    ) -&gt; list[str]:\n        \"\"\"Remove selected nodes in allowed_dict from sep_set.\n\n        Args:\n            sep_set (list[str]): sep set\n            allowed_dict (dict[str, list[str]]): allowed dict\n\n        Returns:\n            list[str]: list with future nodes removed.\n        \"\"\"\n        dict_values = {item for value in allowed_dict.values() for item in value}\n        return [item for item in sep_set if item in dict_values]\n\n    def _unshielded_triples(self, pdag: PDAG) -&gt; list[tuple[str, str, str]]:\n        \"\"\"Unshielded triples of a given PDAG.\n\n        Args:\n            pdag (PDAG): Skeleton after first phase\n\n        Returns:\n            list[tuple[str]]: List of unshielded triples of the form (i,j,k).\n        \"\"\"\n        unshielded_triples: list[tuple[str, str, str]] = []\n        for node in set(pdag.nodes):\n            neighbors = pdag.neighbors(node=node)\n            for neighbor in neighbors:\n                distant_neighbors = list(pdag.neighbors(neighbor) - {node})\n                if distant_neighbors:\n                    unshielded_triples.extend(\n                        [\n                            (node, neighbor, dist_neigh)\n                            for dist_neigh in distant_neighbors\n                            if (node, dist_neigh) not in pdag.undir_edges\n                            and (dist_neigh, node) not in pdag.undir_edges\n                        ]\n                    )\n        unique_triples: list[tuple[str, str, str]] = []\n        for tri in unshielded_triples:\n            equivalent_tri = tri[::-1]\n            if equivalent_tri not in unique_triples:\n                unique_triples.append(tri)\n        return unique_triples\n\n    def _power_set(self, a_set: set[str]) -&gt; set[frozenset[str]]:\n        \"\"\"Return power set.\n\n        Args:\n            a_set (set): Input set.\n\n        Returns:\n            frozenset[str]: all combinations of set members.\n        \"\"\"\n        length = len(a_set)\n        power_set: set[frozenset[str]] = {\n            frozenset({e for e, b in zip(a_set, f\"{i:{length}b}\") if b == \"1\"})\n            for i in range(2**length)\n        }\n        return power_set\n\n    def _get_conditioning_sets(\n        self, triples: tuple[str, str, str], pdag: PDAG\n    ) -&gt; set[frozenset[str]]:\n        \"\"\"Return conditioning set.\n\n        Args:\n            triples (tuple): triples\n            pdag (PDAG): current pdag\n\n        Returns:\n            set[tuple[str]]: conditioning set from tuples and pdag.\n        \"\"\"\n        set_a = self._power_set(pdag.neighbors(node=triples[0]))\n        set_b = self._power_set(pdag.neighbors(node=triples[2]))\n\n        unique_cond_set: set[frozenset[str]] = set_a.union(set_b)\n        return unique_cond_set\n\n    def _orient_vstructs_and_flag_amgiguities(\n        self,\n        data: dict[str, np.ndarray],\n        alpha: float = 0.05,\n        threshold: float = 0.5,\n    ) -&gt; None:\n        \"\"\"Given data orient all unshielded triples to vstrutures if possible.\n\n        Args:\n            data (dict[str, np.ndarray]): data\n            alpha (float, optional): Significance level of test. Defaults to 0.05.\n            threshold (float, optional): Threshold for ambiguity condition. Defaults to 0.5.\n\n        Raises:\n            AssertionError: If skeleton changes due to operation error is thrown.\n        \"\"\"\n        pdag = self.pdag.copy()\n        all_unshielded_triples = self._unshielded_triples(pdag=pdag)\n        flag = []\n        for triple in all_unshielded_triples:\n            node_i, node_j, node_k = triple\n            cond_sets = self._get_conditioning_sets(\n                triples=triple, pdag=pdag\n            )  # getting all adj(X_i) and adj(X_k)\n\n            conditioning_subsetter = []  # initiate candidate conditioning sets\n\n            for cond_set in list(cond_sets):\n                sep_set = list(cond_set)\n                if not sep_set:\n                    _, p_value = self.ci_test.test(x_data=data[node_i], y_data=data[node_k])\n                else:\n                    _, p_value = self.ci_test.test(\n                        x_data=data[node_i],\n                        y_data=data[node_k],\n                        z_data=np.concatenate(\n                            [dat for node, dat in data.items() if node in sep_set], axis=1\n                        ),\n                    )\n                if p_value &gt;= alpha:  # if p_value is large then test statistic is small i.e.\n                    # the Null of (cond) independence cannot be rejected.\n                    conditioning_subsetter.append(\n                        sep_set\n                    )  # all subsets that give us cond. independence\n\n            if sum([node_j in sep for sep in conditioning_subsetter]) == threshold * len(\n                conditioning_subsetter\n            ):\n                flag.append(triple)\n            elif sum([node_j in sep for sep in conditioning_subsetter]) &lt; threshold * len(\n                conditioning_subsetter\n            ):\n                pdag.undir_to_dir_edge(tail=node_i, head=node_j)\n                pdag.undir_to_dir_edge(tail=node_k, head=node_j)\n\n        original_skeleton = nx.from_pandas_adjacency(pdag.adjacency_matrix, create_using=nx.Graph)\n\n        if not self.skeleton.equals(nx.to_pandas_adjacency(original_skeleton)):\n            raise AssertionError(\n                \"Skeleton has changed. This shouldn't be possible. Check your inputs!\"\n            )\n        self.pdag = pdag\n        self.ambiguities = flag\n\n    def maximally_orient(\n        self, data: dict[str, np.ndarray], alpha: float = 0.05, threshold: float = 0.5\n    ) -&gt; None:\n        \"\"\"Given a skeleton, the following orientation steps are taken.\n\n            1. All undirected edges between layers are immediately oriented\n                according to the given layering.\n            2. Potential v-structures are ordiented.\n            3. The remaining undirected edges are oriented according to the four\n                Meek rules.\n\n        Args:\n            data (dict[str, np.ndarray]): The data.\n            alpha (float, optional): The significance\n                threshold for the conditional independence test. Defaults to 0.05.\n            threshold (float, optional): The majority vote threshold for deciding\n                on ambiguous collder structures. Defaults to 0.5.\n        \"\"\"\n        # orient immediately according to layering if present\n        if self.layering is not None:\n            self._orient_between_layers()\n        # Orient v-structures\n        self._orient_vstructs_and_flag_amgiguities(data=data, alpha=alpha, threshold=threshold)\n        # Apply Meek Rules\n        self._orient_according_to_meek_rules()\n\n    def _adj_size_criterion(self, skel: pd.DataFrame, ell: int) -&gt; bool:\n        r\"\"\"Check if |adj(C, X_i) \\\\ {X_j}| &gt;= l for every pair of adjacent vertices in C.\n\n        Args:\n            skel (pd.DataFrame): Skeleton C\n            ell (int): size of separating sets\n\n        Returns:\n            bool: True if size of adjacency set is larger or equal l and False else.\n        \"\"\"\n        assert skel.shape[0] == skel.shape[1]\n        columns = skel.columns\n        columns = sorted(columns)\n        k = list(combinations(columns, 2))\n        sorted(k, reverse=True)\n        node_pairs = [(x, y) for x, y in k]\n        less_l = 0\n        for node_i, node_j in node_pairs:\n            adj_i = set(skel.index[skel[node_i] != 0].tolist())\n            adj_ij = adj_i - {node_j}\n            if len(adj_ij) &lt; ell:\n                less_l += 1\n            else:\n                break\n        if less_l == len(node_pairs):\n            return False\n        else:\n            return True\n\n    def _between_edges(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Return between edges when layers are present.\n\n        Returns:\n            list[tuple]: list of edges between layers.\n        \"\"\"\n        if self.layering is None:\n            raise ValueError(\"Layering is not provided. Cannot retrieve between edges.\")\n        no_orient = []\n        undir_edges = self.pdag.undir_edges\n        for edge in undir_edges:\n            i, j = edge\n            for cell_nodes in self.layering.values():\n                if set([i, j]).issubset(set(cell_nodes)):\n                    no_orient.append(edge)\n                    break\n        return list(set(undir_edges).difference(set(no_orient)))\n\n    def _orient_between_layers(self) -&gt; None:\n        \"\"\"Orients edges between layers.\"\"\"\n        if self.layering is None:\n            raise ValueError(\"Layering is not provided. Cannot orient between edges.\")\n        to_orient = self._between_edges()\n\n        flat_mapper: list[str] = []\n        for nodelist in self.layering.values():\n            flat_mapper.extend(nodelist)\n\n        final_edge_list = []\n        for edge_to_orient in to_orient:\n            sorted_edge = sorted(edge_to_orient, key=flat_mapper.index)\n            final_edge_list.append(tuple(sorted_edge))\n\n        for tail, head in final_edge_list:\n            self.pdag.undir_to_dir_edge(tail=tail, head=head)\n\n    def _orient_according_to_meek_rules(self) -&gt; None:\n        \"\"\"Orient edges according to Meek rules.\"\"\"\n        cpdag = self.pdag.copy()\n        cpdag = rule_1(pdag=cpdag)\n        cpdag = rule_2(pdag=cpdag)\n        cpdag = rule_3(pdag=cpdag)\n        cpdag = rule_4(pdag=cpdag)\n        self.pdag = cpdag\n\n    @property\n    def skeleton(self) -&gt; pd.DataFrame:\n        \"\"\"Represent the underlying skeleton as adjacency matrix.\n\n        Returns:\n            pd.DataFrame: Adjacency matrix of the skeleton.\n        \"\"\"\n        undir_graph = nx.from_pandas_adjacency(self.pdag.adjacency_matrix, create_using=nx.Graph)\n        return nx.to_pandas_adjacency(undir_graph)\n\n    @property\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Represent the underlying learned PDAG as adjacency matrix.\n\n        Returns:\n            pd.DataFrame: Adjacency matrix of the PDAG.\n        \"\"\"\n        amat = self.pdag.adjacency_matrix.values\n        cpdag_amat = amat.copy()\n        upper_triangle_indices = np.triu_indices_from(amat, k=1)\n        mask = (amat[upper_triangle_indices] == 1) &amp; (amat[upper_triangle_indices[::-1]] == 1)\n\n        # Set these entries to 2 in both (i, j) and (j, i) locations\n        cpdag_amat[upper_triangle_indices[0][mask], upper_triangle_indices[1][mask]] = 2\n        cpdag_amat[upper_triangle_indices[1][mask], upper_triangle_indices[0][mask]] = 2\n\n        amat_names = self.pdag.adjacency_matrix.columns\n\n        return pd.DataFrame(cpdag_amat, columns=amat_names, index=amat_names)\n\n    @property\n    def causal_order(self) -&gt; list[str] | None:\n        \"\"\"Returns causal order if PDAG is in fact a DAG.\n\n        Else it will return None.\n\n        Returns:\n            list[str] | None: causal order if appropriate.\n        \"\"\"\n        ordering = None\n        if self.pdag.dir_edges and not self.pdag.undir_edges:\n            dag = DAG(nodes=self.pdag.nodes, edges=self.pdag.dir_edges)\n            ordering = dag.causal_order\n        return ordering\n</code></pre>"},{"location":"api/#gresit.group_pc.GroupPC.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Represent the underlying learned PDAG as adjacency matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Adjacency matrix of the PDAG.</p>"},{"location":"api/#gresit.group_pc.GroupPC.causal_order","title":"<code>causal_order</code>  <code>property</code>","text":"<p>Returns causal order if PDAG is in fact a DAG.</p> <p>Else it will return None.</p> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>list[str] | None: causal order if appropriate.</p>"},{"location":"api/#gresit.group_pc.GroupPC.skeleton","title":"<code>skeleton</code>  <code>property</code>","text":"<p>Represent the underlying skeleton as adjacency matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Adjacency matrix of the skeleton.</p>"},{"location":"api/#gresit.group_pc.GroupPC.__init__","title":"<code>__init__(alpha=0.05, test=FisherZVec)</code>","text":"<p>Initiates VectorPC.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Acts as a tuning parameter. The significance threshold for the conditional independence test. The smaller, the sparser the resulting graph. Defaults to 0.05.</p> <code>0.05</code> <code>test</code> <code>CItest</code> <p>Which CI test to use.</p> <code>FisherZVec</code> Source code in <code>gresit/group_pc.py</code> <pre><code>def __init__(self, alpha: float = 0.05, test: type[CItest] = FisherZVec) -&gt; None:\n    \"\"\"Initiates VectorPC.\n\n    Args:\n        alpha (float, optional): Acts as a tuning parameter. The significance\n            threshold for the conditional independence test. The smaller,\n            the sparser the resulting graph. Defaults to 0.05.\n        test (CItest, optional): Which CI test to use.\n    \"\"\"\n    self.layering: dict[str, list[str]] | None = None\n    self.alpha: float = alpha\n    self.pdag: PDAG = PDAG()\n    self.ambiguities: list[tuple[str, str, str]] = []\n    self.skel: pd.DataFrame\n    self.ci_test = test()\n</code></pre>"},{"location":"api/#gresit.group_pc.GroupPC.learn_graph","title":"<code>learn_graph(data_dict, threshold=0.5, layering=None)</code>","text":"<p>Learns the graph from the given data.</p> <p>If layering is provided it is taken to be unambiguous. If layering is not Null, then the separation sets may never contain variables that appear in future layers to the pair of variables considered.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict | ndarray</code> <p>relevant data.</p> required <code>threshold</code> <code>float</code> <p>The majority vote threshold for deciding on ambiguous collider structures. Defaults to 0.5.</p> <code>0.5</code> <code>layering</code> <code>dict[str, list[str]]</code> <p>The layering of the nodes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>Graph estimate.</p> Source code in <code>gresit/group_pc.py</code> <pre><code>def learn_graph(\n    self,\n    data_dict: dict[str, np.ndarray],\n    threshold: float = 0.5,\n    layering: dict[str, list[str]] | None = None,\n) -&gt; PDAG:\n    \"\"\"Learns the graph from the given data.\n\n    If layering is provided it is taken to be unambiguous.\n    If layering is not Null, then the separation sets may never\n    contain variables that appear in future layers to the pair of\n    variables considered.\n\n    Args:\n        data_dict (dict | ndarray): relevant data.\n        threshold (float, optional): The majority vote threshold for deciding\n            on ambiguous collider structures. Defaults to 0.5.\n        layering (dict[str, list[str]], optional): The layering of the nodes.\n\n    Returns:\n        PDAG: Graph estimate.\n\n    \"\"\"\n    self.layering = layering\n\n    self._find_skeleton(data=data_dict, alpha=self.alpha, layering=layering)\n    self.maximally_orient(data=data_dict, alpha=self.alpha, threshold=threshold)\n    return self.pdag\n</code></pre>"},{"location":"api/#gresit.group_pc.GroupPC.maximally_orient","title":"<code>maximally_orient(data, alpha=0.05, threshold=0.5)</code>","text":"<p>Given a skeleton, the following orientation steps are taken.</p> <pre><code>1. All undirected edges between layers are immediately oriented\n    according to the given layering.\n2. Potential v-structures are ordiented.\n3. The remaining undirected edges are oriented according to the four\n    Meek rules.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, ndarray]</code> <p>The data.</p> required <code>alpha</code> <code>float</code> <p>The significance threshold for the conditional independence test. Defaults to 0.05.</p> <code>0.05</code> <code>threshold</code> <code>float</code> <p>The majority vote threshold for deciding on ambiguous collder structures. Defaults to 0.5.</p> <code>0.5</code> Source code in <code>gresit/group_pc.py</code> <pre><code>def maximally_orient(\n    self, data: dict[str, np.ndarray], alpha: float = 0.05, threshold: float = 0.5\n) -&gt; None:\n    \"\"\"Given a skeleton, the following orientation steps are taken.\n\n        1. All undirected edges between layers are immediately oriented\n            according to the given layering.\n        2. Potential v-structures are ordiented.\n        3. The remaining undirected edges are oriented according to the four\n            Meek rules.\n\n    Args:\n        data (dict[str, np.ndarray]): The data.\n        alpha (float, optional): The significance\n            threshold for the conditional independence test. Defaults to 0.05.\n        threshold (float, optional): The majority vote threshold for deciding\n            on ambiguous collder structures. Defaults to 0.5.\n    \"\"\"\n    # orient immediately according to layering if present\n    if self.layering is not None:\n        self._orient_between_layers()\n    # Orient v-structures\n    self._orient_vstructs_and_flag_amgiguities(data=data, alpha=alpha, threshold=threshold)\n    # Apply Meek Rules\n    self._orient_according_to_meek_rules()\n</code></pre>"},{"location":"api/#gresit.group_pc.MicroPC","title":"<code>MicroPC</code>","text":"<p>               Bases: <code>LearnAlgo</code></p> <p>Standard PC stable on micro nodes aggregated after the fact.</p> Source code in <code>gresit/group_pc.py</code> <pre><code>class MicroPC(LearnAlgo):\n    \"\"\"Standard PC stable on micro nodes aggregated after the fact.\"\"\"\n\n    def __init__(self, alpha: float = 0.05) -&gt; None:\n        \"\"\"Inits the object.\n\n        Args:\n            alpha (float, optional): Significance level of the test. Defaults to 0.05.\n        \"\"\"\n        self.alpha = alpha\n        self.graph: DAG | PDAG\n\n    def _causallearn2amat(self, causal_learn_graph: np.ndarray) -&gt; np.ndarray:\n        amat = np.zeros(causal_learn_graph.shape)\n        for col in range(causal_learn_graph.shape[1]):\n            for row in range(causal_learn_graph.shape[0]):\n                if causal_learn_graph[row, col] == -1 and causal_learn_graph[col, row] == 1:\n                    amat[row, col] = 1\n                if causal_learn_graph[row, col] == -1 and causal_learn_graph[col, row] == -1:\n                    amat[row, col] = amat[col, row] = 1\n                if causal_learn_graph[row, col] == 1 and causal_learn_graph[col, row] == 1:\n                    amat[row, col] = amat[col, row] = 1\n        return amat\n\n    def learn_graph(self, data_dict: dict[str, np.ndarray], *args: Any, **kwargs: Any) -&gt; GRAPH:\n        \"\"\"Learn graph.\n\n        Args:\n            data_dict (_type_): _description_\n            *args (Any): additional args.\n            **kwargs (Any): additional kwargs.\n\n        Returns:\n            PDAG: _description_\n        \"\"\"\n        micro_data = np.concatenate([d_data for d_data in data_dict.values()], axis=1)\n        micro_pc = pc(\n            data=micro_data, alpha=self.alpha, indep_test=\"fisherz\", uc_rule=1, show_progress=True\n        )\n        micro_amat = self._causallearn2amat(causal_learn_graph=micro_pc.G.graph)\n        Q = _make_group_mapping(data_dict=data_dict)\n        interim_group_adjacency_matrix = Q @ micro_amat @ Q.T\n        np.fill_diagonal(interim_group_adjacency_matrix, 0)\n        group_adjacency_matrix = (interim_group_adjacency_matrix &gt; 0).astype(int)\n\n        group_graph: DAG | PDAG\n        if not np.any((group_adjacency_matrix == 1) &amp; (group_adjacency_matrix.T == 1)):\n            group_graph = DAG.from_pandas_adjacency(\n                pd.DataFrame(\n                    group_adjacency_matrix, columns=data_dict.keys(), index=data_dict.keys()\n                )\n            )\n        else:\n            group_graph = PDAG.from_pandas_adjacency(\n                pd.DataFrame(\n                    group_adjacency_matrix, columns=data_dict.keys(), index=data_dict.keys()\n                )\n            )\n\n        self.graph = group_graph\n        return group_graph\n\n    @property\n    def causal_order(self) -&gt; list[str] | None:\n        \"\"\"Causal order.\"\"\"\n        if isinstance(self.graph, DAG):\n            return self.graph.causal_order\n        else:\n            return None\n\n    @property\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Adjacency matrix.\"\"\"\n        return self.graph.adjacency_matrix\n</code></pre>"},{"location":"api/#gresit.group_pc.MicroPC.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Adjacency matrix.</p>"},{"location":"api/#gresit.group_pc.MicroPC.causal_order","title":"<code>causal_order</code>  <code>property</code>","text":"<p>Causal order.</p>"},{"location":"api/#gresit.group_pc.MicroPC.__init__","title":"<code>__init__(alpha=0.05)</code>","text":"<p>Inits the object.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Significance level of the test. Defaults to 0.05.</p> <code>0.05</code> Source code in <code>gresit/group_pc.py</code> <pre><code>def __init__(self, alpha: float = 0.05) -&gt; None:\n    \"\"\"Inits the object.\n\n    Args:\n        alpha (float, optional): Significance level of the test. Defaults to 0.05.\n    \"\"\"\n    self.alpha = alpha\n    self.graph: DAG | PDAG\n</code></pre>"},{"location":"api/#gresit.group_pc.MicroPC.learn_graph","title":"<code>learn_graph(data_dict, *args, **kwargs)</code>","text":"<p>Learn graph.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>_type_</code> <p>description</p> required <code>*args</code> <code>Any</code> <p>additional args.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>additional kwargs.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PDAG</code> <code>GRAPH</code> <p>description</p> Source code in <code>gresit/group_pc.py</code> <pre><code>def learn_graph(self, data_dict: dict[str, np.ndarray], *args: Any, **kwargs: Any) -&gt; GRAPH:\n    \"\"\"Learn graph.\n\n    Args:\n        data_dict (_type_): _description_\n        *args (Any): additional args.\n        **kwargs (Any): additional kwargs.\n\n    Returns:\n        PDAG: _description_\n    \"\"\"\n    micro_data = np.concatenate([d_data for d_data in data_dict.values()], axis=1)\n    micro_pc = pc(\n        data=micro_data, alpha=self.alpha, indep_test=\"fisherz\", uc_rule=1, show_progress=True\n    )\n    micro_amat = self._causallearn2amat(causal_learn_graph=micro_pc.G.graph)\n    Q = _make_group_mapping(data_dict=data_dict)\n    interim_group_adjacency_matrix = Q @ micro_amat @ Q.T\n    np.fill_diagonal(interim_group_adjacency_matrix, 0)\n    group_adjacency_matrix = (interim_group_adjacency_matrix &gt; 0).astype(int)\n\n    group_graph: DAG | PDAG\n    if not np.any((group_adjacency_matrix == 1) &amp; (group_adjacency_matrix.T == 1)):\n        group_graph = DAG.from_pandas_adjacency(\n            pd.DataFrame(\n                group_adjacency_matrix, columns=data_dict.keys(), index=data_dict.keys()\n            )\n        )\n    else:\n        group_graph = PDAG.from_pandas_adjacency(\n            pd.DataFrame(\n                group_adjacency_matrix, columns=data_dict.keys(), index=data_dict.keys()\n            )\n        )\n\n    self.graph = group_graph\n    return group_graph\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit","title":"<code>GroupResit</code>","text":"<p>               Bases: <code>LearnAlgo</code></p> <p>A class representing the groupResit algorithm.</p> <p>This algorithm is used to learn a DAG based on vector/group valued ANMs.</p> Source code in <code>gresit/group_resit.py</code> <pre><code>class GroupResit(LearnAlgo):\n    \"\"\"A class representing the groupResit algorithm.\n\n    This algorithm is used to learn a DAG based on vector/group valued ANMs.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: MultiRegressor,\n        test: type[Itest],\n        alpha: float = 0.01,\n        pruning_method: str = \"murgs\",\n        test_size: float = 0.2,\n        local_regression_method: str = \"kernel\",\n    ) -&gt; None:\n        \"\"\"Initialize the GroupResit object.\n\n        Args:\n            regressor (MultivariateRegressor): A regressor object.\n            test (IndependenceTest): An independence test object.\n            alpha (float): Alpha\n            pruning_method (str): The pruning method\n            test_size (float): Relative size of test-dataset, 0 means no test data\n            local_regression_method (str): Type of local linear smoother to use. Options are\n                `loess`, `kernel`, soon to be implemented `spline`. Defaults to `kernel`.\n        \"\"\"\n        self.regressor = regressor\n        self.independence_test = test()\n        self.local_regression_method = local_regression_method\n        self._pa: dict[str, list[str]] = {}\n        self._pa_history: dict[str, dict[float, list[str]]] = {}\n        self.alpha_level: float\n        self.alpha = alpha\n        self.pruning_method = pruning_method\n        self.test_size = test_size\n        self.DAG: DAG\n        self.layering: dict[str, list[str]] = {}\n        super().__init__()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Repr method.\n\n        Returns:\n            str: Description of the object.\n        \"\"\"\n        return f\"GroupResit(regressor={self.regressor}, independence_test={self.independence_test})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Str method.\n\n        Returns:\n            str: Human-readable description of the object.\n        \"\"\"\n        method_description = {\n            \"Regression method: \": self.regressor.__class__.__name__,\n            \"Independece test:\": self.independence_test.__class__.__name__,\n            \"Inferred causal order: \": \"Yes\" if self._causal_order else \"Not yet\",\n        }\n        s = \"\"\n        for info, info_text in method_description.items():\n            s += f\"{info:&lt;14}{info_text:&gt;5}\\n\"\n\n        return s\n\n    def _get_causal_order(\n        self,\n        data_dict: dict[str, np.ndarray],\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; None:\n        \"\"\"Get causal order of the groups respecting the current layering.\n\n        Args:\n            data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n                group name and values to the corresponding data.\n            layering (dict[str, list[str]]): A dictionary of layering information. Keys correspond\n                to the layer and values to the variable names within each layer.\n        \"\"\"\n        if layering is None:\n            layering = {\"L\": list(data_dict.keys())}\n\n        self.layering = layering\n        pa: dict[str, list[str]] = {}\n        pi: list[str] = []\n        data_to_delete_from = data_dict.copy()\n\n        indices = np.arange(data_dict[list(data_dict.keys())[0]].shape[0])\n        if self.test_size &gt; 0:\n            idx_train, idx_test = train_test_split(\n                indices,\n                test_size=self.test_size,\n                random_state=2024,\n            )\n        else:\n            idx_test = indices\n            idx_train = indices\n\n        self._idx_test = idx_test\n\n        for _, vars in reversed(layering.items()):\n            if pa:  # add previous layer nodes to current layer parents\n                for _, pre_layer_parents in pa.items():\n                    pre_layer_parents.extend(vars)\n\n            within_layer_order = vars.copy()\n            for _ in vars:\n                if len(within_layer_order) == 1:  # in each layer if there's only one node left,\n                    # this must be the first in the causal ordering\n                    pa[within_layer_order[0]] = []\n                    pi.insert(0, within_layer_order[0])\n                    del data_to_delete_from[within_layer_order[0]]\n                    continue\n\n                test_stats: list[np.float64] = []\n                for var in within_layer_order:\n                    Y = data_to_delete_from[var].copy()  # remove columns from data!\n                    X = np.concatenate(\n                        [_d for _group, _d in data_to_delete_from.items() if _group != var],\n                        axis=1,\n                    )\n\n                    X_train = self._standardize(X[idx_train])\n                    Y_train = self._standardize(Y[idx_train])\n\n                    X_test = (X[idx_test] - X[idx_train].mean(axis=0)) / (X[idx_train].std(axis=0))\n                    Y_test = (Y[idx_test] - Y[idx_train].mean(axis=0)) / (Y[idx_train].std(axis=0))\n\n                    self.regressor.fit(X=X_train, Y=Y_train)\n                    Y_pred = self.regressor.predict(X_test)\n\n                    residuals = Y_test - np.squeeze(np.asarray(Y_pred))\n                    residuals = self._standardize(residuals)\n                    X_test = self._standardize(X_test)\n                    test_stat, _ = self.independence_test.test(x_data=residuals, y_data=X_test)\n                    test_stats.append(test_stat)\n\n                k = within_layer_order[np.argmin(test_stats)]  # get most independent group\n                within_layer_order.remove(k)  # remove from remaining order\n                pa[k] = within_layer_order.copy()  # add all within layer potential parents\n                pi.insert(0, k)  # prepend to causal ordering\n\n                del data_to_delete_from[k]\n\n        self._causal_order = pi\n        self._pa = pa\n\n    def _standardize(self, x: np.ndarray) -&gt; np.ndarray:\n        return (x - x.mean(axis=0)) / x.std(axis=0)\n\n    def _get_causal_order_via_dependence_loss(\n        self,\n        data_dict: dict[str, np.ndarray],\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; None:\n        \"\"\"Get causal order of the groups respecting the current layering.\n\n        Args:\n            data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n                group name and values to the corresponding data.\n            layering (dict[str, list[str]]): A dictionary of layering information. Keys correspond\n                to the layer and values to the variable names within each layer.\n        \"\"\"\n        if layering is None:\n            layering = {\"L\": list(data_dict.keys())}\n\n        self.layering = layering\n        pa: dict[str, list[str]] = {}\n        pi: list[str] = []\n        data_to_delete_from = data_dict.copy()\n\n        indices = np.arange(data_dict[list(data_dict.keys())[0]].shape[0])\n        if self.test_size &gt; 0:\n            idx_train, idx_test = train_test_split(\n                indices,\n                test_size=self.test_size,\n                random_state=2024,\n            )\n        else:\n            idx_test = indices\n            idx_train = indices\n\n        self._idx_test = idx_test\n\n        for _, vars in reversed(layering.items()):\n            if pa:  # add previous layer nodes to current layer parents\n                for _, pre_layer_parents in pa.items():\n                    pre_layer_parents.extend(vars)\n\n            within_layer_order = vars.copy()\n            for _ in vars:\n                if len(within_layer_order) == 1:  # in each layer if there's only one node left,\n                    # this must be the first in the causal ordering\n                    pa[within_layer_order[0]] = []\n                    pi.insert(0, within_layer_order[0])\n                    del data_to_delete_from[within_layer_order[0]]\n                    continue\n\n                final_loss_values: list[np.float64] = []\n                for var in within_layer_order:\n                    Y = data_to_delete_from[var].copy()  # remove columns from data!\n                    X = np.concatenate(\n                        [_d for _group, _d in data_to_delete_from.items() if _group != var],\n                        axis=1,\n                    )\n                    if isinstance(self.regressor, Multioutcome_MLP):\n                        self.regressor.fit(X=X, Y=Y, idx_train=idx_train, idx_test=idx_test)\n                        final_loss_values.append(self.regressor.final_loss)\n                    else:\n                        raise ValueError(\"Independence loss only implemented in MLP.\")\n\n                k = within_layer_order[np.argmin(final_loss_values)]  # get most independent group\n                within_layer_order.remove(k)  # remove from remaining order\n                pa[k] = within_layer_order.copy()  # add all within layer potential parents\n                pi.insert(0, k)  # prepend to causal ordering\n                del data_to_delete_from[k]\n\n        self._causal_order = pi\n        self._pa = pa\n\n    def _independence_prune(\n        self,\n        data_dict: dict[str, np.ndarray],\n        alpha: float = 0.01,\n    ) -&gt; None:\n        \"\"\"Prune according to Peters et al. (2014).\n\n        Args:\n            data_dict (dict[str, np.ndarray]): Data in form of dict with groups as keys\n                and data as values.\n            alpha (float, optional): Significance level. Defaults to 0.01.\n        \"\"\"\n        # pruning\n        dat = data_dict.copy()\n        pa = self._pa\n        pi = self._causal_order\n        idx_test = self._idx_test\n\n        for k in pi:  # for each but the first node in the causal order\n            # Take every parent and check for independence\n            parents = pa[k].copy()\n            if not parents:\n                continue\n\n            for parent in parents:\n                Y = dat[k].copy()\n                # see whether parent may be removed\n                remainders = [var for var in parents if var != parent]\n                if not remainders:\n                    continue\n\n                X = np.concatenate(\n                    [dat[x] for x in remainders],\n                    axis=1,\n                )\n\n                self.regressor.fit(\n                    X=X[idx_test],\n                    Y=Y[idx_test],\n                )\n\n                y_pred = self.regressor.predict(X[idx_test])\n                residual = Y[idx_test] - np.squeeze(np.asarray(y_pred))\n                _, decision = self.independence_test.test(x_data=residual, y_data=X[idx_test])\n                if isinstance(decision, float):\n                    if decision &gt; alpha:  # if indepedence is not rejected, remove the edge\n                        pa[k].remove(parent)\n                elif isinstance(decision, str):\n                    if decision == \"Reject Null of vector independence: False\":\n                        pa[k].remove(parent)\n                else:\n                    raise ValueError(\"Test decision is neither float nor string\")\n\n        self.alpha_level = alpha\n        self._pa = pa\n\n    def _sparse_regression_pruning(\n        self, data_dict: dict[str, np.ndarray], nlambda: int = 30\n    ) -&gt; None:\n        # pruning\n        dat = data_dict.copy()\n        if self.test_size &gt; 0:\n            dat = {nodes: data for nodes, data in data_dict.items()}\n            # data[self._idx_test]\n        pa = self._pa\n        pi = self._causal_order\n        for k in pi:  # for each but the first node in the causal order\n            # Take every parent and check for independence\n            potential_parents = pa[k].copy()\n            if not potential_parents:\n                continue\n            # get Y_data and X_data\n            Y_data = dat[k]\n            # Create dict\n            X_data = {key: dat[key] for key in potential_parents if key in dat}\n            # initiate MURGS object\n            murgs = MURGS()\n            murgs.fit(\n                X_data=X_data,\n                Y_data=Y_data,\n                nlambda=nlambda,\n                precalculate_smooths=True,\n                local_regression_method=self.local_regression_method,\n            )\n            # extract zero groups\n            zero_groups = murgs.zero_groups\n            pa[k] = [parent for i, parent in enumerate(potential_parents) if not zero_groups[i]]\n            self._pa_history[k] = {\n                penalty: [\n                    parent for i, parent in enumerate(potential_parents) if not zero_groups[i]\n                ]\n                for penalty, zero_groups in murgs.zero_group_history.items()\n            }\n\n        self._pa = pa\n\n    def _dict_preprocessing(self, data_dict: dict[str, np.ndarray]) -&gt; dict[str, np.ndarray]:\n        \"\"\"Dict preprocessing dealing with univariate groups.\n\n        Args:\n            data_dict (dict[str, np.ndarray]): data dict.\n\n        Returns:\n            dict[str, np.ndarray]: data dict with axis added when univariate.\n        \"\"\"\n        for key, value in data_dict.items():\n            if value.ndim == 1:\n                data_dict[key] = value[:, np.newaxis]\n        return data_dict\n\n    def learn_graph(\n        self,\n        data_dict: dict[str, np.ndarray],\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; DAG:\n        \"\"\"Learn the causal graph.\n\n        Args:\n            data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n                group name and values to the corresponding data.\n            layering (dict[str, list[str]]): A dictionary of layering information. Keys correspond\n                to the layer and values to the variable names within each layer.\n\n        Raises:\n            NotImplementedError: _description_\n\n        Returns:\n            DAG: DAG estimate.\n        \"\"\"\n        clean_data_dict = self._dict_preprocessing(data_dict)\n        self._get_causal_order(data_dict=clean_data_dict, layering=layering)\n        if self.pruning_method == \"murgs\":\n            self._sparse_regression_pruning(data_dict=clean_data_dict)\n        elif self.pruning_method == \"independence\":\n            self._independence_prune(data_dict=clean_data_dict, alpha=self.alpha)\n        else:\n            raise NotImplementedError()\n\n        edge_list = [(parent, child) for child in self._pa for parent in self._pa[child]]\n\n        learned_DAG = DAG(nodes=self._causal_order)\n        learned_DAG.add_edges_from(edge_list)\n        self._adjacency_matrix = learned_DAG.adjacency_matrix\n\n        self.DAG = learned_DAG\n        return learned_DAG\n\n    def _insert_known_causal_order(self, pi: list[str]) -&gt; None:\n        \"\"\"Insert a known causal order.\n\n        Args:\n            pi (list[str]): A list of group names in the causal order.\n        \"\"\"\n        self._causal_order = pi\n        pa = {}\n        for node in pi:\n            index = pi.index(node)\n            pa[node] = pi[0:index]\n        self._pa = pa\n\n    def model_selection_with_known_causal_order(\n        self,\n        pi: list[str],\n        data_dict: dict[str, np.ndarray],\n        alpha: float = 0.01,\n        pruning_method: str = \"murgs\",\n    ) -&gt; None:\n        \"\"\"Given a known causal order perform model selection.\n\n        Args:\n            pi (list[str]): Causal ordering. Entries in `pi` need\n                to coincide with the keys in `data_dict`.\n            data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n                group name and values to the corresponding data.\n            alpha (float, optional): The significance level for the independence test.\n                Defaults to 0.1.\n            pruning_method (str, optional): The pruning method to use. Defaults to \"murgs\".\n                other options include `\"independence\"`\n\n        \"\"\"\n        self._insert_known_causal_order(pi=pi)\n        if not hasattr(self, \"_idx_test\"):\n            self._idx_test = np.arange(data_dict[list(data_dict.keys())[0]].shape[0])\n        if pruning_method == \"murgs\":\n            self._sparse_regression_pruning(data_dict=data_dict)\n        elif pruning_method == \"independence\":\n            self._independence_prune(data_dict=data_dict, alpha=alpha)\n        else:\n            raise NotImplementedError()\n\n        edge_list = [(parent, child) for child in self._pa for parent in self._pa[child]]\n\n        learned_DAG = DAG(nodes=self._causal_order)\n        learned_DAG.add_edges_from(edge_list)\n        self._adjacency_matrix = learned_DAG.adjacency_matrix\n        self.DAG = learned_DAG\n\n    @property\n    def causal_order(self) -&gt; list[str] | None:\n        \"\"\"Causal order.\"\"\"\n        return self._causal_order\n\n    @property\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Adjacency matrix.\"\"\"\n        return self._adjacency_matrix\n\n    def _add_layered_layout_to_graph(self, nx_graph: nx.DiGraph) -&gt; nx.DiGraph:\n        \"\"\"Add coordinate pos to nx.DiGraph.\n\n        Args:\n            nx_graph (nx.DiGraph): DAG in question.\n\n        Returns:\n            nx_graph (nx.DiGraph): DAG with attributes set.\n        \"\"\"\n        if len(self.layering) &gt; 1:\n            for layer, nodes in enumerate(self.layering.values()):\n                for node in nodes:\n                    nx_graph.nodes[node][\"layer\"] = layer\n        else:\n            for layer, nodes in enumerate(nx.topological_generations(nx_graph)):\n                for node in nodes:\n                    nx_graph.nodes[node][\"layer\"] = layer\n\n        # Plot multipartite_layout using the \"layer\" node attribute\n        pos = nx.multipartite_layout(nx_graph, subset_key=\"layer\")\n        nx.set_node_attributes(G=nx_graph, name=\"pos\", values=pos)\n        return nx_graph\n\n    def show(self, title: str = \"Group RESIT DAG\") -&gt; None:\n        \"\"\"Plot the learned DAG.\n\n        The plot is interactive,\n        hovering over the nodes reveals the node labels.\n        Colors get brighter the higher the node degree.\n\n        Args:\n            title (str, optional): Plot title. Defaults to \"Group RESIT DAG\".\n\n        Raises:\n            AssertionError: Throws error if DAG not yet learned.\n        \"\"\"\n        if not hasattr(self, \"DAG\"):\n            raise AssertionError(\"No graph to plot. Learn the graph first.\")\n\n        nx_dag = self.DAG.to_networkx()\n\n        nx_dag = self._add_layered_layout_to_graph(nx_graph=nx_dag)\n\n        edge_x = []\n        edge_y = []\n        for edge in nx_dag.edges():\n            x0, y0 = nx_dag.nodes[edge[0]][\"pos\"]\n            x1, y1 = nx_dag.nodes[edge[1]][\"pos\"]\n            edge_x.append(x0)\n            edge_x.append(x1)\n            edge_x.append(None)\n            edge_y.append(y0)\n            edge_y.append(y1)\n            edge_y.append(None)\n\n        edge_trace = go.Scatter(\n            x=edge_x,\n            y=edge_y,\n            line=dict(width=0.5, color=\"#888\"),\n            hoverinfo=\"none\",\n            mode=\"lines+markers\",\n            marker=dict(size=10, symbol=\"arrow-bar-up\", angleref=\"previous\"),\n        )\n\n        node_x = []\n        node_y = []\n        for node in nx_dag.nodes():\n            x, y = nx_dag.nodes[node][\"pos\"]\n            node_x.append(x)\n            node_y.append(y)\n\n        node_trace = go.Scatter(\n            x=node_x,\n            y=node_y,\n            mode=\"markers\",\n            hoverinfo=\"text\",\n            marker=dict(\n                showscale=True,\n                # colorscale options\n                #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n                #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n                #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n                colorscale=\"YlGnBu\",\n                reversescale=True,\n                color=[],\n                size=10,\n                colorbar=dict(thickness=15, title=\"node degree\", xanchor=\"left\", titleside=\"right\"),\n                line_width=2,\n            ),\n        )\n\n        node_adjacencies = []\n        node_text = []\n        for node, adjacencies in enumerate(nx_dag.adjacency()):\n            node_adjacencies.append(len(adjacencies[1]))\n            node_text.append(list(nx_dag.nodes)[node])\n\n        node_trace.marker.color = node_adjacencies\n        node_trace.text = node_text\n\n        fig = go.Figure(\n            data=[edge_trace, node_trace],\n            layout=go.Layout(\n                title=title,\n                # titlefont_size=16,\n                showlegend=False,\n                hovermode=\"closest\",\n                margin=dict(b=20, l=5, r=5, t=40),\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            ),\n        )\n        fig.show()\n\n    def _layer_pos(\n        self, G: nx.DiGraph, layers: dict[str, list[str]], layer_gap: float = 8.0\n    ) -&gt; dict[str | int, np.ndarray]:\n        pos = {}\n        for i, nodes in enumerate(layers.values()):\n            pos.update(\n                nx.spring_layout(\n                    G.subgraph(nodes), center=np.array([layer_gap * i, 0]), seed=42, k=50\n                )\n            )\n        return pos\n\n    def _create_edge_trace(\n        self, G: nx.DiGraph, pos: dict[str | int, np.ndarray], highlight: bool = False\n    ) -&gt; go.Scatter:\n        edge_x, edge_y = [], []\n\n        if G.edges():\n            for edge in G.edges():\n                x0, y0 = pos[edge[0]]  # Source node\n                x1, y1 = pos[edge[1]]  # Target node\n                # Store edge coordinates\n                edge_x.extend([x0, x1, None])  # Keep source unchanged\n                edge_y.extend([y0, y1, None])\n\n        # Define edge trace with customized width and color\n        edge_trace = go.Scatter(\n            x=edge_x,\n            y=edge_y,\n            line=dict(\n                width=2 if highlight else 1,  # Thicker for first graph\n                color=\"dimgray\" if highlight else \"#888\",  # Darker first graph edges\n            ),\n            hoverinfo=\"none\",\n            mode=\"lines+markers\",\n            marker=dict(size=10, symbol=\"arrow-bar-up\", angleref=\"previous\"),\n        )\n\n        return edge_trace\n\n    def _make_graph_list(self) -&gt; list[nx.DiGraph]:\n        unique_parent_history_sparser = {}\n        unique_parent_history = {}\n\n        for node, possible_parents in self._pa_history.items():\n            unique_parent_history[node] = sorted(\n                [\n                    list(x)\n                    for x in {\n                        frozenset(sublist)\n                        for sublist in [parents for parents in possible_parents.values()]\n                    }\n                    if len(x) &gt;= len(self._pa[node])\n                ],\n                key=len,\n            )\n            unique_parent_history_sparser[node] = sorted(\n                [\n                    list(x)\n                    for x in {\n                        frozenset(sublist)\n                        for sublist in [parents for parents in possible_parents.values()]\n                    }\n                    if len(x) &lt; len(self._pa[node])\n                ],\n                key=len,\n            )\n\n        max_length = max([len(penalties) for penalties in unique_parent_history.values()])\n        # Pad each list of lists\n        padded_dict = unique_parent_history.copy()\n        for node, _ in unique_parent_history.items():\n            last_element = (\n                unique_parent_history[node][-1] if unique_parent_history[node] else []\n            )  # Last element of the list\n\n            while len(unique_parent_history[node]) &lt; max_length:\n                padded_dict[node].append(last_element)\n\n        max_length_sparser = max(\n            [len(penalties) for penalties in unique_parent_history_sparser.values()]\n        )\n        # Pad each list of lists\n        padded_dict_sparser = unique_parent_history_sparser.copy()\n        for node, _ in unique_parent_history_sparser.items():\n            last_element = (\n                unique_parent_history_sparser[node][-1]\n                if unique_parent_history_sparser[node]\n                else []\n            )  # Last element of the list\n\n            while len(unique_parent_history_sparser[node]) &lt; max_length_sparser:\n                padded_dict_sparser[node].append(last_element)\n\n        combined_dict = {}\n        for key in padded_dict.keys():\n            combined_dict[key] = padded_dict_sparser[key] + padded_dict[key]\n\n        graph_list = []\n        for i in range(max_length_sparser + max_length):\n            pa = {node: parents[i] for node, parents in combined_dict.items()}\n            edge_list = [(parent, child) for child in pa for parent in pa[child]]\n            learned_DAG = nx.DiGraph()\n            learned_DAG.add_nodes_from(list(pa.keys()))\n            learned_DAG.add_edges_from(edge_list)\n            graph_list.append(learned_DAG)\n\n        return graph_list\n\n    def show_interactive(self, layer_gap: float = 8.0) -&gt; go.Figure:\n        \"\"\"Show interactive plot with slider to select sparsity level.\n\n        Args:\n            layer_gap (float, optional): gap between layers when displaying. Defaults to 8.0.\n        \"\"\"\n        # Fixed layout for consistency\n        graph_list = self._make_graph_list()\n        pos = self._layer_pos(G=graph_list[-1], layers=self.layering, layer_gap=layer_gap)\n        nodes = list(graph_list[-1].nodes)\n\n        # Extract node coordinates\n        node_x, node_y = zip(*[pos[n] for n in nodes])\n\n        node_trace = go.Scatter(\n            x=node_x,\n            y=node_y,\n            mode=\"markers\",\n            text=nodes,\n            hoverinfo=\"text\",\n            marker=dict(size=15, color=\"lightblue\", line=dict(color=\"black\", width=1)),\n        )\n\n        # Identify the \"optimal\" graph\n        optimal_graph_id = next(\n            (\n                i\n                for i, graph in enumerate(graph_list)\n                if nx.utils.graphs_equal(self.DAG.to_networkx(), graph)\n            ),\n            None,\n        )\n        if optimal_graph_id is not None:\n            # Create the optimal graph's edge trace\n            optimal_edge_trace = self._create_edge_trace(\n                G=graph_list[optimal_graph_id], pos=pos, highlight=True\n            )\n        else:\n            raise ValueError(\"Optimal graph not found in graph list.\")\n\n        # Create frames for each graph\n        frames = []\n        num_frames = len(graph_list)\n\n        for i, G in enumerate(graph_list):\n            # highlight = i == optimal_graph_id  # Highlight only the optimal graph\n            edge_trace = self._create_edge_trace(G, pos, highlight=False)\n\n            # If we have reached or surpassed the optimal graph, add its highlighted edges\n            if i &gt;= optimal_graph_id:\n                frames.append(\n                    go.Frame(\n                        data=[node_trace, edge_trace, optimal_edge_trace],  # Include optimal edges\n                        name=f\"Graph {i + 1}\",\n                    )\n                )\n            else:\n                frames.append(\n                    go.Frame(\n                        data=[node_trace, edge_trace],  # Normal edges only\n                        name=f\"Graph {i + 1}\",\n                    )\n                )\n\n        # Custom slider labels (first, optimal, and last only)\n        slider_labels = [\n            \"high\"\n            if i == 0\n            else \"optimal\"\n            if i == optimal_graph_id\n            else \"low\"\n            if i == num_frames - 1\n            else \"\"  # Empty label for intermediate frames\n            for i in range(num_frames)\n        ]\n\n        # **Ensure first graph's edges are displayed initially**\n        first_edge_trace = self._create_edge_trace(\n            graph_list[0], pos, highlight=(0 == optimal_graph_id)\n        )\n\n        fig = go.Figure(\n            data=[\n                node_trace,\n                first_edge_trace,  # Now displaying edges initially\n            ],\n            layout=go.Layout(\n                title=\"\",\n                showlegend=False,\n                hovermode=\"closest\",\n                margin=dict(l=20, r=20, t=40, b=20),\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                sliders=[\n                    {\n                        \"steps\": [\n                            {\n                                \"args\": [\n                                    [frame.name],\n                                    {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\", \"redraw\": True},\n                                ],\n                                \"label\": slider_labels[i],  # Apply custom labels\n                                \"method\": \"animate\",\n                            }\n                            for i, frame in enumerate(frames)\n                        ],\n                        \"active\": 0,\n                        \"currentvalue\": {\"prefix\": \"Penalty level: \", \"font\": {\"size\": 16}},\n                        \"pad\": {\"b\": 10, \"t\": 50},\n                    }\n                ],\n            ),\n            frames=frames,  # Ensure frames update correctly\n        )\n\n        # Show figure\n        return fig\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Adjacency matrix.</p>"},{"location":"api/#gresit.group_resit.GroupResit.causal_order","title":"<code>causal_order</code>  <code>property</code>","text":"<p>Causal order.</p>"},{"location":"api/#gresit.group_resit.GroupResit.__init__","title":"<code>__init__(regressor, test, alpha=0.01, pruning_method='murgs', test_size=0.2, local_regression_method='kernel')</code>","text":"<p>Initialize the GroupResit object.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>MultivariateRegressor</code> <p>A regressor object.</p> required <code>test</code> <code>IndependenceTest</code> <p>An independence test object.</p> required <code>alpha</code> <code>float</code> <p>Alpha</p> <code>0.01</code> <code>pruning_method</code> <code>str</code> <p>The pruning method</p> <code>'murgs'</code> <code>test_size</code> <code>float</code> <p>Relative size of test-dataset, 0 means no test data</p> <code>0.2</code> <code>local_regression_method</code> <code>str</code> <p>Type of local linear smoother to use. Options are <code>loess</code>, <code>kernel</code>, soon to be implemented <code>spline</code>. Defaults to <code>kernel</code>.</p> <code>'kernel'</code> Source code in <code>gresit/group_resit.py</code> <pre><code>def __init__(\n    self,\n    regressor: MultiRegressor,\n    test: type[Itest],\n    alpha: float = 0.01,\n    pruning_method: str = \"murgs\",\n    test_size: float = 0.2,\n    local_regression_method: str = \"kernel\",\n) -&gt; None:\n    \"\"\"Initialize the GroupResit object.\n\n    Args:\n        regressor (MultivariateRegressor): A regressor object.\n        test (IndependenceTest): An independence test object.\n        alpha (float): Alpha\n        pruning_method (str): The pruning method\n        test_size (float): Relative size of test-dataset, 0 means no test data\n        local_regression_method (str): Type of local linear smoother to use. Options are\n            `loess`, `kernel`, soon to be implemented `spline`. Defaults to `kernel`.\n    \"\"\"\n    self.regressor = regressor\n    self.independence_test = test()\n    self.local_regression_method = local_regression_method\n    self._pa: dict[str, list[str]] = {}\n    self._pa_history: dict[str, dict[float, list[str]]] = {}\n    self.alpha_level: float\n    self.alpha = alpha\n    self.pruning_method = pruning_method\n    self.test_size = test_size\n    self.DAG: DAG\n    self.layering: dict[str, list[str]] = {}\n    super().__init__()\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.__repr__","title":"<code>__repr__()</code>","text":"<p>Repr method.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Description of the object.</p> Source code in <code>gresit/group_resit.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Repr method.\n\n    Returns:\n        str: Description of the object.\n    \"\"\"\n    return f\"GroupResit(regressor={self.regressor}, independence_test={self.independence_test})\"\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.__str__","title":"<code>__str__()</code>","text":"<p>Str method.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human-readable description of the object.</p> Source code in <code>gresit/group_resit.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Str method.\n\n    Returns:\n        str: Human-readable description of the object.\n    \"\"\"\n    method_description = {\n        \"Regression method: \": self.regressor.__class__.__name__,\n        \"Independece test:\": self.independence_test.__class__.__name__,\n        \"Inferred causal order: \": \"Yes\" if self._causal_order else \"Not yet\",\n    }\n    s = \"\"\n    for info, info_text in method_description.items():\n        s += f\"{info:&lt;14}{info_text:&gt;5}\\n\"\n\n    return s\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.learn_graph","title":"<code>learn_graph(data_dict, layering=None)</code>","text":"<p>Learn the causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict[str, ndarray]</code> <p>A dictionary of np.ndarrays. Key corresponds to group name and values to the corresponding data.</p> required <code>layering</code> <code>dict[str, list[str]]</code> <p>A dictionary of layering information. Keys correspond to the layer and values to the variable names within each layer.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>DAG estimate.</p> Source code in <code>gresit/group_resit.py</code> <pre><code>def learn_graph(\n    self,\n    data_dict: dict[str, np.ndarray],\n    layering: dict[str, list[str]] | None = None,\n) -&gt; DAG:\n    \"\"\"Learn the causal graph.\n\n    Args:\n        data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n            group name and values to the corresponding data.\n        layering (dict[str, list[str]]): A dictionary of layering information. Keys correspond\n            to the layer and values to the variable names within each layer.\n\n    Raises:\n        NotImplementedError: _description_\n\n    Returns:\n        DAG: DAG estimate.\n    \"\"\"\n    clean_data_dict = self._dict_preprocessing(data_dict)\n    self._get_causal_order(data_dict=clean_data_dict, layering=layering)\n    if self.pruning_method == \"murgs\":\n        self._sparse_regression_pruning(data_dict=clean_data_dict)\n    elif self.pruning_method == \"independence\":\n        self._independence_prune(data_dict=clean_data_dict, alpha=self.alpha)\n    else:\n        raise NotImplementedError()\n\n    edge_list = [(parent, child) for child in self._pa for parent in self._pa[child]]\n\n    learned_DAG = DAG(nodes=self._causal_order)\n    learned_DAG.add_edges_from(edge_list)\n    self._adjacency_matrix = learned_DAG.adjacency_matrix\n\n    self.DAG = learned_DAG\n    return learned_DAG\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.model_selection_with_known_causal_order","title":"<code>model_selection_with_known_causal_order(pi, data_dict, alpha=0.01, pruning_method='murgs')</code>","text":"<p>Given a known causal order perform model selection.</p> <p>Parameters:</p> Name Type Description Default <code>pi</code> <code>list[str]</code> <p>Causal ordering. Entries in <code>pi</code> need to coincide with the keys in <code>data_dict</code>.</p> required <code>data_dict</code> <code>dict[str, ndarray]</code> <p>A dictionary of np.ndarrays. Key corresponds to group name and values to the corresponding data.</p> required <code>alpha</code> <code>float</code> <p>The significance level for the independence test. Defaults to 0.1.</p> <code>0.01</code> <code>pruning_method</code> <code>str</code> <p>The pruning method to use. Defaults to \"murgs\". other options include <code>\"independence\"</code></p> <code>'murgs'</code> Source code in <code>gresit/group_resit.py</code> <pre><code>def model_selection_with_known_causal_order(\n    self,\n    pi: list[str],\n    data_dict: dict[str, np.ndarray],\n    alpha: float = 0.01,\n    pruning_method: str = \"murgs\",\n) -&gt; None:\n    \"\"\"Given a known causal order perform model selection.\n\n    Args:\n        pi (list[str]): Causal ordering. Entries in `pi` need\n            to coincide with the keys in `data_dict`.\n        data_dict (dict[str, np.ndarray]): A dictionary of np.ndarrays. Key corresponds to\n            group name and values to the corresponding data.\n        alpha (float, optional): The significance level for the independence test.\n            Defaults to 0.1.\n        pruning_method (str, optional): The pruning method to use. Defaults to \"murgs\".\n            other options include `\"independence\"`\n\n    \"\"\"\n    self._insert_known_causal_order(pi=pi)\n    if not hasattr(self, \"_idx_test\"):\n        self._idx_test = np.arange(data_dict[list(data_dict.keys())[0]].shape[0])\n    if pruning_method == \"murgs\":\n        self._sparse_regression_pruning(data_dict=data_dict)\n    elif pruning_method == \"independence\":\n        self._independence_prune(data_dict=data_dict, alpha=alpha)\n    else:\n        raise NotImplementedError()\n\n    edge_list = [(parent, child) for child in self._pa for parent in self._pa[child]]\n\n    learned_DAG = DAG(nodes=self._causal_order)\n    learned_DAG.add_edges_from(edge_list)\n    self._adjacency_matrix = learned_DAG.adjacency_matrix\n    self.DAG = learned_DAG\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.show","title":"<code>show(title='Group RESIT DAG')</code>","text":"<p>Plot the learned DAG.</p> <p>The plot is interactive, hovering over the nodes reveals the node labels. Colors get brighter the higher the node degree.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Plot title. Defaults to \"Group RESIT DAG\".</p> <code>'Group RESIT DAG'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>Throws error if DAG not yet learned.</p> Source code in <code>gresit/group_resit.py</code> <pre><code>def show(self, title: str = \"Group RESIT DAG\") -&gt; None:\n    \"\"\"Plot the learned DAG.\n\n    The plot is interactive,\n    hovering over the nodes reveals the node labels.\n    Colors get brighter the higher the node degree.\n\n    Args:\n        title (str, optional): Plot title. Defaults to \"Group RESIT DAG\".\n\n    Raises:\n        AssertionError: Throws error if DAG not yet learned.\n    \"\"\"\n    if not hasattr(self, \"DAG\"):\n        raise AssertionError(\"No graph to plot. Learn the graph first.\")\n\n    nx_dag = self.DAG.to_networkx()\n\n    nx_dag = self._add_layered_layout_to_graph(nx_graph=nx_dag)\n\n    edge_x = []\n    edge_y = []\n    for edge in nx_dag.edges():\n        x0, y0 = nx_dag.nodes[edge[0]][\"pos\"]\n        x1, y1 = nx_dag.nodes[edge[1]][\"pos\"]\n        edge_x.append(x0)\n        edge_x.append(x1)\n        edge_x.append(None)\n        edge_y.append(y0)\n        edge_y.append(y1)\n        edge_y.append(None)\n\n    edge_trace = go.Scatter(\n        x=edge_x,\n        y=edge_y,\n        line=dict(width=0.5, color=\"#888\"),\n        hoverinfo=\"none\",\n        mode=\"lines+markers\",\n        marker=dict(size=10, symbol=\"arrow-bar-up\", angleref=\"previous\"),\n    )\n\n    node_x = []\n    node_y = []\n    for node in nx_dag.nodes():\n        x, y = nx_dag.nodes[node][\"pos\"]\n        node_x.append(x)\n        node_y.append(y)\n\n    node_trace = go.Scatter(\n        x=node_x,\n        y=node_y,\n        mode=\"markers\",\n        hoverinfo=\"text\",\n        marker=dict(\n            showscale=True,\n            # colorscale options\n            #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n            #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n            #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n            colorscale=\"YlGnBu\",\n            reversescale=True,\n            color=[],\n            size=10,\n            colorbar=dict(thickness=15, title=\"node degree\", xanchor=\"left\", titleside=\"right\"),\n            line_width=2,\n        ),\n    )\n\n    node_adjacencies = []\n    node_text = []\n    for node, adjacencies in enumerate(nx_dag.adjacency()):\n        node_adjacencies.append(len(adjacencies[1]))\n        node_text.append(list(nx_dag.nodes)[node])\n\n    node_trace.marker.color = node_adjacencies\n    node_trace.text = node_text\n\n    fig = go.Figure(\n        data=[edge_trace, node_trace],\n        layout=go.Layout(\n            title=title,\n            # titlefont_size=16,\n            showlegend=False,\n            hovermode=\"closest\",\n            margin=dict(b=20, l=5, r=5, t=40),\n            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        ),\n    )\n    fig.show()\n</code></pre>"},{"location":"api/#gresit.group_resit.GroupResit.show_interactive","title":"<code>show_interactive(layer_gap=8.0)</code>","text":"<p>Show interactive plot with slider to select sparsity level.</p> <p>Parameters:</p> Name Type Description Default <code>layer_gap</code> <code>float</code> <p>gap between layers when displaying. Defaults to 8.0.</p> <code>8.0</code> Source code in <code>gresit/group_resit.py</code> <pre><code>def show_interactive(self, layer_gap: float = 8.0) -&gt; go.Figure:\n    \"\"\"Show interactive plot with slider to select sparsity level.\n\n    Args:\n        layer_gap (float, optional): gap between layers when displaying. Defaults to 8.0.\n    \"\"\"\n    # Fixed layout for consistency\n    graph_list = self._make_graph_list()\n    pos = self._layer_pos(G=graph_list[-1], layers=self.layering, layer_gap=layer_gap)\n    nodes = list(graph_list[-1].nodes)\n\n    # Extract node coordinates\n    node_x, node_y = zip(*[pos[n] for n in nodes])\n\n    node_trace = go.Scatter(\n        x=node_x,\n        y=node_y,\n        mode=\"markers\",\n        text=nodes,\n        hoverinfo=\"text\",\n        marker=dict(size=15, color=\"lightblue\", line=dict(color=\"black\", width=1)),\n    )\n\n    # Identify the \"optimal\" graph\n    optimal_graph_id = next(\n        (\n            i\n            for i, graph in enumerate(graph_list)\n            if nx.utils.graphs_equal(self.DAG.to_networkx(), graph)\n        ),\n        None,\n    )\n    if optimal_graph_id is not None:\n        # Create the optimal graph's edge trace\n        optimal_edge_trace = self._create_edge_trace(\n            G=graph_list[optimal_graph_id], pos=pos, highlight=True\n        )\n    else:\n        raise ValueError(\"Optimal graph not found in graph list.\")\n\n    # Create frames for each graph\n    frames = []\n    num_frames = len(graph_list)\n\n    for i, G in enumerate(graph_list):\n        # highlight = i == optimal_graph_id  # Highlight only the optimal graph\n        edge_trace = self._create_edge_trace(G, pos, highlight=False)\n\n        # If we have reached or surpassed the optimal graph, add its highlighted edges\n        if i &gt;= optimal_graph_id:\n            frames.append(\n                go.Frame(\n                    data=[node_trace, edge_trace, optimal_edge_trace],  # Include optimal edges\n                    name=f\"Graph {i + 1}\",\n                )\n            )\n        else:\n            frames.append(\n                go.Frame(\n                    data=[node_trace, edge_trace],  # Normal edges only\n                    name=f\"Graph {i + 1}\",\n                )\n            )\n\n    # Custom slider labels (first, optimal, and last only)\n    slider_labels = [\n        \"high\"\n        if i == 0\n        else \"optimal\"\n        if i == optimal_graph_id\n        else \"low\"\n        if i == num_frames - 1\n        else \"\"  # Empty label for intermediate frames\n        for i in range(num_frames)\n    ]\n\n    # **Ensure first graph's edges are displayed initially**\n    first_edge_trace = self._create_edge_trace(\n        graph_list[0], pos, highlight=(0 == optimal_graph_id)\n    )\n\n    fig = go.Figure(\n        data=[\n            node_trace,\n            first_edge_trace,  # Now displaying edges initially\n        ],\n        layout=go.Layout(\n            title=\"\",\n            showlegend=False,\n            hovermode=\"closest\",\n            margin=dict(l=20, r=20, t=40, b=20),\n            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            sliders=[\n                {\n                    \"steps\": [\n                        {\n                            \"args\": [\n                                [frame.name],\n                                {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\", \"redraw\": True},\n                            ],\n                            \"label\": slider_labels[i],  # Apply custom labels\n                            \"method\": \"animate\",\n                        }\n                        for i, frame in enumerate(frames)\n                    ],\n                    \"active\": 0,\n                    \"currentvalue\": {\"prefix\": \"Penalty level: \", \"font\": {\"size\": 16}},\n                    \"pad\": {\"b\": 10, \"t\": 50},\n                }\n            ],\n        ),\n        frames=frames,  # Ensure frames update correctly\n    )\n\n    # Show figure\n    return fig\n</code></pre>"},{"location":"api/#gresit.torch_models.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Early stopping to conserve compute resources.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>class EarlyStopping:\n    \"\"\"Early stopping to conserve compute resources.\"\"\"\n\n    def __init__(\n        self, patience: int = 5, min_delta: float = 0.0, restore_best_weights: bool = True\n    ) -&gt; None:\n        \"\"\"Initializes the EarlyStopping class.\n\n        Args:\n            patience (int, optional): Number of epochs to wait for the validation error to improve.\n                Defaults to 5.\n            min_delta (float, optional): Minimum change that should be considered an improvement.\n                Defaults to 0.0.\n            restore_best_weights (bool, optional): Restores the weights to the values they were when\n                the validation set was best. Defaults to True.\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_model = None\n        self.best_loss: torch.Tensor | None = None\n        self.counter = 0\n        self.status = \"\"\n\n    def __call__(self, model: nn.Module, val_loss: float) -&gt; bool:\n        \"\"\"Stopping actions.\n\n        Args:\n            model (nn.Module): NN model\n            val_loss (float): Value of loss function.\n\n        Returns:\n            bool: True if training may be concluded.\n        \"\"\"\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_model = deepcopy(model.state_dict())\n        elif self.best_loss - val_loss &gt;= self.min_delta:\n            self.best_model = deepcopy(model.state_dict())\n            self.best_loss = val_loss\n            self.counter = 0\n            self.status = f\"Improvement found, counter reset to {self.counter}\"\n        else:\n            self.counter += 1\n            self.status = f\"No improvement in the last {self.counter} epochs\"\n            if self.counter &gt;= self.patience:\n                self.status = \"Early stopping triggered\"\n                if self.restore_best_weights:\n                    model.load_state_dict(self.best_model)\n                return True\n        return False\n</code></pre>"},{"location":"api/#gresit.torch_models.EarlyStopping.__call__","title":"<code>__call__(model, val_loss)</code>","text":"<p>Stopping actions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>NN model</p> required <code>val_loss</code> <code>float</code> <p>Value of loss function.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if training may be concluded.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def __call__(self, model: nn.Module, val_loss: float) -&gt; bool:\n    \"\"\"Stopping actions.\n\n    Args:\n        model (nn.Module): NN model\n        val_loss (float): Value of loss function.\n\n    Returns:\n        bool: True if training may be concluded.\n    \"\"\"\n    if self.best_loss is None:\n        self.best_loss = val_loss\n        self.best_model = deepcopy(model.state_dict())\n    elif self.best_loss - val_loss &gt;= self.min_delta:\n        self.best_model = deepcopy(model.state_dict())\n        self.best_loss = val_loss\n        self.counter = 0\n        self.status = f\"Improvement found, counter reset to {self.counter}\"\n    else:\n        self.counter += 1\n        self.status = f\"No improvement in the last {self.counter} epochs\"\n        if self.counter &gt;= self.patience:\n            self.status = \"Early stopping triggered\"\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_model)\n            return True\n    return False\n</code></pre>"},{"location":"api/#gresit.torch_models.EarlyStopping.__init__","title":"<code>__init__(patience=5, min_delta=0.0, restore_best_weights=True)</code>","text":"<p>Initializes the EarlyStopping class.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs to wait for the validation error to improve. Defaults to 5.</p> <code>5</code> <code>min_delta</code> <code>float</code> <p>Minimum change that should be considered an improvement. Defaults to 0.0.</p> <code>0.0</code> <code>restore_best_weights</code> <code>bool</code> <p>Restores the weights to the values they were when the validation set was best. Defaults to True.</p> <code>True</code> Source code in <code>gresit/torch_models.py</code> <pre><code>def __init__(\n    self, patience: int = 5, min_delta: float = 0.0, restore_best_weights: bool = True\n) -&gt; None:\n    \"\"\"Initializes the EarlyStopping class.\n\n    Args:\n        patience (int, optional): Number of epochs to wait for the validation error to improve.\n            Defaults to 5.\n        min_delta (float, optional): Minimum change that should be considered an improvement.\n            Defaults to 0.0.\n        restore_best_weights (bool, optional): Restores the weights to the values they were when\n            the validation set was best. Defaults to True.\n    \"\"\"\n    self.patience = patience\n    self.min_delta = min_delta\n    self.restore_best_weights = restore_best_weights\n    self.best_model = None\n    self.best_loss: torch.Tensor | None = None\n    self.counter = 0\n    self.status = \"\"\n</code></pre>"},{"location":"api/#gresit.torch_models.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Torch MLP with single-hidden layer and sigmoid non-linearity.</p> <p>Not too fancy but does what it is supposed to do.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>class MLP(nn.Module):  # type: ignore\n    \"\"\"Torch MLP with single-hidden layer and sigmoid non-linearity.\n\n    Not too fancy but does what it is supposed to do.\n    \"\"\"\n\n    def __init__(\n        self, input_dim: int, output_dim: int, hidden_dim: int = 100, dropout: float = 0.0\n    ) -&gt; None:\n        \"\"\"Initializes the NN.\n\n        Args:\n            input_dim (int): dim of input layer\n            output_dim (int, optional): dim of output layer.\n            hidden_dim (int, optional): dim of hidden layer. Defaults to 100.\n            dropout (float, optional): dropout probability. Defaults to 0.0.\n        \"\"\"\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.dropout = dropout\n\n        # define the hidden layer and final layers\n        self.hidden1 = nn.Linear(self.input_dim, self.hidden_dim, dtype=torch.float32)\n        self.hidden2 = nn.Linear(self.hidden_dim, self.hidden_dim, dtype=torch.float32)\n        self.final = nn.Linear(self.hidden_dim, self.output_dim, dtype=torch.float32, bias=False)\n        self.bias = nn.Parameter(\n            data=torch.Tensor([0] * self.output_dim),\n            requires_grad=False,\n        ).to(torch.float32)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward passing the model.\n\n        Args:\n            x (torch.Tensor): training data.\n\n        Returns:\n            torch.Tensor: prediction after pass through nn.\n        \"\"\"\n        x = self.hidden1(x)  # apply hidden layer\n        x = torch.tanh(x)  # apply sigmoid non-linearity\n        x = nn.Dropout(p=self.dropout)(x)\n        x = self.hidden2(x)  # apply hidden layer\n        x = nn.Dropout(p=self.dropout)(x)\n        x = torch.tanh(x)  # apply sigmoid non-linearity\n        x = nn.Dropout(p=self.dropout)(x)\n        x = self.final(x)  # apply final layer\n        x = x + self.bias  # correct for bias\n\n        return x\n\n    def update_bias(self, bias_value: torch.Tensor) -&gt; None:\n        \"\"\"Update bias due to HSIC location invariance.\n\n        Args:\n            bias_value (torch.Tensor): _description_\n\n        Raises:\n            ValueError: _description_\n        \"\"\"\n        if bias_value.shape == self.bias.shape:\n            self.bias.copy_(bias_value)\n        else:\n            raise ValueError(\"Shape mismatch\")\n</code></pre>"},{"location":"api/#gresit.torch_models.MLP.__init__","title":"<code>__init__(input_dim, output_dim, hidden_dim=100, dropout=0.0)</code>","text":"<p>Initializes the NN.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>dim of input layer</p> required <code>output_dim</code> <code>int</code> <p>dim of output layer.</p> required <code>hidden_dim</code> <code>int</code> <p>dim of hidden layer. Defaults to 100.</p> <code>100</code> <code>dropout</code> <code>float</code> <p>dropout probability. Defaults to 0.0.</p> <code>0.0</code> Source code in <code>gresit/torch_models.py</code> <pre><code>def __init__(\n    self, input_dim: int, output_dim: int, hidden_dim: int = 100, dropout: float = 0.0\n) -&gt; None:\n    \"\"\"Initializes the NN.\n\n    Args:\n        input_dim (int): dim of input layer\n        output_dim (int, optional): dim of output layer.\n        hidden_dim (int, optional): dim of hidden layer. Defaults to 100.\n        dropout (float, optional): dropout probability. Defaults to 0.0.\n    \"\"\"\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.dropout = dropout\n\n    # define the hidden layer and final layers\n    self.hidden1 = nn.Linear(self.input_dim, self.hidden_dim, dtype=torch.float32)\n    self.hidden2 = nn.Linear(self.hidden_dim, self.hidden_dim, dtype=torch.float32)\n    self.final = nn.Linear(self.hidden_dim, self.output_dim, dtype=torch.float32, bias=False)\n    self.bias = nn.Parameter(\n        data=torch.Tensor([0] * self.output_dim),\n        requires_grad=False,\n    ).to(torch.float32)\n</code></pre>"},{"location":"api/#gresit.torch_models.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward passing the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>training data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: prediction after pass through nn.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward passing the model.\n\n    Args:\n        x (torch.Tensor): training data.\n\n    Returns:\n        torch.Tensor: prediction after pass through nn.\n    \"\"\"\n    x = self.hidden1(x)  # apply hidden layer\n    x = torch.tanh(x)  # apply sigmoid non-linearity\n    x = nn.Dropout(p=self.dropout)(x)\n    x = self.hidden2(x)  # apply hidden layer\n    x = nn.Dropout(p=self.dropout)(x)\n    x = torch.tanh(x)  # apply sigmoid non-linearity\n    x = nn.Dropout(p=self.dropout)(x)\n    x = self.final(x)  # apply final layer\n    x = x + self.bias  # correct for bias\n\n    return x\n</code></pre>"},{"location":"api/#gresit.torch_models.MLP.update_bias","title":"<code>update_bias(bias_value)</code>","text":"<p>Update bias due to HSIC location invariance.</p> <p>Parameters:</p> Name Type Description Default <code>bias_value</code> <code>Tensor</code> <p>description</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def update_bias(self, bias_value: torch.Tensor) -&gt; None:\n    \"\"\"Update bias due to HSIC location invariance.\n\n    Args:\n        bias_value (torch.Tensor): _description_\n\n    Raises:\n        ValueError: _description_\n    \"\"\"\n    if bias_value.shape == self.bias.shape:\n        self.bias.copy_(bias_value)\n    else:\n        raise ValueError(\"Shape mismatch\")\n</code></pre>"},{"location":"api/#gresit.torch_models.MultioutcomeGPR","title":"<code>MultioutcomeGPR</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>MultioutcomeGPR Gaussian Process Regression class.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>class MultioutcomeGPR(MultiRegressor):\n    \"\"\"MultioutcomeGPR Gaussian Process Regression class.\"\"\"\n\n    def __init__(\n        self,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        n_epochs: int = 300,\n        patience: int = 50,\n        learning_rate: float = 0.01,\n        val_size: float = 0.2,\n        batch_size: int | None = None,\n        es: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize MLP.\n\n        Args:\n            rng (np.random.Generator, optional): _description_.\n                Defaults to np.random.default_rng(seed=2024).\n            n_epochs (int): number of times the data gets passed trough the MLP.\n                Defaults to 6.\n            patience (int, optional): Minimal number of epochs to train before early stopping\n                applies\n            learning_rate (float, optional): _description_. Defaults to 1e-3.\n            val_size (float): Relative size of the validation dataset\n            batch_size (int): Batch size.\n            es (bool, optional): Early stopping.\n        \"\"\"\n        super().__init__(rng)\n\n        self.training_info: pd.DataFrame\n        self.prediction: torch.Tensor\n        self._Y_test: np.ndarray | torch.Tensor\n        self._X_test: np.ndarray | torch.Tensor\n        self._model: nn.Module\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.val_size = val_size\n        self.es = es\n\n        has_mps = torch.backends.mps.is_built()\n        self.device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def fit(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        idx_train: np.ndarray | None = None,\n        idx_test: np.ndarray | None = None,\n    ) -&gt; None:\n        \"\"\"Fit the MLP model.\n\n        Args:\n            X (np.ndarray): Input data\n            Y (np.ndarray): Target data\n            idx_train (np.ndarray): training indices\n            idx_test (np.ndarray): test indices\n\n        Returns:\n            int: epoch at which training was stopped.\n        \"\"\"\n        if idx_train is None and idx_test is None:\n            X_train, X_val, Y_train, Y_val = self.split_and_standardize(\n                X=X,\n                Y=Y,\n                test_size=self.val_size,\n            )\n        else:\n            X_train, X_val, Y_train, Y_val = (X[idx_train], X[idx_test], Y[idx_train], Y[idx_test])\n\n        self._X_train = torch.from_numpy(X_train).float().to(self.device)\n        self._Y_train = torch.from_numpy(Y_train).float().to(self.device)\n\n        self._X_val = torch.from_numpy(X_val).float().to(self.device)\n        self._Y_val = torch.from_numpy(Y_val).float().to(self.device)\n\n        num_tasks = self._Y_train.size(1)\n\n        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n        model = MultitaskGPModel(self._X_train, self._Y_train, likelihood).to(self.device)\n        if self.es:\n            es = EarlyStopping(patience=self.patience)\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=self.learning_rate,\n        )\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n        history_val = []\n        for i in range(self.n_epochs):\n            optimizer.zero_grad()\n            output = model(self._X_train)\n            loss = -mll(output, self._Y_train)\n            loss.backward()\n            optimizer.step()\n\n            # Evaluate Model\n            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n                model.eval().to(self.device)\n                likelihood.eval().to(self.device)\n                predictions = likelihood(model(self._X_val))\n                Y_pred = predictions.mean\n                vloss = loss_mse(self._X_val, Y_pred, self._Y_val, device=self.device)\n\n                history_val.append(\n                    {\n                        \"epoch\": i,\n                        \"task\": \"regression\",\n                        \"lr\": self.learning_rate,\n                        \"loss\": vloss.detach().cpu().numpy(),\n                    }\n                )\n                if self.es:\n                    if es(model, vloss):\n                        print(f\"{es.status}\")\n                        break\n\n        self.validation_info = pd.DataFrame(history_val)\n        self.prediction = make_preds_single(model, self._X_val)\n        self._model = model\n        self._likelihood = likelihood\n        self.final_loss = es.best_loss if self.es else None\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Make predictions.\n\n        Returns:\n            np.ndarray: Predictions\n        \"\"\"\n        if X_test is None:\n            prediction = self.prediction.detach().numpy()\n        else:\n            X_torch_test = torch.from_numpy(X_test).float().to(self.device)\n            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n                pred_model = self._model.eval().to(self.device)\n                pred_likelihood = self._likelihood.eval().to(self.device)\n                predictions = pred_likelihood(pred_model(X_torch_test))\n                pred = predictions.mean\n            prediction = pred.detach().cpu().numpy()\n        return prediction\n\n    def plot_training_info(self) -&gt; None:\n        \"\"\"Plot some summary over the MSE during training.\"\"\"\n        sns.lmplot(\n            x=\"trainstep\",\n            y=\"loss\",\n            hue=\"task\",\n            lowess=True,\n            scatter_kws={\"alpha\": 0.5},\n            line_kws={\"linewidth\": 2},\n            data=self.training_info,\n        )\n\n        sns.lmplot(\n            x=\"epoch\",\n            y=\"loss\",\n            hue=\"task\",\n            lowess=True,\n            scatter_kws={\"alpha\": 0.5},\n            line_kws={\"linewidth\": 2},\n            data=self.validation_info,\n        )\n</code></pre>"},{"location":"api/#gresit.torch_models.MultioutcomeGPR.__init__","title":"<code>__init__(rng=np.random.default_rng(seed=2024), n_epochs=300, patience=50, learning_rate=0.01, val_size=0.2, batch_size=None, es=True)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>description. Defaults to np.random.default_rng(seed=2024).</p> <code>default_rng(seed=2024)</code> <code>n_epochs</code> <code>int</code> <p>number of times the data gets passed trough the MLP. Defaults to 6.</p> <code>300</code> <code>patience</code> <code>int</code> <p>Minimal number of epochs to train before early stopping applies</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>description. Defaults to 1e-3.</p> <code>0.01</code> <code>val_size</code> <code>float</code> <p>Relative size of the validation dataset</p> <code>0.2</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <code>es</code> <code>bool</code> <p>Early stopping.</p> <code>True</code> Source code in <code>gresit/torch_models.py</code> <pre><code>def __init__(\n    self,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    n_epochs: int = 300,\n    patience: int = 50,\n    learning_rate: float = 0.01,\n    val_size: float = 0.2,\n    batch_size: int | None = None,\n    es: bool = True,\n) -&gt; None:\n    \"\"\"Initialize MLP.\n\n    Args:\n        rng (np.random.Generator, optional): _description_.\n            Defaults to np.random.default_rng(seed=2024).\n        n_epochs (int): number of times the data gets passed trough the MLP.\n            Defaults to 6.\n        patience (int, optional): Minimal number of epochs to train before early stopping\n            applies\n        learning_rate (float, optional): _description_. Defaults to 1e-3.\n        val_size (float): Relative size of the validation dataset\n        batch_size (int): Batch size.\n        es (bool, optional): Early stopping.\n    \"\"\"\n    super().__init__(rng)\n\n    self.training_info: pd.DataFrame\n    self.prediction: torch.Tensor\n    self._Y_test: np.ndarray | torch.Tensor\n    self._X_test: np.ndarray | torch.Tensor\n    self._model: nn.Module\n    self.learning_rate = learning_rate\n    self.n_epochs = n_epochs\n    self.patience = patience\n    self.batch_size = batch_size\n    self.val_size = val_size\n    self.es = es\n\n    has_mps = torch.backends.mps.is_built()\n    self.device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre>"},{"location":"api/#gresit.torch_models.MultioutcomeGPR.fit","title":"<code>fit(X, Y, idx_train=None, idx_test=None)</code>","text":"<p>Fit the MLP model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data</p> required <code>Y</code> <code>ndarray</code> <p>Target data</p> required <code>idx_train</code> <code>ndarray</code> <p>training indices</p> <code>None</code> <code>idx_test</code> <code>ndarray</code> <p>test indices</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>None</code> <p>epoch at which training was stopped.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray,\n    idx_train: np.ndarray | None = None,\n    idx_test: np.ndarray | None = None,\n) -&gt; None:\n    \"\"\"Fit the MLP model.\n\n    Args:\n        X (np.ndarray): Input data\n        Y (np.ndarray): Target data\n        idx_train (np.ndarray): training indices\n        idx_test (np.ndarray): test indices\n\n    Returns:\n        int: epoch at which training was stopped.\n    \"\"\"\n    if idx_train is None and idx_test is None:\n        X_train, X_val, Y_train, Y_val = self.split_and_standardize(\n            X=X,\n            Y=Y,\n            test_size=self.val_size,\n        )\n    else:\n        X_train, X_val, Y_train, Y_val = (X[idx_train], X[idx_test], Y[idx_train], Y[idx_test])\n\n    self._X_train = torch.from_numpy(X_train).float().to(self.device)\n    self._Y_train = torch.from_numpy(Y_train).float().to(self.device)\n\n    self._X_val = torch.from_numpy(X_val).float().to(self.device)\n    self._Y_val = torch.from_numpy(Y_val).float().to(self.device)\n\n    num_tasks = self._Y_train.size(1)\n\n    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n    model = MultitaskGPModel(self._X_train, self._Y_train, likelihood).to(self.device)\n    if self.es:\n        es = EarlyStopping(patience=self.patience)\n\n    # Find optimal model hyperparameters\n    model.train()\n    likelihood.train()\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=self.learning_rate,\n    )\n\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    history_val = []\n    for i in range(self.n_epochs):\n        optimizer.zero_grad()\n        output = model(self._X_train)\n        loss = -mll(output, self._Y_train)\n        loss.backward()\n        optimizer.step()\n\n        # Evaluate Model\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            model.eval().to(self.device)\n            likelihood.eval().to(self.device)\n            predictions = likelihood(model(self._X_val))\n            Y_pred = predictions.mean\n            vloss = loss_mse(self._X_val, Y_pred, self._Y_val, device=self.device)\n\n            history_val.append(\n                {\n                    \"epoch\": i,\n                    \"task\": \"regression\",\n                    \"lr\": self.learning_rate,\n                    \"loss\": vloss.detach().cpu().numpy(),\n                }\n            )\n            if self.es:\n                if es(model, vloss):\n                    print(f\"{es.status}\")\n                    break\n\n    self.validation_info = pd.DataFrame(history_val)\n    self.prediction = make_preds_single(model, self._X_val)\n    self._model = model\n    self._likelihood = likelihood\n    self.final_loss = es.best_loss if self.es else None\n</code></pre>"},{"location":"api/#gresit.torch_models.MultioutcomeGPR.plot_training_info","title":"<code>plot_training_info()</code>","text":"<p>Plot some summary over the MSE during training.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def plot_training_info(self) -&gt; None:\n    \"\"\"Plot some summary over the MSE during training.\"\"\"\n    sns.lmplot(\n        x=\"trainstep\",\n        y=\"loss\",\n        hue=\"task\",\n        lowess=True,\n        scatter_kws={\"alpha\": 0.5},\n        line_kws={\"linewidth\": 2},\n        data=self.training_info,\n    )\n\n    sns.lmplot(\n        x=\"epoch\",\n        y=\"loss\",\n        hue=\"task\",\n        lowess=True,\n        scatter_kws={\"alpha\": 0.5},\n        line_kws={\"linewidth\": 2},\n        data=self.validation_info,\n    )\n</code></pre>"},{"location":"api/#gresit.torch_models.MultioutcomeGPR.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Make predictions.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predictions</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Make predictions.\n\n    Returns:\n        np.ndarray: Predictions\n    \"\"\"\n    if X_test is None:\n        prediction = self.prediction.detach().numpy()\n    else:\n        X_torch_test = torch.from_numpy(X_test).float().to(self.device)\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            pred_model = self._model.eval().to(self.device)\n            pred_likelihood = self._likelihood.eval().to(self.device)\n            predictions = pred_likelihood(pred_model(X_torch_test))\n            pred = predictions.mean\n        prediction = pred.detach().cpu().numpy()\n    return prediction\n</code></pre>"},{"location":"api/#gresit.torch_models.Multioutcome_MLP","title":"<code>Multioutcome_MLP</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>Fit simple MLP with one hidden layer.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>class Multioutcome_MLP(MultiRegressor):\n    \"\"\"Fit simple MLP with one hidden layer.\"\"\"\n\n    def __init__(\n        self,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        loss: str = \"mse\",\n        dropout_proba: float = 0.6,\n        n_epochs: int = 300,\n        patience: int = 50,\n        learning_rate: float = 0.01,\n        val_size: float = 0.2,\n        batch_size: int = 200,\n        es: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize MLP.\n\n        Args:\n            rng (np.random.Generator, optional): _description_.\n                Defaults to np.random.default_rng(seed=2024).\n            loss (str, optional): Standard mse loss is default.\n                Other options are `hsic` and `disco`.\n                Defaults to \"mse\".\n            dropout_proba (float, optional): _description_. Defaults to 0.6.\n            n_epochs (int): number of times the data gets passed trough the MLP.\n                Defaults to 6.\n            patience (int, optional): Minimal number of epochs to train before early stopping\n                applies\n            learning_rate (float, optional): _description_. Defaults to 1e-3.\n            val_size (float): Relative size of the validation dataset\n            batch_size (int): Batch size.\n            es (bool, optional): Early stopping. Defaults to true.\n        \"\"\"\n        super().__init__(rng)\n\n        self.training_info: pd.DataFrame\n        self.prediction: torch.Tensor\n        self.input_dim: int\n        self.output_dim: int\n        self._Y_test: np.ndarray | torch.Tensor\n        self._X_test: np.ndarray | torch.Tensor\n        self._model: nn.Module\n        self.dropout_proba = dropout_proba\n        self.learning_rate = learning_rate\n        self.n_epochs = n_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.val_size = val_size\n        self.es = es\n\n        self.loss_name = loss\n\n        has_mps = torch.backends.mps.is_built()\n        self.device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def _compute_bias(\n        self,\n        model: nn.Module,\n        y_true: torch.Tensor,\n        x: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        y_pred_interim: torch.Tensor = model(x)\n        return y_true.mean(dim=0) - y_pred_interim.mean(dim=0)\n\n    def _standardize(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return (x - x.mean(dim=0)) / x.std(dim=0)\n\n    def fit(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        idx_train: np.ndarray | None = None,\n        idx_test: np.ndarray | None = None,\n    ) -&gt; None:\n        \"\"\"Fit the MLP model.\n\n        Args:\n            X (np.ndarray): Input data\n            Y (np.ndarray): Target data\n            idx_train (np.ndarray): training indices\n            idx_test (np.ndarray): test indices\n\n        Returns:\n            int: epoch at which training was stopped.\n        \"\"\"\n        if idx_train is None and idx_test is None:\n            X_train, X_val, Y_train, Y_val = self.split_and_standardize(\n                X=X,\n                Y=Y,\n                test_size=self.val_size,\n            )\n        else:\n            X_train, X_val, Y_train, Y_val = (X[idx_train], X[idx_test], Y[idx_train], Y[idx_test])\n            X_train = (X_train - X_train.mean(axis=0)) / (X_train.std(axis=0))\n            Y_train = (Y_train - Y_train.mean(axis=0)) / (Y_train.std(axis=0))\n            X_val = (X_val - X_train.mean(axis=0)) / (X_train.std(axis=0))\n            Y_val = (Y_val - Y_train.mean(axis=0)) / (Y_train.std(axis=0))\n\n        if self.loss_name == \"hisc\":\n            loss_fn = loss_hsic\n        else:\n            loss_fn = loss_mse\n\n        self._X_train = torch.from_numpy(X_train).float().to(self.device)\n        self._Y_train = torch.from_numpy(Y_train).float().to(self.device)\n\n        self._X_val = torch.from_numpy(X_val).float().to(self.device)\n        self._Y_val = torch.from_numpy(Y_val).float().to(self.device)\n\n        model = MLP(\n            input_dim=self._X_train.shape[1],\n            output_dim=self._Y_train.shape[1],\n            dropout=self.dropout_proba,\n        ).to(self.device)\n\n        if self.es:\n            es = EarlyStopping(patience=self.patience)\n\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=self.learning_rate,\n            amsgrad=True,\n        )\n\n        dataset_train = TensorDataset(self._X_train, self._Y_train)\n        dataloader_train = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True)\n\n        history_train = []\n        history_val = []\n        trainstep = 0\n        for i in tqdm(range(self.n_epochs), ncols=100):\n            for j, (x_batch, y_batch) in enumerate(dataloader_train):\n                Y_pred = model(x_batch)\n                loss = loss_fn(\n                    self._standardize(x_batch),\n                    self._standardize(Y_pred),\n                    self._standardize(y_batch),\n                    device=self.device,\n                )\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                history_train.append(\n                    {\n                        \"epoch\": i,\n                        \"minibatch\": j,\n                        \"trainstep\": trainstep,\n                        \"task\": \"regression\",\n                        \"loss\": loss.detach().cpu().numpy(),\n                    }\n                )\n                trainstep += 1\n\n            # Evaluate Model\n            with torch.no_grad():\n                if self.loss_name != \"mse\":\n                    bias = self._compute_bias(model=model, y_true=self._Y_val, x=self._X_val)\n                    model.update_bias(bias)\n                Y_pred = model(self._X_val)\n                vloss = loss_fn(\n                    self._standardize(self._X_val),\n                    self._standardize(Y_pred),\n                    self._standardize(self._Y_val),\n                    device=self.device,\n                )\n\n                history_val.append(\n                    {\n                        \"epoch\": i,\n                        \"trainstep\": trainstep,\n                        \"task\": \"regression\",\n                        # \"lr\": lr_scheduler.get_last_lr()[0],\n                        \"lr\": self.learning_rate,\n                        \"loss\": vloss.detach().cpu().numpy(),\n                    }\n                )\n                if self.es:\n                    if es(model, vloss):\n                        print(f\"{es.status}\")\n                        break\n\n        self.training_info = pd.DataFrame(history_train)\n        self.validation_info = pd.DataFrame(history_val)\n        self.prediction = make_preds_single(model, self._X_val)\n        self._model = model\n        self.final_loss = (\n            es.best_loss.detach().cpu().numpy() if self.es and es.best_loss else np.infty\n        )\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Make predictions.\n\n        Returns:\n            np.ndarray: Predictions\n        \"\"\"\n        if X_test is None:\n            prediction = self.prediction.detach().numpy()\n        else:\n            X_torch_test = torch.from_numpy(X_test).float().to(self.device)\n            pred = make_preds_single(self._model, X_torch_test)\n            prediction = pred.detach().cpu().numpy()\n        return prediction\n\n    def plot_training_info(self) -&gt; None:\n        \"\"\"Plot some summary over the MSE during training.\"\"\"\n        sns.lmplot(\n            x=\"trainstep\",\n            y=\"loss\",\n            hue=\"task\",\n            lowess=True,\n            scatter_kws={\"alpha\": 0.5},\n            line_kws={\"linewidth\": 2},\n            data=self.training_info,\n        )\n\n        sns.lmplot(\n            x=\"epoch\",\n            y=\"loss\",\n            hue=\"task\",\n            lowess=True,\n            scatter_kws={\"alpha\": 0.5},\n            line_kws={\"linewidth\": 2},\n            data=self.validation_info,\n        )\n</code></pre>"},{"location":"api/#gresit.torch_models.Multioutcome_MLP.__init__","title":"<code>__init__(rng=np.random.default_rng(seed=2024), loss='mse', dropout_proba=0.6, n_epochs=300, patience=50, learning_rate=0.01, val_size=0.2, batch_size=200, es=True)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>description. Defaults to np.random.default_rng(seed=2024).</p> <code>default_rng(seed=2024)</code> <code>loss</code> <code>str</code> <p>Standard mse loss is default. Other options are <code>hsic</code> and <code>disco</code>. Defaults to \"mse\".</p> <code>'mse'</code> <code>dropout_proba</code> <code>float</code> <p>description. Defaults to 0.6.</p> <code>0.6</code> <code>n_epochs</code> <code>int</code> <p>number of times the data gets passed trough the MLP. Defaults to 6.</p> <code>300</code> <code>patience</code> <code>int</code> <p>Minimal number of epochs to train before early stopping applies</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>description. Defaults to 1e-3.</p> <code>0.01</code> <code>val_size</code> <code>float</code> <p>Relative size of the validation dataset</p> <code>0.2</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>200</code> <code>es</code> <code>bool</code> <p>Early stopping. Defaults to true.</p> <code>True</code> Source code in <code>gresit/torch_models.py</code> <pre><code>def __init__(\n    self,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    loss: str = \"mse\",\n    dropout_proba: float = 0.6,\n    n_epochs: int = 300,\n    patience: int = 50,\n    learning_rate: float = 0.01,\n    val_size: float = 0.2,\n    batch_size: int = 200,\n    es: bool = True,\n) -&gt; None:\n    \"\"\"Initialize MLP.\n\n    Args:\n        rng (np.random.Generator, optional): _description_.\n            Defaults to np.random.default_rng(seed=2024).\n        loss (str, optional): Standard mse loss is default.\n            Other options are `hsic` and `disco`.\n            Defaults to \"mse\".\n        dropout_proba (float, optional): _description_. Defaults to 0.6.\n        n_epochs (int): number of times the data gets passed trough the MLP.\n            Defaults to 6.\n        patience (int, optional): Minimal number of epochs to train before early stopping\n            applies\n        learning_rate (float, optional): _description_. Defaults to 1e-3.\n        val_size (float): Relative size of the validation dataset\n        batch_size (int): Batch size.\n        es (bool, optional): Early stopping. Defaults to true.\n    \"\"\"\n    super().__init__(rng)\n\n    self.training_info: pd.DataFrame\n    self.prediction: torch.Tensor\n    self.input_dim: int\n    self.output_dim: int\n    self._Y_test: np.ndarray | torch.Tensor\n    self._X_test: np.ndarray | torch.Tensor\n    self._model: nn.Module\n    self.dropout_proba = dropout_proba\n    self.learning_rate = learning_rate\n    self.n_epochs = n_epochs\n    self.patience = patience\n    self.batch_size = batch_size\n    self.val_size = val_size\n    self.es = es\n\n    self.loss_name = loss\n\n    has_mps = torch.backends.mps.is_built()\n    self.device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre>"},{"location":"api/#gresit.torch_models.Multioutcome_MLP.fit","title":"<code>fit(X, Y, idx_train=None, idx_test=None)</code>","text":"<p>Fit the MLP model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data</p> required <code>Y</code> <code>ndarray</code> <p>Target data</p> required <code>idx_train</code> <code>ndarray</code> <p>training indices</p> <code>None</code> <code>idx_test</code> <code>ndarray</code> <p>test indices</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>None</code> <p>epoch at which training was stopped.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray,\n    idx_train: np.ndarray | None = None,\n    idx_test: np.ndarray | None = None,\n) -&gt; None:\n    \"\"\"Fit the MLP model.\n\n    Args:\n        X (np.ndarray): Input data\n        Y (np.ndarray): Target data\n        idx_train (np.ndarray): training indices\n        idx_test (np.ndarray): test indices\n\n    Returns:\n        int: epoch at which training was stopped.\n    \"\"\"\n    if idx_train is None and idx_test is None:\n        X_train, X_val, Y_train, Y_val = self.split_and_standardize(\n            X=X,\n            Y=Y,\n            test_size=self.val_size,\n        )\n    else:\n        X_train, X_val, Y_train, Y_val = (X[idx_train], X[idx_test], Y[idx_train], Y[idx_test])\n        X_train = (X_train - X_train.mean(axis=0)) / (X_train.std(axis=0))\n        Y_train = (Y_train - Y_train.mean(axis=0)) / (Y_train.std(axis=0))\n        X_val = (X_val - X_train.mean(axis=0)) / (X_train.std(axis=0))\n        Y_val = (Y_val - Y_train.mean(axis=0)) / (Y_train.std(axis=0))\n\n    if self.loss_name == \"hisc\":\n        loss_fn = loss_hsic\n    else:\n        loss_fn = loss_mse\n\n    self._X_train = torch.from_numpy(X_train).float().to(self.device)\n    self._Y_train = torch.from_numpy(Y_train).float().to(self.device)\n\n    self._X_val = torch.from_numpy(X_val).float().to(self.device)\n    self._Y_val = torch.from_numpy(Y_val).float().to(self.device)\n\n    model = MLP(\n        input_dim=self._X_train.shape[1],\n        output_dim=self._Y_train.shape[1],\n        dropout=self.dropout_proba,\n    ).to(self.device)\n\n    if self.es:\n        es = EarlyStopping(patience=self.patience)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=self.learning_rate,\n        amsgrad=True,\n    )\n\n    dataset_train = TensorDataset(self._X_train, self._Y_train)\n    dataloader_train = DataLoader(dataset_train, batch_size=self.batch_size, shuffle=True)\n\n    history_train = []\n    history_val = []\n    trainstep = 0\n    for i in tqdm(range(self.n_epochs), ncols=100):\n        for j, (x_batch, y_batch) in enumerate(dataloader_train):\n            Y_pred = model(x_batch)\n            loss = loss_fn(\n                self._standardize(x_batch),\n                self._standardize(Y_pred),\n                self._standardize(y_batch),\n                device=self.device,\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            history_train.append(\n                {\n                    \"epoch\": i,\n                    \"minibatch\": j,\n                    \"trainstep\": trainstep,\n                    \"task\": \"regression\",\n                    \"loss\": loss.detach().cpu().numpy(),\n                }\n            )\n            trainstep += 1\n\n        # Evaluate Model\n        with torch.no_grad():\n            if self.loss_name != \"mse\":\n                bias = self._compute_bias(model=model, y_true=self._Y_val, x=self._X_val)\n                model.update_bias(bias)\n            Y_pred = model(self._X_val)\n            vloss = loss_fn(\n                self._standardize(self._X_val),\n                self._standardize(Y_pred),\n                self._standardize(self._Y_val),\n                device=self.device,\n            )\n\n            history_val.append(\n                {\n                    \"epoch\": i,\n                    \"trainstep\": trainstep,\n                    \"task\": \"regression\",\n                    # \"lr\": lr_scheduler.get_last_lr()[0],\n                    \"lr\": self.learning_rate,\n                    \"loss\": vloss.detach().cpu().numpy(),\n                }\n            )\n            if self.es:\n                if es(model, vloss):\n                    print(f\"{es.status}\")\n                    break\n\n    self.training_info = pd.DataFrame(history_train)\n    self.validation_info = pd.DataFrame(history_val)\n    self.prediction = make_preds_single(model, self._X_val)\n    self._model = model\n    self.final_loss = (\n        es.best_loss.detach().cpu().numpy() if self.es and es.best_loss else np.infty\n    )\n</code></pre>"},{"location":"api/#gresit.torch_models.Multioutcome_MLP.plot_training_info","title":"<code>plot_training_info()</code>","text":"<p>Plot some summary over the MSE during training.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def plot_training_info(self) -&gt; None:\n    \"\"\"Plot some summary over the MSE during training.\"\"\"\n    sns.lmplot(\n        x=\"trainstep\",\n        y=\"loss\",\n        hue=\"task\",\n        lowess=True,\n        scatter_kws={\"alpha\": 0.5},\n        line_kws={\"linewidth\": 2},\n        data=self.training_info,\n    )\n\n    sns.lmplot(\n        x=\"epoch\",\n        y=\"loss\",\n        hue=\"task\",\n        lowess=True,\n        scatter_kws={\"alpha\": 0.5},\n        line_kws={\"linewidth\": 2},\n        data=self.validation_info,\n    )\n</code></pre>"},{"location":"api/#gresit.torch_models.Multioutcome_MLP.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Make predictions.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predictions</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Make predictions.\n\n    Returns:\n        np.ndarray: Predictions\n    \"\"\"\n    if X_test is None:\n        prediction = self.prediction.detach().numpy()\n    else:\n        X_torch_test = torch.from_numpy(X_test).float().to(self.device)\n        pred = make_preds_single(self._model, X_torch_test)\n        prediction = pred.detach().cpu().numpy()\n    return prediction\n</code></pre>"},{"location":"api/#gresit.torch_models.MultitaskGPModel","title":"<code>MultitaskGPModel</code>","text":"<p>               Bases: <code>ExactGP</code></p> <p>Multitask GPR.</p> Source code in <code>gresit/torch_models.py</code> <pre><code>class MultitaskGPModel(gpytorch.models.ExactGP):  # type: ignore\n    \"\"\"Multitask GPR.\"\"\"\n\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods._GaussianLikelihoodBase,\n    ) -&gt; None:\n        \"\"\"Inits the model.\n\n        Args:\n            train_x (_type_): _description_\n            train_y (_type_): _description_\n            likelihood (_type_): _description_\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        num_tasks = train_y.size(1)\n        self.mean_module = gpytorch.means.MultitaskMean(\n            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n        )\n        self.covar_module = gpytorch.kernels.MultitaskKernel(\n            gpytorch.kernels.RBFKernel(), num_tasks=num_tasks, rank=1\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Args:\n            x (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n</code></pre>"},{"location":"api/#gresit.torch_models.MultitaskGPModel.__init__","title":"<code>__init__(train_x, train_y, likelihood)</code>","text":"<p>Inits the model.</p> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <code>_type_</code> <p>description</p> required <code>train_y</code> <code>_type_</code> <p>description</p> required <code>likelihood</code> <code>_type_</code> <p>description</p> required Source code in <code>gresit/torch_models.py</code> <pre><code>def __init__(\n    self,\n    train_x: torch.Tensor,\n    train_y: torch.Tensor,\n    likelihood: gpytorch.likelihoods._GaussianLikelihoodBase,\n) -&gt; None:\n    \"\"\"Inits the model.\n\n    Args:\n        train_x (_type_): _description_\n        train_y (_type_): _description_\n        likelihood (_type_): _description_\n    \"\"\"\n    super().__init__(train_x, train_y, likelihood)\n    num_tasks = train_y.size(1)\n    self.mean_module = gpytorch.means.MultitaskMean(\n        gpytorch.means.ConstantMean(), num_tasks=num_tasks\n    )\n    self.covar_module = gpytorch.kernels.MultitaskKernel(\n        gpytorch.kernels.RBFKernel(), num_tasks=num_tasks, rank=1\n    )\n</code></pre>"},{"location":"api/#gresit.torch_models.MultitaskGPModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>Tensor</code> <p>description</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Args:\n        x (_type_): _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    mean_x = self.mean_module(x)\n    covar_x = self.covar_module(x)\n    return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n</code></pre>"},{"location":"api/#gresit.torch_models.make_preds_single","title":"<code>make_preds_single(model, X)</code>","text":"<p>Helper function for predicting.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>nn model instance</p> required <code>X</code> <code>Tensor</code> <p>test data</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>Tensor</code> <p>description</p> Source code in <code>gresit/torch_models.py</code> <pre><code>def make_preds_single(model: nn.Module, X: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Helper function for predicting.\n\n    Args:\n        model (nn.Model): nn model instance\n        X (torch.Tensor): test data\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    # helper function to make predictions for a model\n    with torch.no_grad():\n        y_hat = model(X)\n    return y_hat\n</code></pre>"},{"location":"api/#independence-tests","title":"Independence Tests","text":"<p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"api/#gresit.independence_tests.CItest","title":"<code>CItest</code>","text":"<p>Abstract meta class for independence tests.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class CItest(metaclass=ABCMeta):\n    \"\"\"Abstract meta class for independence tests.\"\"\"\n\n    def _check_input(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n        z_data: np.ndarray | pd.DataFrame | pd.Series | None = None,\n    ) -&gt; None:\n        if not isinstance(x_data, np.ndarray | pd.DataFrame | pd.Series):\n            raise TypeError(\"x_data must be of type np.ndarray, pd.DataFrame, or pd.Series\")\n        if not isinstance(y_data, np.ndarray | pd.DataFrame | pd.Series):\n            raise TypeError(\"y_data must be of type np.ndarray, pd.DataFrame, or pd.Series\")\n        if not isinstance(z_data, np.ndarray | pd.DataFrame | pd.Series) or z_data is None:\n            raise TypeError(\"y_data must be of type np.ndarray, pd.DataFrame, or pd.Series\")\n\n    @abstractmethod\n    def test(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n        z_data: np.ndarray | pd.DataFrame | pd.Series | None = None,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Abstract method for independence tests.\n\n        Args:\n            x_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n            y_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n            z_data (np.ndarray | pd.DataFrame | pd.Series | None): Variables involved in the test\n\n\n        Returns:\n            tuple[float, float]: Test statistic and corresponding pvalue (Test decision).\n        \"\"\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.CItest.test","title":"<code>test(x_data, y_data, z_data=None)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for independence tests.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray | DataFrame | Series</code> <p>Variables involved in the test</p> required <code>y_data</code> <code>ndarray | DataFrame | Series</code> <p>Variables involved in the test</p> required <code>z_data</code> <code>ndarray | DataFrame | Series | None</code> <p>Variables involved in the test</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: Test statistic and corresponding pvalue (Test decision).</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>@abstractmethod\ndef test(\n    self,\n    x_data: np.ndarray | pd.DataFrame | pd.Series,\n    y_data: np.ndarray | pd.DataFrame | pd.Series,\n    z_data: np.ndarray | pd.DataFrame | pd.Series | None = None,\n) -&gt; tuple[float, float]:\n    \"\"\"Abstract method for independence tests.\n\n    Args:\n        x_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n        y_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n        z_data (np.ndarray | pd.DataFrame | pd.Series | None): Variables involved in the test\n\n\n    Returns:\n        tuple[float, float]: Test statistic and corresponding pvalue (Test decision).\n    \"\"\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.DISCO","title":"<code>DISCO</code>","text":"<p>               Bases: <code>Itest</code></p> <p>Simple Wrapper class around the squared distance covariance from the dcor class.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class DISCO(Itest):\n    \"\"\"Simple Wrapper class around the squared distance covariance from the dcor class.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Inits the object.\"\"\"\n        pass\n\n    def test(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n    ) -&gt; tuple[float, str]:\n        \"\"\"Test for independence between two vectors Finite joint first moments are assumed.\n\n        Args:\n            x_data (np.ndarray | pd.DataFrame | pd.Series): x-data involved in the test\n            y_data (np.ndarray | pd.DataFrame | pd.Series): y-data involved in the test\n\n        Returns:\n            tuple[float, str]: Distance covariance value and some string to comply with format.\n        \"\"\"\n        self._check_input(x_data, y_data)\n        return u_distance_covariance_sqr(x=x_data, y=y_data), \"Squared Distance Covariance\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.DISCO.__init__","title":"<code>__init__()</code>","text":"<p>Inits the object.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Inits the object.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#gresit.independence_tests.DISCO.test","title":"<code>test(x_data, y_data)</code>","text":"<p>Test for independence between two vectors Finite joint first moments are assumed.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray | DataFrame | Series</code> <p>x-data involved in the test</p> required <code>y_data</code> <code>ndarray | DataFrame | Series</code> <p>y-data involved in the test</p> required <p>Returns:</p> Type Description <code>tuple[float, str]</code> <p>tuple[float, str]: Distance covariance value and some string to comply with format.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def test(\n    self,\n    x_data: np.ndarray | pd.DataFrame | pd.Series,\n    y_data: np.ndarray | pd.DataFrame | pd.Series,\n) -&gt; tuple[float, str]:\n    \"\"\"Test for independence between two vectors Finite joint first moments are assumed.\n\n    Args:\n        x_data (np.ndarray | pd.DataFrame | pd.Series): x-data involved in the test\n        y_data (np.ndarray | pd.DataFrame | pd.Series): y-data involved in the test\n\n    Returns:\n        tuple[float, str]: Distance covariance value and some string to comply with format.\n    \"\"\"\n    self._check_input(x_data, y_data)\n    return u_distance_covariance_sqr(x=x_data, y=y_data), \"Squared Distance Covariance\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.FisherZVec","title":"<code>FisherZVec</code>","text":"<p>               Bases: <code>CItest</code></p> <p>Simple extension of standard Fisher-Z test for independence.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class FisherZVec(CItest):\n    \"\"\"Simple extension of standard Fisher-Z test for independence.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Init of the object.\"\"\"\n        pass\n\n    def test(\n        self,\n        x_data: np.ndarray,\n        y_data: np.ndarray,\n        z_data: np.ndarray | None = None,\n        corr_threshold: float = 0.999,\n    ) -&gt; tuple[float, float]:\n        \"\"\"Retrieve (composite) p_value using Fisher z-transformation.\n\n        Appropriate when data is jointly Gaussian.\n\n        Args:\n            x_data (np.ndarray): X_data.\n            y_data (np.ndarray): Y_data.\n            z_data (np.ndarray | None): Z_data. defaults to None.\n            corr_threshold (float, optional): Threshold to make sure\n                r in [-1,1]. Defaults to 0.999.\n\n        Returns:\n            tuple[float,float]: test_statistic, p_value\n        \"\"\"\n        n = x_data.shape[0]\n\n        if z_data is not None:\n            sep_set_length = z_data.shape[1]\n            corrdata = np.empty((n, 2 + z_data.shape[1], x_data.shape[1] * y_data.shape[1]))\n            k = -1\n            for i, j in product(range(x_data.shape[1]), range(y_data.shape[1])):\n                k += 1\n                corrdata[:, :, k] = np.concatenate(\n                    [x_data[:, i][:, np.newaxis], y_data[:, j][:, np.newaxis], z_data], axis=1\n                )\n            precision_matrices = np.empty(\n                (2 + z_data.shape[1], 2 + z_data.shape[1], x_data.shape[1] * y_data.shape[1])\n            )\n            for k in range(precision_matrices.shape[-1]):\n                corrmat = np.corrcoef(corrdata[:, :, k].T)\n                try:\n                    precision_matrices[:, :, k] = np.linalg.inv(corrmat)\n                except np.linalg.LinAlgError as error:\n                    raise ValueError(\n                        \"The correlation matrix of your data is singular. \\\n                        Partial correlations cannot be estimated. Are there  \\\n                        collinearities in your data?\"\n                    ) from error\n\n            r = np.empty(precision_matrices.shape[-1])\n            for k in range(precision_matrices.shape[-1]):\n                precision_matrix = precision_matrices[:, :, k]\n                r[k] = (\n                    -1\n                    * precision_matrix[0, 1]\n                    / np.sqrt(np.abs(precision_matrix[0, 0] * precision_matrix[1, 1]))\n                )\n        else:\n            sep_set_length = 0\n            uncond = []\n            for i in range(x_data.shape[1]):\n                uncond.append(\n                    np.corrcoef(np.concatenate([x_data[:, i][:, np.newaxis], y_data], axis=1).T)[\n                        1:, 0\n                    ]\n                )\n            r = np.concatenate(uncond)\n\n        r = np.minimum(\n            corr_threshold, np.maximum(-1 * corr_threshold, r)\n        )  # make r between -1 and 1\n        # Fisher\u2019s z-transform\n        factor = np.sqrt(n - sep_set_length - 3)\n        z_transform = factor * 0.5 * np.log((1 + r) / (1 - r))\n        test_stat = factor * z_transform\n        p_value = 2 * (1 - norm.cdf(abs(z_transform)))\n\n        final_test_stat = test_stat[np.argmin(np.abs(test_stat))]\n        final_p_value = np.max(p_value)\n\n        return (float(final_test_stat), float(final_p_value))\n</code></pre>"},{"location":"api/#gresit.independence_tests.FisherZVec.__init__","title":"<code>__init__()</code>","text":"<p>Init of the object.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Init of the object.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#gresit.independence_tests.FisherZVec.test","title":"<code>test(x_data, y_data, z_data=None, corr_threshold=0.999)</code>","text":"<p>Retrieve (composite) p_value using Fisher z-transformation.</p> <p>Appropriate when data is jointly Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>X_data.</p> required <code>y_data</code> <code>ndarray</code> <p>Y_data.</p> required <code>z_data</code> <code>ndarray | None</code> <p>Z_data. defaults to None.</p> <code>None</code> <code>corr_threshold</code> <code>float</code> <p>Threshold to make sure r in [-1,1]. Defaults to 0.999.</p> <code>0.999</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: test_statistic, p_value</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def test(\n    self,\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    z_data: np.ndarray | None = None,\n    corr_threshold: float = 0.999,\n) -&gt; tuple[float, float]:\n    \"\"\"Retrieve (composite) p_value using Fisher z-transformation.\n\n    Appropriate when data is jointly Gaussian.\n\n    Args:\n        x_data (np.ndarray): X_data.\n        y_data (np.ndarray): Y_data.\n        z_data (np.ndarray | None): Z_data. defaults to None.\n        corr_threshold (float, optional): Threshold to make sure\n            r in [-1,1]. Defaults to 0.999.\n\n    Returns:\n        tuple[float,float]: test_statistic, p_value\n    \"\"\"\n    n = x_data.shape[0]\n\n    if z_data is not None:\n        sep_set_length = z_data.shape[1]\n        corrdata = np.empty((n, 2 + z_data.shape[1], x_data.shape[1] * y_data.shape[1]))\n        k = -1\n        for i, j in product(range(x_data.shape[1]), range(y_data.shape[1])):\n            k += 1\n            corrdata[:, :, k] = np.concatenate(\n                [x_data[:, i][:, np.newaxis], y_data[:, j][:, np.newaxis], z_data], axis=1\n            )\n        precision_matrices = np.empty(\n            (2 + z_data.shape[1], 2 + z_data.shape[1], x_data.shape[1] * y_data.shape[1])\n        )\n        for k in range(precision_matrices.shape[-1]):\n            corrmat = np.corrcoef(corrdata[:, :, k].T)\n            try:\n                precision_matrices[:, :, k] = np.linalg.inv(corrmat)\n            except np.linalg.LinAlgError as error:\n                raise ValueError(\n                    \"The correlation matrix of your data is singular. \\\n                    Partial correlations cannot be estimated. Are there  \\\n                    collinearities in your data?\"\n                ) from error\n\n        r = np.empty(precision_matrices.shape[-1])\n        for k in range(precision_matrices.shape[-1]):\n            precision_matrix = precision_matrices[:, :, k]\n            r[k] = (\n                -1\n                * precision_matrix[0, 1]\n                / np.sqrt(np.abs(precision_matrix[0, 0] * precision_matrix[1, 1]))\n            )\n    else:\n        sep_set_length = 0\n        uncond = []\n        for i in range(x_data.shape[1]):\n            uncond.append(\n                np.corrcoef(np.concatenate([x_data[:, i][:, np.newaxis], y_data], axis=1).T)[\n                    1:, 0\n                ]\n            )\n        r = np.concatenate(uncond)\n\n    r = np.minimum(\n        corr_threshold, np.maximum(-1 * corr_threshold, r)\n    )  # make r between -1 and 1\n    # Fisher\u2019s z-transform\n    factor = np.sqrt(n - sep_set_length - 3)\n    z_transform = factor * 0.5 * np.log((1 + r) / (1 - r))\n    test_stat = factor * z_transform\n    p_value = 2 * (1 - norm.cdf(abs(z_transform)))\n\n    final_test_stat = test_stat[np.argmin(np.abs(test_stat))]\n    final_p_value = np.max(p_value)\n\n    return (float(final_test_stat), float(final_p_value))\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC","title":"<code>HSIC</code>","text":"<p>               Bases: <code>Itest</code></p> <p>Hilbert-Schmidt Independence Criterion (HSIC) test.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class HSIC(Itest):\n    \"\"\"Hilbert-Schmidt Independence Criterion (HSIC) test.\"\"\"\n\n    def test(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n        bw_method: str = \"mdbs\",\n    ) -&gt; tuple[float, float]:\n        \"\"\"Test for independence between two vectors.\n\n        Args:\n            x_data (np.ndarray | pd.DataFrame | pd.Series): x-data involved in the test\n            y_data (np.ndarray | pd.DataFrame | pd.Series): y-data involved in the test\n            bw_method (str, optional): The method used to calculate the bandwidth of the HSIC.\n                * ``mdbs`` : Median distance between samples.\n                * ``scott`` : Scott's Rule of Thumb.\n                * ``silverman`` : Silverman's Rule of Thumb.. Defaults to \"mdbs\".\n\n\n        Returns:\n            tuple[float, float]: Test statistic and corresponding pvalue.\n        \"\"\"\n        self._check_input(x_data, y_data)\n        x_data = x_data if isinstance(x_data, np.ndarray) else x_data.values\n        y_data = y_data if isinstance(y_data, np.ndarray) else y_data.values\n        test_stat, p_value = self.hsic_test_gamma(X=x_data, Y=y_data, bw_method=bw_method)\n        return test_stat, p_value\n\n    def get_kernel_width(self, X: np.ndarray, sample_cut: int = 100) -&gt; np.float64:\n        \"\"\"Calculate the bandwidth to median distance between points.\n\n        Use at most 100 points (since median is only a heuristic,\n        and 100 points is sufficient for a robust estimate).\n\n        Args:\n            X (np.ndarray): shape (n_samples, n_features) Training data,\n            sample_cut (int, optional): Number of samples to use for bandwidth calculation.\n\n        Returns:\n            float: The bandwidth parameter.\n        \"\"\"\n        n_samples = X.shape[0]\n        if n_samples &gt; sample_cut:\n            X_med = X[:sample_cut, :]\n            n_samples = sample_cut\n        else:\n            X_med = X\n\n        G = np.sum(X_med * X_med, 1).reshape(n_samples, 1)\n        dists = G + G.T - 2 * np.dot(X_med, X_med.T)\n        dists = dists - np.tril(dists)\n        dists = dists.reshape(n_samples**2, 1)\n\n        return np.sqrt(0.5 * np.median(dists[dists &gt; 0]))\n\n    def _rbf_dot(self, X: np.ndarray, width: float) -&gt; np.ndarray:\n        \"\"\"Calculate rbf dot, in special case with X dot X.\n\n        Args:\n            X (np.ndarray): data\n            width (float): bandwidth parameter\n\n        Returns:\n            np.ndarray: Kernel matrix.\n        \"\"\"\n        G = np.sum(X * X, axis=1)\n        H = G[None, :] + G[:, None] - 2 * np.dot(X, X.T)\n        return np.exp(-H / 2 / (width**2))\n\n    def get_gram_matrix(self, X: np.ndarray, width: float) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Get the centered gram matrices.\n\n        Args:\n            X (np.ndarray): shape (n_samples, n_features)\n                Training data, where ``n_samples`` is the number of samples\n                and ``n_features`` is the number of features.\n            width (float): The bandwidth parameter.\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: The centered gram matrices.\n        \"\"\"\n        n = X.shape[0]\n\n        K = self._rbf_dot(X, width)\n        K_colsums = K.sum(axis=0)\n        K_rowsums = K.sum(axis=1)\n        K_allsum = K_rowsums.sum()\n        Kc = K - (K_colsums[None, :] + K_rowsums[:, None]) / n + (K_allsum / n**2)\n        return K, Kc\n\n    def hsic_teststat(self, Kc: np.ndarray, Lc: np.ndarray, n: int) -&gt; np.float64:\n        \"\"\"Get the HSIC statistic.\n\n        Args:\n            Kc (np.ndarray): Centered gram matrix.\n            Lc (np.ndarray): Centered gram matrix.\n            n (int): Sample size.\n\n        Returns:\n            float: HSIC statistic.\n        \"\"\"\n        # test statistic m*HSICb under H1\n        return 1 / n * np.sum(Kc.T * Lc)\n\n    def hsic_test_gamma(\n        self, X: np.ndarray, Y: np.ndarray, bw_method: str = \"mdbs\"\n    ) -&gt; tuple[float, float]:\n        \"\"\"Get the HSIC statistic and p-value.\n\n        Args:\n            X (np.ndarray): data, possibly vector-valued.\n            Y (np.ndarray): data, possibly vector-valued.\n            bw_method (str, optional): The method used to calculate the bandwidth of the HSIC.\n                * ``mdbs`` : Median distance between samples.\n                * ``scott`` : Scott's Rule of Thumb.\n                * ``silverman`` : Silverman's Rule of Thumb.. Defaults to \"mdbs\".\n\n        Returns:\n            tuple[float, float]: HSIC test statistic and corresponding p-value\n        \"\"\"\n        X = X.reshape(-1, 1) if X.ndim == 1 else X\n        Y = Y.reshape(-1, 1) if Y.ndim == 1 else Y\n\n        if bw_method == \"scott\":\n            width_x = bandwidths.bw_scott(X)\n            width_y = bandwidths.bw_scott(Y)\n        elif bw_method == \"silverman\":\n            width_x = bandwidths.bw_silverman(X)\n            width_y = bandwidths.bw_silverman(Y)\n        # Get kernel width to median distance between points\n        else:\n            width_x = self.get_kernel_width(X)\n            width_y = self.get_kernel_width(Y)\n\n        # these are slightly biased estimates of centered gram matrices\n        K, Kc = self.get_gram_matrix(X, width_x)\n        L, Lc = self.get_gram_matrix(Y, width_y)\n\n        # test statistic m*HSICb under H1\n        n = X.shape[0]\n        test_stat = self.hsic_teststat(Kc, Lc, n)\n\n        var = (1 / 6 * Kc * Lc) ** 2\n        # second subtracted term is bias correction\n        var = 1 / n / (n - 1) * (np.sum(var) - np.trace(var))\n        # variance under H0\n        var = 72 * (n - 4) * (n - 5) / n / (n - 1) / (n - 2) / (n - 3) * var\n\n        K[np.diag_indices(n)] = 0\n        L[np.diag_indices(n)] = 0\n        mu_X = 1 / n / (n - 1) * K.sum()\n        mu_Y = 1 / n / (n - 1) * L.sum()\n        # mean under H0\n        mean = 1 / n * (1 + mu_X * mu_Y - mu_X - mu_Y)\n\n        alpha = mean**2 / var\n        # threshold for hsicArr*m\n        beta = var * n / mean\n        p = gamma.sf(test_stat, alpha, scale=beta)\n\n        return test_stat, p\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC.get_gram_matrix","title":"<code>get_gram_matrix(X, width)</code>","text":"<p>Get the centered gram matrices.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>shape (n_samples, n_features) Training data, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> required <code>width</code> <code>float</code> <p>The bandwidth parameter.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: The centered gram matrices.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def get_gram_matrix(self, X: np.ndarray, width: float) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the centered gram matrices.\n\n    Args:\n        X (np.ndarray): shape (n_samples, n_features)\n            Training data, where ``n_samples`` is the number of samples\n            and ``n_features`` is the number of features.\n        width (float): The bandwidth parameter.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The centered gram matrices.\n    \"\"\"\n    n = X.shape[0]\n\n    K = self._rbf_dot(X, width)\n    K_colsums = K.sum(axis=0)\n    K_rowsums = K.sum(axis=1)\n    K_allsum = K_rowsums.sum()\n    Kc = K - (K_colsums[None, :] + K_rowsums[:, None]) / n + (K_allsum / n**2)\n    return K, Kc\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC.get_kernel_width","title":"<code>get_kernel_width(X, sample_cut=100)</code>","text":"<p>Calculate the bandwidth to median distance between points.</p> <p>Use at most 100 points (since median is only a heuristic, and 100 points is sufficient for a robust estimate).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>shape (n_samples, n_features) Training data,</p> required <code>sample_cut</code> <code>int</code> <p>Number of samples to use for bandwidth calculation.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float64</code> <p>The bandwidth parameter.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def get_kernel_width(self, X: np.ndarray, sample_cut: int = 100) -&gt; np.float64:\n    \"\"\"Calculate the bandwidth to median distance between points.\n\n    Use at most 100 points (since median is only a heuristic,\n    and 100 points is sufficient for a robust estimate).\n\n    Args:\n        X (np.ndarray): shape (n_samples, n_features) Training data,\n        sample_cut (int, optional): Number of samples to use for bandwidth calculation.\n\n    Returns:\n        float: The bandwidth parameter.\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples &gt; sample_cut:\n        X_med = X[:sample_cut, :]\n        n_samples = sample_cut\n    else:\n        X_med = X\n\n    G = np.sum(X_med * X_med, 1).reshape(n_samples, 1)\n    dists = G + G.T - 2 * np.dot(X_med, X_med.T)\n    dists = dists - np.tril(dists)\n    dists = dists.reshape(n_samples**2, 1)\n\n    return np.sqrt(0.5 * np.median(dists[dists &gt; 0]))\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC.hsic_test_gamma","title":"<code>hsic_test_gamma(X, Y, bw_method='mdbs')</code>","text":"<p>Get the HSIC statistic and p-value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>data, possibly vector-valued.</p> required <code>Y</code> <code>ndarray</code> <p>data, possibly vector-valued.</p> required <code>bw_method</code> <code>str</code> <p>The method used to calculate the bandwidth of the HSIC. * <code>mdbs</code> : Median distance between samples. * <code>scott</code> : Scott's Rule of Thumb. * <code>silverman</code> : Silverman's Rule of Thumb.. Defaults to \"mdbs\".</p> <code>'mdbs'</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: HSIC test statistic and corresponding p-value</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def hsic_test_gamma(\n    self, X: np.ndarray, Y: np.ndarray, bw_method: str = \"mdbs\"\n) -&gt; tuple[float, float]:\n    \"\"\"Get the HSIC statistic and p-value.\n\n    Args:\n        X (np.ndarray): data, possibly vector-valued.\n        Y (np.ndarray): data, possibly vector-valued.\n        bw_method (str, optional): The method used to calculate the bandwidth of the HSIC.\n            * ``mdbs`` : Median distance between samples.\n            * ``scott`` : Scott's Rule of Thumb.\n            * ``silverman`` : Silverman's Rule of Thumb.. Defaults to \"mdbs\".\n\n    Returns:\n        tuple[float, float]: HSIC test statistic and corresponding p-value\n    \"\"\"\n    X = X.reshape(-1, 1) if X.ndim == 1 else X\n    Y = Y.reshape(-1, 1) if Y.ndim == 1 else Y\n\n    if bw_method == \"scott\":\n        width_x = bandwidths.bw_scott(X)\n        width_y = bandwidths.bw_scott(Y)\n    elif bw_method == \"silverman\":\n        width_x = bandwidths.bw_silverman(X)\n        width_y = bandwidths.bw_silverman(Y)\n    # Get kernel width to median distance between points\n    else:\n        width_x = self.get_kernel_width(X)\n        width_y = self.get_kernel_width(Y)\n\n    # these are slightly biased estimates of centered gram matrices\n    K, Kc = self.get_gram_matrix(X, width_x)\n    L, Lc = self.get_gram_matrix(Y, width_y)\n\n    # test statistic m*HSICb under H1\n    n = X.shape[0]\n    test_stat = self.hsic_teststat(Kc, Lc, n)\n\n    var = (1 / 6 * Kc * Lc) ** 2\n    # second subtracted term is bias correction\n    var = 1 / n / (n - 1) * (np.sum(var) - np.trace(var))\n    # variance under H0\n    var = 72 * (n - 4) * (n - 5) / n / (n - 1) / (n - 2) / (n - 3) * var\n\n    K[np.diag_indices(n)] = 0\n    L[np.diag_indices(n)] = 0\n    mu_X = 1 / n / (n - 1) * K.sum()\n    mu_Y = 1 / n / (n - 1) * L.sum()\n    # mean under H0\n    mean = 1 / n * (1 + mu_X * mu_Y - mu_X - mu_Y)\n\n    alpha = mean**2 / var\n    # threshold for hsicArr*m\n    beta = var * n / mean\n    p = gamma.sf(test_stat, alpha, scale=beta)\n\n    return test_stat, p\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC.hsic_teststat","title":"<code>hsic_teststat(Kc, Lc, n)</code>","text":"<p>Get the HSIC statistic.</p> <p>Parameters:</p> Name Type Description Default <code>Kc</code> <code>ndarray</code> <p>Centered gram matrix.</p> required <code>Lc</code> <code>ndarray</code> <p>Centered gram matrix.</p> required <code>n</code> <code>int</code> <p>Sample size.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float64</code> <p>HSIC statistic.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def hsic_teststat(self, Kc: np.ndarray, Lc: np.ndarray, n: int) -&gt; np.float64:\n    \"\"\"Get the HSIC statistic.\n\n    Args:\n        Kc (np.ndarray): Centered gram matrix.\n        Lc (np.ndarray): Centered gram matrix.\n        n (int): Sample size.\n\n    Returns:\n        float: HSIC statistic.\n    \"\"\"\n    # test statistic m*HSICb under H1\n    return 1 / n * np.sum(Kc.T * Lc)\n</code></pre>"},{"location":"api/#gresit.independence_tests.HSIC.test","title":"<code>test(x_data, y_data, bw_method='mdbs')</code>","text":"<p>Test for independence between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray | DataFrame | Series</code> <p>x-data involved in the test</p> required <code>y_data</code> <code>ndarray | DataFrame | Series</code> <p>y-data involved in the test</p> required <code>bw_method</code> <code>str</code> <p>The method used to calculate the bandwidth of the HSIC. * <code>mdbs</code> : Median distance between samples. * <code>scott</code> : Scott's Rule of Thumb. * <code>silverman</code> : Silverman's Rule of Thumb.. Defaults to \"mdbs\".</p> <code>'mdbs'</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: Test statistic and corresponding pvalue.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def test(\n    self,\n    x_data: np.ndarray | pd.DataFrame | pd.Series,\n    y_data: np.ndarray | pd.DataFrame | pd.Series,\n    bw_method: str = \"mdbs\",\n) -&gt; tuple[float, float]:\n    \"\"\"Test for independence between two vectors.\n\n    Args:\n        x_data (np.ndarray | pd.DataFrame | pd.Series): x-data involved in the test\n        y_data (np.ndarray | pd.DataFrame | pd.Series): y-data involved in the test\n        bw_method (str, optional): The method used to calculate the bandwidth of the HSIC.\n            * ``mdbs`` : Median distance between samples.\n            * ``scott`` : Scott's Rule of Thumb.\n            * ``silverman`` : Silverman's Rule of Thumb.. Defaults to \"mdbs\".\n\n\n    Returns:\n        tuple[float, float]: Test statistic and corresponding pvalue.\n    \"\"\"\n    self._check_input(x_data, y_data)\n    x_data = x_data if isinstance(x_data, np.ndarray) else x_data.values\n    y_data = y_data if isinstance(y_data, np.ndarray) else y_data.values\n    test_stat, p_value = self.hsic_test_gamma(X=x_data, Y=y_data, bw_method=bw_method)\n    return test_stat, p_value\n</code></pre>"},{"location":"api/#gresit.independence_tests.Itest","title":"<code>Itest</code>","text":"<p>Abstract meta class for independence tests.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class Itest(metaclass=ABCMeta):\n    \"\"\"Abstract meta class for independence tests.\"\"\"\n\n    def _check_input(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n    ) -&gt; None:\n        if not isinstance(x_data, np.ndarray | pd.DataFrame | pd.Series):\n            raise TypeError(\"x_data must be of type np.ndarray, pd.DataFrame, or pd.Series\")\n        if not isinstance(y_data, np.ndarray | pd.DataFrame | pd.Series):\n            raise TypeError(\"y_data must be of type np.ndarray, pd.DataFrame, or pd.Series\")\n\n    @abstractmethod\n    def test(\n        self,\n        x_data: np.ndarray | pd.DataFrame | pd.Series,\n        y_data: np.ndarray | pd.DataFrame | pd.Series,\n    ) -&gt; tuple[float, float | str]:\n        \"\"\"Abstract method for independence tests.\n\n        Args:\n            x_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n            y_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n\n\n        Returns:\n            tuple[float, float | str]: Test statistic and corresponding pvalue (Test decision).\n        \"\"\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.Itest.test","title":"<code>test(x_data, y_data)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for independence tests.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray | DataFrame | Series</code> <p>Variables involved in the test</p> required <code>y_data</code> <code>ndarray | DataFrame | Series</code> <p>Variables involved in the test</p> required <p>Returns:</p> Type Description <code>tuple[float, float | str]</code> <p>tuple[float, float | str]: Test statistic and corresponding pvalue (Test decision).</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>@abstractmethod\ndef test(\n    self,\n    x_data: np.ndarray | pd.DataFrame | pd.Series,\n    y_data: np.ndarray | pd.DataFrame | pd.Series,\n) -&gt; tuple[float, float | str]:\n    \"\"\"Abstract method for independence tests.\n\n    Args:\n        x_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n        y_data (np.ndarray | pd.DataFrame | pd.Series): Variables involved in the test\n\n\n    Returns:\n        tuple[float, float | str]: Test statistic and corresponding pvalue (Test decision).\n    \"\"\"\n</code></pre>"},{"location":"api/#gresit.independence_tests.KernelCI","title":"<code>KernelCI</code>","text":"<p>               Bases: <code>CItest</code></p> <p>Kernel HSIC wrapper around causal-learn.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>class KernelCI(CItest):\n    \"\"\"Kernel HSIC wrapper around causal-learn.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Init of the object.\"\"\"\n        pass\n\n    def test(\n        self,\n        x_data: np.ndarray,\n        y_data: np.ndarray,\n        z_data: np.ndarray | None = None,\n    ) -&gt; tuple[float, float]:\n        \"\"\"KCI test wrapper.\n\n        Args:\n            x_data (np.ndarray): _description_\n            y_data (np.ndarray): _description_\n            z_data (np.ndarray | None, optional): _description_. Defaults to None.\n\n        Returns:\n            tuple[float, float]: Test statistic and p_value.\n        \"\"\"\n        if z_data is None:\n            unconditional_test = KCI.KCI_UInd()\n            p_value, test_stat = unconditional_test.compute_pvalue(data_x=x_data, data_y=y_data)\n        else:\n            conditional_test = KCI.KCI_CInd()\n            p_value, test_stat = conditional_test.compute_pvalue(\n                data_x=x_data, data_y=y_data, data_z=z_data\n            )\n\n        return float(test_stat), float(p_value)\n</code></pre>"},{"location":"api/#gresit.independence_tests.KernelCI.__init__","title":"<code>__init__()</code>","text":"<p>Init of the object.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Init of the object.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#gresit.independence_tests.KernelCI.test","title":"<code>test(x_data, y_data, z_data=None)</code>","text":"<p>KCI test wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>ndarray</code> <p>description</p> required <code>y_data</code> <code>ndarray</code> <p>description</p> required <code>z_data</code> <code>ndarray | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: Test statistic and p_value.</p> Source code in <code>gresit/independence_tests.py</code> <pre><code>def test(\n    self,\n    x_data: np.ndarray,\n    y_data: np.ndarray,\n    z_data: np.ndarray | None = None,\n) -&gt; tuple[float, float]:\n    \"\"\"KCI test wrapper.\n\n    Args:\n        x_data (np.ndarray): _description_\n        y_data (np.ndarray): _description_\n        z_data (np.ndarray | None, optional): _description_. Defaults to None.\n\n    Returns:\n        tuple[float, float]: Test statistic and p_value.\n    \"\"\"\n    if z_data is None:\n        unconditional_test = KCI.KCI_UInd()\n        p_value, test_stat = unconditional_test.compute_pvalue(data_x=x_data, data_y=y_data)\n    else:\n        conditional_test = KCI.KCI_CInd()\n        p_value, test_stat = conditional_test.compute_pvalue(\n            data_x=x_data, data_y=y_data, data_z=z_data\n        )\n\n    return float(test_stat), float(p_value)\n</code></pre>"},{"location":"api/#gresit.independence_tests.xi_vec_corr","title":"<code>xi_vec_corr(x, y)</code>","text":"<p>Compute the correlation coefficient between x and y.</p> <p>according to the xi coefficient defined by Chatterjee.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>description</p> required <code>y</code> <code>ndarray</code> <p>description</p> required Source code in <code>gresit/independence_tests.py</code> <pre><code>def xi_vec_corr(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute the correlation coefficient between x and y.\n\n    according to the xi coefficient defined by Chatterjee.\n\n    Args:\n        x (np.ndarray): _description_\n        y (np.ndarray): _description_\n    \"\"\"\n\n    def rank_order(vector: np.ndarray) -&gt; list[np.ndarray]:\n        random_index = np.random.choice(np.arange(length), length, replace=False)\n        randomized_vector = vector[random_index]\n        ranked_vector = rankdata(randomized_vector, method=\"ordinal\")\n        answer = [ranked_vector[j] for _, j in sorted(zip(random_index, range(length)))]\n        return answer\n\n    def compute_d_sequence(y: np.ndarray) -&gt; float:\n        ell = rankdata([-i for i in y], method=\"max\")\n        return float(np.sum(ell * (length - ell)) / (length**3))\n\n    def compute_xi_coefficient(vector: np.ndarray) -&gt; float:\n        mean_absolute = np.sum(np.abs([a - b for a, b in zip(vector[:-1], vector[1:])]))\n        return float(1 - mean_absolute / (2 * (length**2) * d_sequence))\n\n    def distance_transform(x: np.ndarray) -&gt; np.ndarray:\n        n = x.shape[0]\n        m = n * (n + 1) / 2\n\n        if x.ndim == 1:\n            diff = x[np.newaxis] - x[..., np.newaxis]\n            a = np.linalg.norm(diff[..., np.newaxis], axis=-1)\n        else:\n            diff = x[:, np.newaxis, :] - x[np.newaxis, :, :]\n            a = np.linalg.norm(diff, axis=-1)\n\n        H = np.eye(n) - 1.0 / n * np.ones((n, n))\n        A = H @ a @ H\n        indices = np.triu_indices(A.shape[0])\n        D_kx = A[indices]\n        if D_kx.shape[0] != m:\n            raise ValueError(\"Shapes do not agree,\")\n        return D_kx\n\n    x_transformed = distance_transform(x)\n    y_transformed = distance_transform(y)\n\n    length = len(x_transformed)\n    x_ordered = np.argsort(rank_order(x_transformed))\n    y_rank_max = rankdata(y_transformed, method=\"max\")\n    x_ordered_max_rank = y_rank_max[x_ordered]\n    d_sequence = compute_d_sequence(y_transformed)\n    correlation = compute_xi_coefficient(x_ordered_max_rank)\n\n    return float(correlation)\n</code></pre>"},{"location":"api/#gresit.model_selection.ConvergenceError","title":"<code>ConvergenceError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Convenience class for convergence error.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>class ConvergenceError(Exception):\n    \"\"\"Convenience class for convergence error.\"\"\"\n\n    def __init__(self, message: str = \"Convergence not reached\"):\n        \"\"\"Returns error message.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"api/#gresit.model_selection.ConvergenceError.__init__","title":"<code>__init__(message='Convergence not reached')</code>","text":"<p>Returns error message.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def __init__(self, message: str = \"Convergence not reached\"):\n    \"\"\"Returns error message.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS","title":"<code>MURGS</code>","text":"<p>Multi-Response Group Sparse Additive Mode (MURGS) class.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>class MURGS:\n    \"\"\"Multi-Response Group Sparse Additive Mode (MURGS) class.\"\"\"\n\n    def __init__(self, group_names: list[str] | None = None) -&gt; None:\n        \"\"\"Initializes the object.\"\"\"\n        self.zero_groups: list[bool]\n        self.zero_group_history: dict[float, list[bool]] = {}\n        self.d_g_long: np.ndarray\n        self.p_g: int = 0\n        self.n_tasks: int\n        self.smoothing_matrices: np.ndarray | dict[str, np.ndarray]\n        self.f_g_hat: np.ndarray | dict[str, np.ndarray]\n        self.chosen_lambda: float = 0.0\n        self.lambda_max_value: float | None = None\n        self.steps_till_convergence: int = -1\n        self.max_iter: int = 10000\n        self.tol: float = 1e-8\n        if group_names is not None:\n            self.group_names = group_names\n        else:\n            self.group_names = []\n        self.predicted_vals: np.ndarray\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Print some useful information.\n\n        Returns:\n            str: _description_\n        \"\"\"\n        return f\"MT-GSpAM on {self.p_g} groups.\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Print some useful summary.\n\n        Returns:\n            str: _description_\n        \"\"\"\n        display_limit = 5\n        setting = {\n            \"Number of Groups: \": self.p_g,\n            \"Group sizes homogenous: \": True if isinstance(self.f_g_hat, np.ndarray) else False,\n            \"Steps until convergence: \": self.steps_till_convergence,\n            \"Final lambda: \": self.chosen_lambda,\n            \"Number of non-zero groups: \": self.p_g - np.asanyarray(self.zero_groups).sum(),\n            \"Non-zero group names: \": self.return_nonzero_groups()\n            if len(self.return_nonzero_groups()) &lt;= display_limit\n            else \"Too many to show\",\n        }\n        s = \"\"\n        for info, info_text in setting.items():\n            s += f\"{info}{info_text}\\n\"\n\n        return s\n\n    def return_nonzero_groups(self) -&gt; list[str]:\n        \"\"\"Return the group names of the nonzero groups.\n\n        Returns:\n            list[str] | str: List of nonzero groups with their actual names if given.\n        \"\"\"\n        if len(self.group_names) &gt; 0:\n            nonzero_groups = [\n                group\n                for group, zero_group in zip(self.group_names, self.zero_groups)\n                if not zero_group\n            ]\n        else:\n            nonzero_groups = [\"You neither provided any group names nor ran the model.\"]\n\n        return nonzero_groups\n\n    def precalculate_smooths(\n        self, X_data: np.ndarray | dict[str, np.ndarray], local_regression_method: str = \"kernel\"\n    ) -&gt; np.ndarray | dict[str, np.ndarray]:\n        \"\"\"Precalculate smoother matrices.\n\n        Input may be both `np.ndarray` and `dict`.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): predictor data.\n            local_regression_method (str): Method to use to calculate smoother matrix. Options\n                currently are `\"loess\"` and `\"kernel\"`. When kernel is chosen, the default is set to\n                Gaussian kernel regression with the standard deviation rule for bandwidth selection.\n\n        Returns:\n            np.ndarray | dict[str, np.ndarray]: smooths.\n        \"\"\"\n        if isinstance(X_data, dict):\n            X_data = self._dict_preprocessing(X_data=X_data)\n            smoothing_matrices = {}\n            for group, data in X_data.items():\n                smoothing_matrices[group] = np.zeros((data.shape[0], data.shape[0], data.shape[1]))\n                for j in range(data.shape[1]):\n                    if local_regression_method == \"loess\":\n                        smoothing_matrices[group][:, :, j] = self._make_loess_smoother_matrix(\n                            data[:, j]\n                        )\n                    elif local_regression_method == \"kernel\":\n                        smoothing_matrices[group][:, :, j] = (\n                            self._make_gaussian_kernel_smoother_matrix(data[:, j])\n                        )\n                    else:\n                        raise NotImplementedError(\"This smoothing method is not implemented yet.\")\n        else:\n            smoothing_matrices = np.zeros(\n                (X_data.shape[0], X_data.shape[0], X_data.shape[1], X_data.shape[2]),\n                dtype=np.float64,\n            )\n            inner_smoother: np.ndarray = smoothing_matrices\n            for num_groups in range(X_data.shape[2]):\n                for group_members in range(X_data.shape[1]):\n                    if local_regression_method == \"loess\":\n                        inner_smoother[:, :, group_members, num_groups] = (\n                            self._make_loess_smoother_matrix(X_data[:, group_members, num_groups])\n                        )\n                    elif local_regression_method == \"kernel\":\n                        inner_smoother[:, :, group_members, num_groups] = (\n                            self._make_gaussian_kernel_smoother_matrix(\n                                X_data[:, group_members, num_groups]\n                            )\n                        )\n                    else:\n                        raise NotImplementedError(\"This smoothing method is not implemented yet.\")\n            smoothing_matrices = inner_smoother\n\n        self.smoothing_matrices = smoothing_matrices\n        return smoothing_matrices\n\n    def _init_functions(\n        self, X_data: np.ndarray | dict[str, np.ndarray]\n    ) -&gt; np.ndarray | dict[str, np.ndarray]:\n        \"\"\"Initiate additive functional components.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): predictor data.\n\n        Returns:\n            np.ndarray | dict[str, np.ndarray]: functions initiated.\n        \"\"\"\n        if isinstance(X_data, dict):\n            f_g = {}\n            for group, data in X_data.items():\n                f_g[group] = np.zeros(data.shape + (self.n_tasks,))\n        else:\n            f_g = np.zeros(X_data.shape + (self.n_tasks,))\n        return f_g\n\n    def _set_dg_pg(self, X_data: np.ndarray | dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Set group sizes and number of groups.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): predictor data.\n        \"\"\"\n        if isinstance(X_data, dict):\n            self.d_g_long = np.array(\n                [dat.shape[1] if len(dat.shape) &gt; 1 else 1 for dat in X_data.values()]\n            )\n            # Number of predictors\n            self.p_g = len(self.d_g_long)\n        else:\n            # Number of predictors\n            self.p_g = X_data.shape[-1]\n\n            # Number of variables per group\n            self.d_g_long = np.repeat(X_data.shape[1], self.p_g)\n            if not len(self.group_names) == self.p_g:\n                self.group_names = [str(intgr) for intgr in range(self.p_g)]\n\n    def _dict_preprocessing(self, X_data: dict[str, np.ndarray]) -&gt; dict[str, np.ndarray]:\n        \"\"\"Dict preprocessing dealing with univariate groups.\n\n        Args:\n            X_data (dict[str, np.ndarray]): Predictor dict.\n\n        Returns:\n            dict[str, np.ndarray]: Predictor dict with axis added when univariate.\n        \"\"\"\n        for key, value in X_data.items():\n            X_data[key] = self._assure_two_dim(value)\n        return X_data\n\n    def _get_m_star(self, s_g_ordered: np.ndarray, penalty: float, d_g: int) -&gt; int:\n        \"\"\"Get m* so as to differentiate the cases.\n\n        Cases involve instanecs where more than one group attains the sup-norm.\n\n        Args:\n            s_g_ordered (np.ndarray): Ordered functional norm of group `g`.\n            penalty (float): current penalty parameter.\n            d_g (int): Number of predictors in group.\n\n        Returns:\n            int: m* value.\n        \"\"\"\n        m_criterion = s_g_ordered.cumsum() - penalty * np.sqrt(d_g)\n        m_prefix = np.array(range(1, m_criterion.shape[0] + 1))\n        return int(m_prefix[np.argmax(m_criterion / m_prefix)])\n\n    def smoother_direct_fit(\n        self,\n        g: int,\n        d_g: int,\n        R_g: np.ndarray,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        local_regression_method: str,\n    ) -&gt; np.ndarray:\n        \"\"\"Direct fit of smoothing.\n\n        Args:\n            g (int): _description_\n            d_g (int): _description_\n            R_g (np.ndarray): _description_\n            X_data (np.ndarray): _description_\n            local_regression_method (str): _description_\n\n        Raises:\n            NotImplementedError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        if local_regression_method == \"loess\":\n            smooth_fit = self.loess_direct_fit(g=g, d_g=d_g, R_g=R_g, X_data=X_data)\n        elif local_regression_method == \"kernel\":\n            smooth_fit = self.gaussian_kernel_direct_fit(g=g, d_g=d_g, R_g=R_g, X_data=X_data)\n        else:\n            raise NotImplementedError(\"Smoothing method not implemented.\")\n        return smooth_fit\n\n    def _init_or_insert_functions(\n        self,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        warm_start_f_hat: np.ndarray | dict[str, np.ndarray] | None = None,\n    ) -&gt; np.ndarray | dict[str, np.ndarray]:\n        if warm_start_f_hat is None:\n            f_g = self._init_functions(X_data=X_data)\n        else:\n            f_g = warm_start_f_hat\n        return f_g\n\n    def block_coordinate_descent(\n        self,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        Y_data: np.ndarray,\n        penalty: float,\n        precalculate_smooths: bool = True,\n        smoothers: np.ndarray | dict[str, np.ndarray] | None = None,\n        local_regression_method: str = \"kernel\",\n        warm_start_f_hat: np.ndarray | dict[str, np.ndarray] | None = None,\n    ) -&gt; None:\n        \"\"\"Block coordinate descent for multitask Sparse Group Lasso.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): _description_\n            Y_data (np.ndarray): _description_\n            penalty (float): _description_\n            precalculate_smooths (bool, optional): _description_. Defaults to True.\n            smoothers (np.ndarray | dict[str, np.ndarray] | None, optional): _description_.\n                Defaults to None.\n            local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n            warm_start_f_hat (np.ndarray | dict[str, np.ndarray] | None, optional): _description_.\n                Defaults to None.\n        \"\"\"\n        if smoothers is None:\n            smoothers = self.precalculate_smooths(\n                X_data=X_data, local_regression_method=local_regression_method\n            )\n\n        self.n_tasks = Y_data.shape[1]\n\n        if isinstance(X_data, dict):\n            if not self.group_names:\n                self.group_names = list(X_data.keys())\n            X_data = self._dict_preprocessing(X_data=X_data)\n\n        # Number of predictors and members in groups\n        self._set_dg_pg(X_data=X_data)\n\n        f_g = self._init_or_insert_functions(X_data, warm_start_f_hat)\n\n        divergent_max_runs = 10\n        divergent_runs = 0\n        max_inc_old = 0.0\n        new_functional_norm = np.zeros((self.p_g, self.n_tasks))\n        # Update loop\n        for t in range(self.max_iter):\n            old_functional_norm = new_functional_norm\n            zero_groups = [False] * self.p_g\n            for g in range(self.p_g):\n                d_g = self.d_g_long[g]\n                # R_g_hat has shape (#samples, #tasks)\n                R_g_hat = self.R_g_hat_update(f_g=f_g, Y_data=Y_data, g=g)\n\n                if precalculate_smooths:\n                    smooth_fit = self.predict_from_linear_smoother(\n                        g=g, smoothing_matrices=smoothers, R_g=R_g_hat\n                    )\n                else:\n                    smooth_fit = self.smoother_direct_fit(\n                        g=g,\n                        d_g=d_g,\n                        X_data=X_data,\n                        R_g=R_g_hat,\n                        local_regression_method=local_regression_method,\n                    )\n\n                # # estimate of || Q^(k)R_g^(k) ||\n                # # shape (1, #tasks)\n                omega_g = self.functional_norm(smooth_fit)\n\n                # sort in descending order for each task\n                s_g_ordered = np.sort(omega_g)[::-1]\n\n                # get m* for the case that more than one group attains the sup-norm.\n                m_opt = self._get_m_star(s_g_ordered=s_g_ordered, penalty=penalty, d_g=d_g)\n\n                # calculate nonzero groups\n                zero_groups[g] = omega_g.sum() &lt;= penalty * np.sqrt(d_g)\n                f_g = self.soft_thresholding_update(\n                    g=g,\n                    f_g=f_g,\n                    zero_groups=zero_groups,\n                    smooth_fit=smooth_fit,\n                    s_g_ordered=s_g_ordered,\n                    penalty=penalty,\n                    m_opt=m_opt,\n                )\n\n            new_functional_norm = self.omega_hat(f_g)\n            max_inc: float = np.sqrt(\n                np.square(new_functional_norm - old_functional_norm).sum(axis=(0, 1))\n            )\n            if max_inc_old &lt; max_inc:\n                divergent_runs += 1\n\n            if divergent_runs &gt; divergent_max_runs:\n                raise ConvergenceError(\n                    f\"Model did not converge in group {g}. Penalty: {penalty} too low?\"\n                )\n            max_inc_old = max_inc\n\n            if np.all(max_inc &lt; self.tol / 2):\n                break\n\n        self.f_g_hat = f_g\n        self.zero_groups = zero_groups\n        self.steps_till_convergence = t\n\n    def R_g_hat_update(\n        self, f_g: np.ndarray | dict[str, np.ndarray], Y_data: np.ndarray, g: int\n    ) -&gt; np.ndarray:\n        \"\"\"Update partial residuals.\n\n        Args:\n            f_g (np.ndarray): Current additive components\n                of shape (#samples, #groups, #predictors, #tasks)\n            Y_data (np.ndarray): Response data of shape (#samples, #tasks)\n            g (int): group in question.\n\n        Returns:\n            np.ndarray: Partial Residuals.\n        \"\"\"\n        if isinstance(f_g, np.ndarray):\n            mask = np.ones(self.p_g, bool)\n            mask[g] = False\n            R_g = Y_data - f_g[:, :, mask, :].sum(axis=(1, 2))\n        else:\n            groups = list(f_g.keys())\n            f_interim_sum = np.asanyarray(\n                [value.sum(axis=1) for key, value in f_g.items() if key != groups[g]]\n            )\n            R_g = Y_data - f_interim_sum.sum(axis=0)\n\n        return R_g\n\n    def predict_from_linear_smoother(\n        self,\n        g: int,\n        smoothing_matrices: np.ndarray | dict[str, np.ndarray],\n        R_g: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Local regression fit.\n\n        Args:\n            g (int): group in question.\n            R_g (np.ndarray): Partial residuals should be of\n                shape (#n_samples. #n_predictors, #n_tasks)\n            smoothing_matrices (np.ndarray): Precalculated smoothing matrices\n                of shape (#n_samples, #n_samples #n_groups, #n_predictors)\n\n        Returns:\n            torch.Tensor: Fitted local regressions.\n        \"\"\"\n        if isinstance(smoothing_matrices, np.ndarray):\n            smoothing_matrix = smoothing_matrices[:, :, :, g]\n        else:\n            smoothing_matrix = list(smoothing_matrices.values())[g]\n\n        return torch.einsum(\n            \"ijk, jl -&gt; ikl\", torch.from_numpy(smoothing_matrix), torch.from_numpy(R_g)\n        ).numpy()\n\n    def loess_direct_fit(\n        self, g: int, d_g: int, R_g: np.ndarray, X_data: np.ndarray | dict[str, np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Local regression fit.\n\n        Args:\n            g (int): group in question.\n            d_g (int): Number of predictors in group.\n            R_g (np.ndarray): Partial residuals should be of\n                shape (#n_samples. #n_predictors, #n_tasks)\n            X_data (np.ndarray): Training data of shape (#n_samples. #n_groups, #n_predictors)\n\n        Returns:\n            np.ndarray: Fitted local regressions.\n        \"\"\"\n        if isinstance(X_data, np.ndarray):\n            data = X_data[:, :, g]\n        else:\n            data = list(X_data.values())[g]\n\n        loess_g = np.zeros((R_g.shape[0], d_g, self.n_tasks))\n        for k in range(self.n_tasks):\n            for j in range(d_g):\n                try:\n                    loess_obj = loess(data[:, j], R_g[:, k])\n                except ValueError:\n                    loess_obj = loess(data[:, j], R_g[:, k], surface=\"direct\")\n                loess_obj.fit()\n                loess_g[:, j, k] = loess_obj.outputs.fitted_values\n        return loess_g\n\n    def gaussian_kernel_direct_fit(\n        self, g: int, d_g: int, R_g: np.ndarray, X_data: np.ndarray | dict[str, np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Local regression fit.\n\n        Args:\n            g (int): group in question.\n            d_g (int): Number of predictors in group.\n            R_g (np.ndarray): Partial residuals should be of\n                shape (#n_samples. #n_predictors, #n_tasks)\n            X_data (np.ndarray): Training data of shape (#n_samples. #n_groups, #n_predictors)\n\n        Returns:\n            np.ndarray: Fitted local regressions.\n        \"\"\"\n        if isinstance(X_data, np.ndarray):\n            data = X_data[:, :, g]\n        else:\n            data = list(X_data.values())[g]\n\n        kernel_reg_g = np.zeros((R_g.shape[0], d_g, self.n_tasks))\n        for k in range(self.n_tasks):\n            for j in range(d_g):\n                kernel_reg_g[:, j, k] = self._torch_gaussian_kernel_regression(\n                    x_data=data[:, j],\n                    y_data=R_g[:, k],\n                    bandwidth=self.plugin_bandwidth(x_j=data[:, j]),\n                )\n\n        return kernel_reg_g\n\n    def _torch_gaussian_kernel_regression(\n        self, x_data: np.ndarray, y_data: np.ndarray, bandwidth: float\n    ) -&gt; np.ndarray:\n        \"\"\"Torch implementation of Gaussian kernel regression.\n\n        Args:\n            x_data (np.ndarray): X data\n            y_data (np.ndarray): Y data\n            bandwidth (float): bandwidth parameter\n\n        Returns:\n            np.ndarray: Predicted y values.\n        \"\"\"\n        x_data = torch.from_numpy(x_data)[:, None]  # Reshape for broadcasting\n        weights = torch.exp(\n            -0.5 * ((x_data - x_data.T) / bandwidth) ** 2\n        )  # Pairwise Gaussian weights\n        weights /= weights.sum(dim=1, keepdim=True)  # Normalize weights along each row\n\n        # Weighted sum to get predictions\n        y_pred = weights @ torch.from_numpy(y_data)\n        return y_pred.numpy()\n\n    def _norm_2(self, x: np.ndarray) -&gt; np.ndarray:\n        return np.sqrt(np.square(x).sum(axis=0))\n\n    def functional_norm(self, array_data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Sample estimate of functional norm for arrays of shape.\n\n            (#samples, #n_group_entries, #tasks).\n\n        Args:\n            array_data (np.ndarray): Array input data.\n\n        Returns:\n            np.ndarray: Array of functional norm estimates.\n        \"\"\"\n        return np.sqrt(np.square(self._norm_2(array_data)).sum(axis=0) / array_data.shape[0])\n\n    def soft_thresholding_update(\n        self,\n        g: int,\n        f_g: np.ndarray | dict[str, np.ndarray],\n        zero_groups: list[bool],\n        smooth_fit: np.ndarray,\n        s_g_ordered: np.ndarray,\n        penalty: float,\n        m_opt: np.ndarray,\n    ) -&gt; np.ndarray | dict[str, np.ndarray]:\n        \"\"\"Soft-thresholding update.\n\n        Args:\n            g (int): _description_\n            f_g (np.ndarray | dict[str, np.ndarray]): _description_\n            zero_groups (list[bool]): _description_\n            smooth_fit (np.ndarray): _description_\n            s_g_ordered (np.ndarray): _description_\n            penalty (float): _description_\n            m_opt (np.ndarray): _description_\n\n        Returns:\n            np.ndarray | dict[str, np.ndarray]: _description_\n        \"\"\"\n        if isinstance(f_g, np.ndarray):\n            f_g_hat = f_g[:, :, g, :]\n        else:\n            f_g_hat = list(f_g.values())[g]\n\n        if zero_groups[g]:\n            f_g_hat = np.zeros(f_g_hat.shape)\n        else:\n            f_g_hat = self.update_loop(\n                f_g_hat=f_g_hat,\n                d_g=self.d_g_long[g],\n                smooth_fit=smooth_fit,\n                m_opt=m_opt,\n                s_g_ordered=s_g_ordered,\n                penalty=penalty,\n            )\n        if isinstance(f_g, np.ndarray):\n            f_g[:, :, g, :] = f_g_hat\n        else:\n            f_g[list(f_g.keys())[g]] = f_g_hat\n        return f_g\n\n    def update_loop(\n        self,\n        f_g_hat: np.ndarray,\n        d_g: int,\n        smooth_fit: np.ndarray,\n        s_g_ordered: np.ndarray,\n        penalty: float,\n        m_opt: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Inner update loop.\n\n        Args:\n            f_g_hat (np.ndarray): _description_\n            d_g (int): _description_\n            smooth_fit (np.ndarray): _description_\n            s_g_ordered (np.ndarray): _description_\n            penalty (float): _description_\n            m_opt (np.ndarray): _description_\n\n        Returns:\n            np.ndarray: _description_\n        \"\"\"\n        for k in range(self.n_tasks):\n            for j in range(d_g):\n                # for each task check whether larger or smaller m*\n                if (k + 1) &gt; m_opt:\n                    f_g_hat[:, j, k] = smooth_fit[:, j, k]\n                else:\n                    s_g_sum = s_g_ordered[:m_opt].sum()\n                    f_g_hat[:, j, k] = (\n                        1\n                        / m_opt\n                        * (s_g_sum - np.sqrt(d_g) * penalty)\n                        * smooth_fit[:, j, k]\n                        / s_g_ordered[k]\n                    )\n                f_g_hat[:, j, k] -= f_g_hat[:, j, k].mean()\n        return f_g_hat\n\n    def omega_hat(self, f_g: np.ndarray | dict[str, np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Calculate Omega hat.\n\n        Args:\n            f_g (np.ndarray): _description_\n\n        Returns:\n            np.ndarray: _description_\n        \"\"\"\n        if isinstance(f_g, np.ndarray):\n            p_g = f_g.shape[2]\n            return np.array([self.functional_norm(f_g[:, :, g, :]) for g in range(p_g)])\n        else:\n            return np.array([self.functional_norm(f_g_hat) for f_g_hat in f_g.values()])\n\n    def _standardize_data(\n        self, X_data: np.ndarray | dict[str, np.ndarray]\n    ) -&gt; np.ndarray | dict[str, np.ndarray]:\n        \"\"\"Standardize X data.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): Predictor array or dict.\n\n        Returns:\n            np.ndarray | dict[str, np.ndarray]: Predictor array or dict standardized.\n        \"\"\"\n        if isinstance(X_data, dict):\n            for key, value in X_data.items():\n                X_data[key] = self._assure_two_dim(value)\n                X_data[key] = (value - value.mean(axis=0)) / value.std(axis=0)\n        else:\n            X_data = (X_data - X_data.mean(axis=0)) / X_data.std(axis=0)\n\n        return X_data\n\n    def _assert_group_sizes_match(self, X_data: np.ndarray | dict[str, np.ndarray]) -&gt; bool:\n        \"\"\"Assert whether all groups have same size.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): predictor data.\n\n        Returns:\n            bool: True if all groups have same size\n        \"\"\"\n        if isinstance(X_data, np.ndarray):\n            return False\n        else:\n            return len({dat.shape[1] if len(dat.shape) &gt; 1 else 1 for dat in X_data.values()}) == 1\n\n    def _assure_two_dim(self, arr: np.ndarray) -&gt; np.ndarray:\n        if arr.ndim == 1:\n            return arr[:, np.newaxis]\n        else:\n            return arr\n\n    def fit(\n        self,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        Y_data: np.ndarray,\n        nlambda: int = 30,\n        lambda_min_ratio: float = 0.005,\n        precalculate_smooths: bool = True,\n        local_regression_method: str = \"kernel\",\n    ) -&gt; None:\n        \"\"\"Fit the multitask group SpAM.\n\n        Args:\n            X_data (np.ndarray): _description_\n            Y_data (np.ndarray): _description_\n            nlambda (int, optional): _description_. Defaults to 30.\n            lambda_min_ratio (float, optional): _description_. Defaults to 0.005.\n            precalculate_smooths (bool, optional): _description_. Defaults to True.\n            local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n        \"\"\"\n        Y_data = self._assure_two_dim(Y_data)\n        Y_data -= Y_data.mean(axis=0)\n        if self._assert_group_sizes_match(X_data=X_data):\n            self.group_names = list(X_data.keys())\n            X_data = np.concatenate([data[..., np.newaxis] for data in X_data.values()], axis=2)\n        X_data = self._standardize_data(X_data=X_data)\n\n        smoothers = self.precalculate_smooths(\n            X_data=X_data, local_regression_method=local_regression_method\n        )\n\n        self.select_penalty(\n            X_data=X_data,\n            Y_data=Y_data,\n            nlambda=nlambda,\n            lambda_min_ratio=lambda_min_ratio,\n            precalculate_smooths=precalculate_smooths,\n            smoothers=smoothers,\n            local_regression_method=local_regression_method,\n        )\n\n    def predict(self) -&gt; np.ndarray:\n        \"\"\"Predict the model.\n\n        Returns:\n            np.ndarray: Predicted values of shape (#n_samples, #n_tasks)\n        \"\"\"\n        if isinstance(self.f_g_hat, np.ndarray):\n            self.predicted_vals = self.f_g_hat.sum(axis=(1, 2))\n        else:\n            self.predicted_vals = np.asanyarray(\n                [f_g.sum(axis=1) for f_g in self.f_g_hat.values()]\n            ).sum(axis=0)\n        return self.predicted_vals\n\n    def plugin_bandwidth(self, x_j: np.ndarray) -&gt; float:\n        \"\"\"Plugin bandwidth for Gaussian Kernel.\n\n        Args:\n            x_j (np.ndarray): data.\n\n        Returns:\n            float: Selected bandwidth.\n        \"\"\"\n        return float(0.6 * np.std(x_j) * x_j.shape[0] ** (-1 / 5))\n\n    def _make_gaussian_kernel_smoother_matrix(self, data: np.ndarray) -&gt; np.ndarray:\n        torch_data = torch.from_numpy(data)[:, None]  # Reshape for broadcasting\n        weights = torch.exp(\n            -0.5 * ((torch_data - torch_data.T) / self.plugin_bandwidth(data)) ** 2\n        )  # Pairwise Gaussian weights\n        weights /= weights.sum(dim=1, keepdim=True)  # Normalize weights along each row\n\n        return weights.numpy()\n\n    def _make_loess_smoother_matrix(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Make smoothing matrix for one instance based on loess.\n\n        Args:\n            x (np.ndarray): data.\n\n        Returns:\n            np.ndarray: smoothing matrix\n        \"\"\"\n        n = x.shape[0]\n        smoother = np.zeros((n, n))\n\n        for i in range(n):\n            e_i = np.zeros(n)\n            e_i[i] = 1\n            loess_obj = loess(x, e_i)\n            loess_obj.fit()\n            smoother[:, i] = loess_obj.outputs.fitted_values\n\n        return smoother\n\n    def gcv(self, Y_data: np.ndarray) -&gt; float:\n        \"\"\"Generalized cross-validation.\n\n        Args:\n            Y_data (np.ndarray): Response data of shape (#n_samples. #n_tasks).\n\n        Returns:\n            float: Value of GCV.\n        \"\"\"\n        n = Y_data.shape[0]\n        numerator: float = np.einsum(\"ij-&gt;\", self._quadratic_loss(Y_data))\n        denominator: float = np.square(np.square(n * self.n_tasks) - n * self.n_tasks * self._df())\n        gcv = numerator / denominator\n        return gcv\n\n    def _quadratic_loss(self, Y_data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Current residual fit in terms of LS criterion.\n\n        Args:\n            Y_data (np.ndarray): _description_\n\n        Returns:\n            np.ndarray: _description_\n        \"\"\"\n        if isinstance(self.f_g_hat, np.ndarray):\n            Q_g = np.square(Y_data - self.f_g_hat.sum(axis=(1, 2)))\n        else:\n            Q_g = np.square(\n                Y_data\n                - np.array([entry.sum(axis=1) for entry in self.f_g_hat.values()]).sum(axis=0)\n            )\n\n        return Q_g\n\n    def _df(self) -&gt; float:\n        \"\"\"Effective degrees of freedom in terms of trace of smoother matrix.\n\n        K multiplication is due to the single-task nature of the predictors.\n        In case of multitask this will need to be adapted in the future.\n\n        Returns:\n            float: K * sum_g^p sum_j^d_g trace(S_j^k) I(||f_j^k|| not 0)\n        \"\"\"\n        v_jk = {}\n        for g, d_g in enumerate(self.d_g_long):\n            v_jk[g] = np.zeros(d_g)\n            for j in range(d_g):\n                if self.zero_groups[g]:\n                    continue\n                elif isinstance(self.smoothing_matrices, np.ndarray):\n                    v_jk[g][j] = np.einsum(\n                        \"ii\",\n                        self.smoothing_matrices[:, :, j, g],\n                    )\n                else:\n                    v_jk[g][j] = np.einsum(\n                        \"ii\",\n                        self.smoothing_matrices[list(self.smoothing_matrices.keys())[g]][:, :, j],\n                    )\n\n        v_jk_sum: float = np.asanyarray([df.sum() for df in v_jk.values()]).sum()\n        v_jk_sum_sum = self.n_tasks * v_jk_sum\n        return v_jk_sum_sum\n\n    def select_penalty(\n        self,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        Y_data: np.ndarray,\n        nlambda: int = 30,\n        lambda_min_ratio: float = 5e-3,\n        smoothers: np.ndarray | dict[str, np.ndarray] | None = None,\n        precalculate_smooths: bool = True,\n        local_regression_method: str = \"kernel\",\n    ) -&gt; None:\n        \"\"\"GCV model selection procedure.\n\n        Args:\n            X_data (np.ndarray): _description_\n            Y_data (np.ndarray): _description_\n            nlambda (int, optional): _description_. Defaults to 30.\n            lambda_min_ratio (float, optional): _description_. Defaults to 5e-3.\n            precalculate_smooths (bool, optional): _description_. Defaults to True.\n            smoothers (np.ndarray | dict[str, np.ndarray] | None): _description_. Defaults to None.\n            local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n        \"\"\"\n        # This is only ever be necessary if the function is called outside fit()\n        if smoothers is None:\n            smoothers = self.precalculate_smooths(\n                X_data=X_data, local_regression_method=local_regression_method\n            )\n\n        self._find_lambda_max_value(\n            X_data=X_data,\n            Y_data=Y_data,\n            smoothers=smoothers,\n            precalculate_smooths=precalculate_smooths,\n            local_regression_method=local_regression_method,\n        )\n        lambda_scale = np.exp(\n            np.linspace(start=np.log(1), stop=np.log(lambda_min_ratio), num=nlambda)\n        )\n        self.lambda_values = lambda_scale * self.lambda_max_value\n\n        gcv_values = np.empty(len(self.lambda_values))\n        f_hat_from_previous_lambda = self._init_functions(X_data=X_data)\n        current_min = np.inf\n        for i, lambda_value in enumerate(self.lambda_values):\n            self._progressBar(i, len(self.lambda_values) - 1, suffix=\"Finding optimal lambda\")\n            try:\n                self.block_coordinate_descent(\n                    X_data=X_data,\n                    Y_data=Y_data,\n                    penalty=lambda_value,\n                    precalculate_smooths=precalculate_smooths,\n                    smoothers=smoothers,\n                    warm_start_f_hat=f_hat_from_previous_lambda,\n                    local_regression_method=local_regression_method,\n                )\n                gcv_values[i] = self.gcv(Y_data=Y_data)\n                f_hat_from_previous_lambda = self.f_g_hat\n                self.zero_group_history[lambda_value] = self.zero_groups\n\n                if gcv_values[i] &lt; current_min:\n                    final_f_hat = self.f_g_hat  # Save for later\n                    final_zero_groups = self.zero_groups  # Save for later\n                    final_steps_till_convergence = self.steps_till_convergence\n                    current_min = gcv_values[i]\n\n            except ConvergenceError:  # Stop if no convergence\n                if i == 0:  # If this is negative even max_lambda didn't converge.\n                    raise ValueError(\n                        \"No lambda value converged. Something wrong with causal order?\"\n                    )\n                break\n\n        self.chosen_lambda = self.lambda_values[np.argmin(gcv_values)]\n        self.f_g_hat = final_f_hat\n        self.zero_groups = final_zero_groups\n        self.steps_till_convergence = final_steps_till_convergence\n        self.gcv_values = gcv_values\n\n    def _progressBar(self, count_value: float, total: float, suffix: str = \"\") -&gt; None:\n        bar_length = 100\n        filled_up_Length = int(round(bar_length * count_value / float(total)))\n        percentage = round(100.0 * count_value / float(total), 1)\n        bar = \"=\" * filled_up_Length + \"-\" * (bar_length - filled_up_Length)\n        sys.stdout.write(f\"[{bar}] {percentage}% ...{suffix}\\r\")\n        sys.stdout.flush()\n\n    def _find_lambda_max_value(\n        self,\n        X_data: np.ndarray | dict[str, np.ndarray],\n        Y_data: np.ndarray,\n        smoothers: np.ndarray | dict[str, np.ndarray] | None = None,\n        precalculate_smooths: bool = True,\n        local_regression_method: str = \"kernel\",\n    ) -&gt; None:\n        \"\"\"Identifies largest smallest penalty that just renders all groups \\\n\n            to be shrunken to zero.\n\n        Args:\n            X_data (np.ndarray | dict[str, np.ndarray]): Predictors.\n            Y_data (np.ndarray): Targets.\n            smoothers (np.ndarray | dict[str, np.ndarray] | None, optional): Pre-calculated\n                Smoothers. Defaults to None.\n            precalculate_smooths (bool): Whether to precalculate smooths. Is just for completeness.\n                Typically the smooths will have been calculated and provided externally when calling\n                this function.\n            local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n        \"\"\"\n        self.n_tasks = Y_data.shape[1]\n\n        if isinstance(X_data, dict):\n            X_data = self._dict_preprocessing(X_data=X_data)\n\n        # Number of predictors and members in groups\n        self._set_dg_pg(X_data=X_data)\n\n        f_g = self._init_functions(X_data=X_data)\n\n        if smoothers is None:\n            smoothers = self.precalculate_smooths(\n                X_data=X_data, local_regression_method=local_regression_method\n            )\n\n        omega_g_sum = np.zeros(self.p_g)\n        for g in range(self.p_g):\n            d_g = self.d_g_long[g]\n            # R_g_hat has shape (#samples, #tasks)\n            R_g_hat = self.R_g_hat_update(f_g=f_g, Y_data=Y_data, g=g)\n\n            if precalculate_smooths:\n                smooth_fit = self.predict_from_linear_smoother(\n                    g=g, smoothing_matrices=smoothers, R_g=R_g_hat\n                )\n            elif local_regression_method == \"loess\":\n                smooth_fit = self.loess_direct_fit(g=g, d_g=d_g, R_g=R_g_hat, X_data=X_data)\n            elif local_regression_method == \"kernel\":\n                smooth_fit = self.gaussian_kernel_direct_fit(\n                    g=g, d_g=d_g, R_g=R_g_hat, X_data=X_data\n                )\n            else:\n                raise NotImplementedError(\"Smoothing method not implemented.\")\n\n            # estimate of || Q^(k)R_g^(k) ||\n            # shape (#predictors, #tasks)\n            omega_g = self.functional_norm(smooth_fit)\n            # lambda_max_value must be at least omega_g.sum()/np.sqrt(d_g)\n            omega_g_sum[g] = omega_g.sum()\n\n        self.lambda_max_value = np.ceil(np.max(omega_g_sum / np.sqrt(self.d_g_long)))\n\n    def plot_gcv_path(self) -&gt; None:\n        \"\"\"Plot GCV path.\"\"\"\n        if self.gcv_values is None:\n            raise ValueError(\"No GCV values available. Run model selection first.\")\n        _, ax = plt.subplots()\n        ax.plot(self.lambda_values, self.gcv_values)\n        plt.axvline(\n            x=self.chosen_lambda,\n            color=\"red\",\n            linestyle=\"--\",\n            alpha=0.35,\n        )\n        plt.xlabel(\"lambda\")\n        plt.ylabel(\"gcv\")\n        plt.show()\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.R_g_hat_update","title":"<code>R_g_hat_update(f_g, Y_data, g)</code>","text":"<p>Update partial residuals.</p> <p>Parameters:</p> Name Type Description Default <code>f_g</code> <code>ndarray</code> <p>Current additive components of shape (#samples, #groups, #predictors, #tasks)</p> required <code>Y_data</code> <code>ndarray</code> <p>Response data of shape (#samples, #tasks)</p> required <code>g</code> <code>int</code> <p>group in question.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Partial Residuals.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def R_g_hat_update(\n    self, f_g: np.ndarray | dict[str, np.ndarray], Y_data: np.ndarray, g: int\n) -&gt; np.ndarray:\n    \"\"\"Update partial residuals.\n\n    Args:\n        f_g (np.ndarray): Current additive components\n            of shape (#samples, #groups, #predictors, #tasks)\n        Y_data (np.ndarray): Response data of shape (#samples, #tasks)\n        g (int): group in question.\n\n    Returns:\n        np.ndarray: Partial Residuals.\n    \"\"\"\n    if isinstance(f_g, np.ndarray):\n        mask = np.ones(self.p_g, bool)\n        mask[g] = False\n        R_g = Y_data - f_g[:, :, mask, :].sum(axis=(1, 2))\n    else:\n        groups = list(f_g.keys())\n        f_interim_sum = np.asanyarray(\n            [value.sum(axis=1) for key, value in f_g.items() if key != groups[g]]\n        )\n        R_g = Y_data - f_interim_sum.sum(axis=0)\n\n    return R_g\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.__init__","title":"<code>__init__(group_names=None)</code>","text":"<p>Initializes the object.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def __init__(self, group_names: list[str] | None = None) -&gt; None:\n    \"\"\"Initializes the object.\"\"\"\n    self.zero_groups: list[bool]\n    self.zero_group_history: dict[float, list[bool]] = {}\n    self.d_g_long: np.ndarray\n    self.p_g: int = 0\n    self.n_tasks: int\n    self.smoothing_matrices: np.ndarray | dict[str, np.ndarray]\n    self.f_g_hat: np.ndarray | dict[str, np.ndarray]\n    self.chosen_lambda: float = 0.0\n    self.lambda_max_value: float | None = None\n    self.steps_till_convergence: int = -1\n    self.max_iter: int = 10000\n    self.tol: float = 1e-8\n    if group_names is not None:\n        self.group_names = group_names\n    else:\n        self.group_names = []\n    self.predicted_vals: np.ndarray\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.__repr__","title":"<code>__repr__()</code>","text":"<p>Print some useful information.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Print some useful information.\n\n    Returns:\n        str: _description_\n    \"\"\"\n    return f\"MT-GSpAM on {self.p_g} groups.\"\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.__str__","title":"<code>__str__()</code>","text":"<p>Print some useful summary.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Print some useful summary.\n\n    Returns:\n        str: _description_\n    \"\"\"\n    display_limit = 5\n    setting = {\n        \"Number of Groups: \": self.p_g,\n        \"Group sizes homogenous: \": True if isinstance(self.f_g_hat, np.ndarray) else False,\n        \"Steps until convergence: \": self.steps_till_convergence,\n        \"Final lambda: \": self.chosen_lambda,\n        \"Number of non-zero groups: \": self.p_g - np.asanyarray(self.zero_groups).sum(),\n        \"Non-zero group names: \": self.return_nonzero_groups()\n        if len(self.return_nonzero_groups()) &lt;= display_limit\n        else \"Too many to show\",\n    }\n    s = \"\"\n    for info, info_text in setting.items():\n        s += f\"{info}{info_text}\\n\"\n\n    return s\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.block_coordinate_descent","title":"<code>block_coordinate_descent(X_data, Y_data, penalty, precalculate_smooths=True, smoothers=None, local_regression_method='kernel', warm_start_f_hat=None)</code>","text":"<p>Block coordinate descent for multitask Sparse Group Lasso.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>ndarray | dict[str, ndarray]</code> <p>description</p> required <code>Y_data</code> <code>ndarray</code> <p>description</p> required <code>penalty</code> <code>float</code> <p>description</p> required <code>precalculate_smooths</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>smoothers</code> <code>ndarray | dict[str, ndarray] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>local_regression_method</code> <code>str</code> <p>Defaults to \"loess\". Other options currently: \"kernel\".</p> <code>'kernel'</code> <code>warm_start_f_hat</code> <code>ndarray | dict[str, ndarray] | None</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>gresit/model_selection.py</code> <pre><code>def block_coordinate_descent(\n    self,\n    X_data: np.ndarray | dict[str, np.ndarray],\n    Y_data: np.ndarray,\n    penalty: float,\n    precalculate_smooths: bool = True,\n    smoothers: np.ndarray | dict[str, np.ndarray] | None = None,\n    local_regression_method: str = \"kernel\",\n    warm_start_f_hat: np.ndarray | dict[str, np.ndarray] | None = None,\n) -&gt; None:\n    \"\"\"Block coordinate descent for multitask Sparse Group Lasso.\n\n    Args:\n        X_data (np.ndarray | dict[str, np.ndarray]): _description_\n        Y_data (np.ndarray): _description_\n        penalty (float): _description_\n        precalculate_smooths (bool, optional): _description_. Defaults to True.\n        smoothers (np.ndarray | dict[str, np.ndarray] | None, optional): _description_.\n            Defaults to None.\n        local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n        warm_start_f_hat (np.ndarray | dict[str, np.ndarray] | None, optional): _description_.\n            Defaults to None.\n    \"\"\"\n    if smoothers is None:\n        smoothers = self.precalculate_smooths(\n            X_data=X_data, local_regression_method=local_regression_method\n        )\n\n    self.n_tasks = Y_data.shape[1]\n\n    if isinstance(X_data, dict):\n        if not self.group_names:\n            self.group_names = list(X_data.keys())\n        X_data = self._dict_preprocessing(X_data=X_data)\n\n    # Number of predictors and members in groups\n    self._set_dg_pg(X_data=X_data)\n\n    f_g = self._init_or_insert_functions(X_data, warm_start_f_hat)\n\n    divergent_max_runs = 10\n    divergent_runs = 0\n    max_inc_old = 0.0\n    new_functional_norm = np.zeros((self.p_g, self.n_tasks))\n    # Update loop\n    for t in range(self.max_iter):\n        old_functional_norm = new_functional_norm\n        zero_groups = [False] * self.p_g\n        for g in range(self.p_g):\n            d_g = self.d_g_long[g]\n            # R_g_hat has shape (#samples, #tasks)\n            R_g_hat = self.R_g_hat_update(f_g=f_g, Y_data=Y_data, g=g)\n\n            if precalculate_smooths:\n                smooth_fit = self.predict_from_linear_smoother(\n                    g=g, smoothing_matrices=smoothers, R_g=R_g_hat\n                )\n            else:\n                smooth_fit = self.smoother_direct_fit(\n                    g=g,\n                    d_g=d_g,\n                    X_data=X_data,\n                    R_g=R_g_hat,\n                    local_regression_method=local_regression_method,\n                )\n\n            # # estimate of || Q^(k)R_g^(k) ||\n            # # shape (1, #tasks)\n            omega_g = self.functional_norm(smooth_fit)\n\n            # sort in descending order for each task\n            s_g_ordered = np.sort(omega_g)[::-1]\n\n            # get m* for the case that more than one group attains the sup-norm.\n            m_opt = self._get_m_star(s_g_ordered=s_g_ordered, penalty=penalty, d_g=d_g)\n\n            # calculate nonzero groups\n            zero_groups[g] = omega_g.sum() &lt;= penalty * np.sqrt(d_g)\n            f_g = self.soft_thresholding_update(\n                g=g,\n                f_g=f_g,\n                zero_groups=zero_groups,\n                smooth_fit=smooth_fit,\n                s_g_ordered=s_g_ordered,\n                penalty=penalty,\n                m_opt=m_opt,\n            )\n\n        new_functional_norm = self.omega_hat(f_g)\n        max_inc: float = np.sqrt(\n            np.square(new_functional_norm - old_functional_norm).sum(axis=(0, 1))\n        )\n        if max_inc_old &lt; max_inc:\n            divergent_runs += 1\n\n        if divergent_runs &gt; divergent_max_runs:\n            raise ConvergenceError(\n                f\"Model did not converge in group {g}. Penalty: {penalty} too low?\"\n            )\n        max_inc_old = max_inc\n\n        if np.all(max_inc &lt; self.tol / 2):\n            break\n\n    self.f_g_hat = f_g\n    self.zero_groups = zero_groups\n    self.steps_till_convergence = t\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.fit","title":"<code>fit(X_data, Y_data, nlambda=30, lambda_min_ratio=0.005, precalculate_smooths=True, local_regression_method='kernel')</code>","text":"<p>Fit the multitask group SpAM.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>ndarray</code> <p>description</p> required <code>Y_data</code> <code>ndarray</code> <p>description</p> required <code>nlambda</code> <code>int</code> <p>description. Defaults to 30.</p> <code>30</code> <code>lambda_min_ratio</code> <code>float</code> <p>description. Defaults to 0.005.</p> <code>0.005</code> <code>precalculate_smooths</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>local_regression_method</code> <code>str</code> <p>Defaults to \"loess\". Other options currently: \"kernel\".</p> <code>'kernel'</code> Source code in <code>gresit/model_selection.py</code> <pre><code>def fit(\n    self,\n    X_data: np.ndarray | dict[str, np.ndarray],\n    Y_data: np.ndarray,\n    nlambda: int = 30,\n    lambda_min_ratio: float = 0.005,\n    precalculate_smooths: bool = True,\n    local_regression_method: str = \"kernel\",\n) -&gt; None:\n    \"\"\"Fit the multitask group SpAM.\n\n    Args:\n        X_data (np.ndarray): _description_\n        Y_data (np.ndarray): _description_\n        nlambda (int, optional): _description_. Defaults to 30.\n        lambda_min_ratio (float, optional): _description_. Defaults to 0.005.\n        precalculate_smooths (bool, optional): _description_. Defaults to True.\n        local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n    \"\"\"\n    Y_data = self._assure_two_dim(Y_data)\n    Y_data -= Y_data.mean(axis=0)\n    if self._assert_group_sizes_match(X_data=X_data):\n        self.group_names = list(X_data.keys())\n        X_data = np.concatenate([data[..., np.newaxis] for data in X_data.values()], axis=2)\n    X_data = self._standardize_data(X_data=X_data)\n\n    smoothers = self.precalculate_smooths(\n        X_data=X_data, local_regression_method=local_regression_method\n    )\n\n    self.select_penalty(\n        X_data=X_data,\n        Y_data=Y_data,\n        nlambda=nlambda,\n        lambda_min_ratio=lambda_min_ratio,\n        precalculate_smooths=precalculate_smooths,\n        smoothers=smoothers,\n        local_regression_method=local_regression_method,\n    )\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.functional_norm","title":"<code>functional_norm(array_data)</code>","text":"<p>Sample estimate of functional norm for arrays of shape.</p> <pre><code>(#samples, #n_group_entries, #tasks).\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>array_data</code> <code>ndarray</code> <p>Array input data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of functional norm estimates.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def functional_norm(self, array_data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Sample estimate of functional norm for arrays of shape.\n\n        (#samples, #n_group_entries, #tasks).\n\n    Args:\n        array_data (np.ndarray): Array input data.\n\n    Returns:\n        np.ndarray: Array of functional norm estimates.\n    \"\"\"\n    return np.sqrt(np.square(self._norm_2(array_data)).sum(axis=0) / array_data.shape[0])\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.gaussian_kernel_direct_fit","title":"<code>gaussian_kernel_direct_fit(g, d_g, R_g, X_data)</code>","text":"<p>Local regression fit.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>int</code> <p>group in question.</p> required <code>d_g</code> <code>int</code> <p>Number of predictors in group.</p> required <code>R_g</code> <code>ndarray</code> <p>Partial residuals should be of shape (#n_samples. #n_predictors, #n_tasks)</p> required <code>X_data</code> <code>ndarray</code> <p>Training data of shape (#n_samples. #n_groups, #n_predictors)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Fitted local regressions.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def gaussian_kernel_direct_fit(\n    self, g: int, d_g: int, R_g: np.ndarray, X_data: np.ndarray | dict[str, np.ndarray]\n) -&gt; np.ndarray:\n    \"\"\"Local regression fit.\n\n    Args:\n        g (int): group in question.\n        d_g (int): Number of predictors in group.\n        R_g (np.ndarray): Partial residuals should be of\n            shape (#n_samples. #n_predictors, #n_tasks)\n        X_data (np.ndarray): Training data of shape (#n_samples. #n_groups, #n_predictors)\n\n    Returns:\n        np.ndarray: Fitted local regressions.\n    \"\"\"\n    if isinstance(X_data, np.ndarray):\n        data = X_data[:, :, g]\n    else:\n        data = list(X_data.values())[g]\n\n    kernel_reg_g = np.zeros((R_g.shape[0], d_g, self.n_tasks))\n    for k in range(self.n_tasks):\n        for j in range(d_g):\n            kernel_reg_g[:, j, k] = self._torch_gaussian_kernel_regression(\n                x_data=data[:, j],\n                y_data=R_g[:, k],\n                bandwidth=self.plugin_bandwidth(x_j=data[:, j]),\n            )\n\n    return kernel_reg_g\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.gcv","title":"<code>gcv(Y_data)</code>","text":"<p>Generalized cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>Y_data</code> <code>ndarray</code> <p>Response data of shape (#n_samples. #n_tasks).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Value of GCV.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def gcv(self, Y_data: np.ndarray) -&gt; float:\n    \"\"\"Generalized cross-validation.\n\n    Args:\n        Y_data (np.ndarray): Response data of shape (#n_samples. #n_tasks).\n\n    Returns:\n        float: Value of GCV.\n    \"\"\"\n    n = Y_data.shape[0]\n    numerator: float = np.einsum(\"ij-&gt;\", self._quadratic_loss(Y_data))\n    denominator: float = np.square(np.square(n * self.n_tasks) - n * self.n_tasks * self._df())\n    gcv = numerator / denominator\n    return gcv\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.loess_direct_fit","title":"<code>loess_direct_fit(g, d_g, R_g, X_data)</code>","text":"<p>Local regression fit.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>int</code> <p>group in question.</p> required <code>d_g</code> <code>int</code> <p>Number of predictors in group.</p> required <code>R_g</code> <code>ndarray</code> <p>Partial residuals should be of shape (#n_samples. #n_predictors, #n_tasks)</p> required <code>X_data</code> <code>ndarray</code> <p>Training data of shape (#n_samples. #n_groups, #n_predictors)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Fitted local regressions.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def loess_direct_fit(\n    self, g: int, d_g: int, R_g: np.ndarray, X_data: np.ndarray | dict[str, np.ndarray]\n) -&gt; np.ndarray:\n    \"\"\"Local regression fit.\n\n    Args:\n        g (int): group in question.\n        d_g (int): Number of predictors in group.\n        R_g (np.ndarray): Partial residuals should be of\n            shape (#n_samples. #n_predictors, #n_tasks)\n        X_data (np.ndarray): Training data of shape (#n_samples. #n_groups, #n_predictors)\n\n    Returns:\n        np.ndarray: Fitted local regressions.\n    \"\"\"\n    if isinstance(X_data, np.ndarray):\n        data = X_data[:, :, g]\n    else:\n        data = list(X_data.values())[g]\n\n    loess_g = np.zeros((R_g.shape[0], d_g, self.n_tasks))\n    for k in range(self.n_tasks):\n        for j in range(d_g):\n            try:\n                loess_obj = loess(data[:, j], R_g[:, k])\n            except ValueError:\n                loess_obj = loess(data[:, j], R_g[:, k], surface=\"direct\")\n            loess_obj.fit()\n            loess_g[:, j, k] = loess_obj.outputs.fitted_values\n    return loess_g\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.omega_hat","title":"<code>omega_hat(f_g)</code>","text":"<p>Calculate Omega hat.</p> <p>Parameters:</p> Name Type Description Default <code>f_g</code> <code>ndarray</code> <p>description</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def omega_hat(self, f_g: np.ndarray | dict[str, np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Calculate Omega hat.\n\n    Args:\n        f_g (np.ndarray): _description_\n\n    Returns:\n        np.ndarray: _description_\n    \"\"\"\n    if isinstance(f_g, np.ndarray):\n        p_g = f_g.shape[2]\n        return np.array([self.functional_norm(f_g[:, :, g, :]) for g in range(p_g)])\n    else:\n        return np.array([self.functional_norm(f_g_hat) for f_g_hat in f_g.values()])\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.plot_gcv_path","title":"<code>plot_gcv_path()</code>","text":"<p>Plot GCV path.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def plot_gcv_path(self) -&gt; None:\n    \"\"\"Plot GCV path.\"\"\"\n    if self.gcv_values is None:\n        raise ValueError(\"No GCV values available. Run model selection first.\")\n    _, ax = plt.subplots()\n    ax.plot(self.lambda_values, self.gcv_values)\n    plt.axvline(\n        x=self.chosen_lambda,\n        color=\"red\",\n        linestyle=\"--\",\n        alpha=0.35,\n    )\n    plt.xlabel(\"lambda\")\n    plt.ylabel(\"gcv\")\n    plt.show()\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.plugin_bandwidth","title":"<code>plugin_bandwidth(x_j)</code>","text":"<p>Plugin bandwidth for Gaussian Kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x_j</code> <code>ndarray</code> <p>data.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Selected bandwidth.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def plugin_bandwidth(self, x_j: np.ndarray) -&gt; float:\n    \"\"\"Plugin bandwidth for Gaussian Kernel.\n\n    Args:\n        x_j (np.ndarray): data.\n\n    Returns:\n        float: Selected bandwidth.\n    \"\"\"\n    return float(0.6 * np.std(x_j) * x_j.shape[0] ** (-1 / 5))\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.precalculate_smooths","title":"<code>precalculate_smooths(X_data, local_regression_method='kernel')</code>","text":"<p>Precalculate smoother matrices.</p> <p>Input may be both <code>np.ndarray</code> and <code>dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>ndarray | dict[str, ndarray]</code> <p>predictor data.</p> required <code>local_regression_method</code> <code>str</code> <p>Method to use to calculate smoother matrix. Options currently are <code>\"loess\"</code> and <code>\"kernel\"</code>. When kernel is chosen, the default is set to Gaussian kernel regression with the standard deviation rule for bandwidth selection.</p> <code>'kernel'</code> <p>Returns:</p> Type Description <code>ndarray | dict[str, ndarray]</code> <p>np.ndarray | dict[str, np.ndarray]: smooths.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def precalculate_smooths(\n    self, X_data: np.ndarray | dict[str, np.ndarray], local_regression_method: str = \"kernel\"\n) -&gt; np.ndarray | dict[str, np.ndarray]:\n    \"\"\"Precalculate smoother matrices.\n\n    Input may be both `np.ndarray` and `dict`.\n\n    Args:\n        X_data (np.ndarray | dict[str, np.ndarray]): predictor data.\n        local_regression_method (str): Method to use to calculate smoother matrix. Options\n            currently are `\"loess\"` and `\"kernel\"`. When kernel is chosen, the default is set to\n            Gaussian kernel regression with the standard deviation rule for bandwidth selection.\n\n    Returns:\n        np.ndarray | dict[str, np.ndarray]: smooths.\n    \"\"\"\n    if isinstance(X_data, dict):\n        X_data = self._dict_preprocessing(X_data=X_data)\n        smoothing_matrices = {}\n        for group, data in X_data.items():\n            smoothing_matrices[group] = np.zeros((data.shape[0], data.shape[0], data.shape[1]))\n            for j in range(data.shape[1]):\n                if local_regression_method == \"loess\":\n                    smoothing_matrices[group][:, :, j] = self._make_loess_smoother_matrix(\n                        data[:, j]\n                    )\n                elif local_regression_method == \"kernel\":\n                    smoothing_matrices[group][:, :, j] = (\n                        self._make_gaussian_kernel_smoother_matrix(data[:, j])\n                    )\n                else:\n                    raise NotImplementedError(\"This smoothing method is not implemented yet.\")\n    else:\n        smoothing_matrices = np.zeros(\n            (X_data.shape[0], X_data.shape[0], X_data.shape[1], X_data.shape[2]),\n            dtype=np.float64,\n        )\n        inner_smoother: np.ndarray = smoothing_matrices\n        for num_groups in range(X_data.shape[2]):\n            for group_members in range(X_data.shape[1]):\n                if local_regression_method == \"loess\":\n                    inner_smoother[:, :, group_members, num_groups] = (\n                        self._make_loess_smoother_matrix(X_data[:, group_members, num_groups])\n                    )\n                elif local_regression_method == \"kernel\":\n                    inner_smoother[:, :, group_members, num_groups] = (\n                        self._make_gaussian_kernel_smoother_matrix(\n                            X_data[:, group_members, num_groups]\n                        )\n                    )\n                else:\n                    raise NotImplementedError(\"This smoothing method is not implemented yet.\")\n        smoothing_matrices = inner_smoother\n\n    self.smoothing_matrices = smoothing_matrices\n    return smoothing_matrices\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.predict","title":"<code>predict()</code>","text":"<p>Predict the model.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted values of shape (#n_samples, #n_tasks)</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def predict(self) -&gt; np.ndarray:\n    \"\"\"Predict the model.\n\n    Returns:\n        np.ndarray: Predicted values of shape (#n_samples, #n_tasks)\n    \"\"\"\n    if isinstance(self.f_g_hat, np.ndarray):\n        self.predicted_vals = self.f_g_hat.sum(axis=(1, 2))\n    else:\n        self.predicted_vals = np.asanyarray(\n            [f_g.sum(axis=1) for f_g in self.f_g_hat.values()]\n        ).sum(axis=0)\n    return self.predicted_vals\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.predict_from_linear_smoother","title":"<code>predict_from_linear_smoother(g, smoothing_matrices, R_g)</code>","text":"<p>Local regression fit.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>int</code> <p>group in question.</p> required <code>R_g</code> <code>ndarray</code> <p>Partial residuals should be of shape (#n_samples. #n_predictors, #n_tasks)</p> required <code>smoothing_matrices</code> <code>ndarray</code> <p>Precalculated smoothing matrices of shape (#n_samples, #n_samples #n_groups, #n_predictors)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>torch.Tensor: Fitted local regressions.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def predict_from_linear_smoother(\n    self,\n    g: int,\n    smoothing_matrices: np.ndarray | dict[str, np.ndarray],\n    R_g: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Local regression fit.\n\n    Args:\n        g (int): group in question.\n        R_g (np.ndarray): Partial residuals should be of\n            shape (#n_samples. #n_predictors, #n_tasks)\n        smoothing_matrices (np.ndarray): Precalculated smoothing matrices\n            of shape (#n_samples, #n_samples #n_groups, #n_predictors)\n\n    Returns:\n        torch.Tensor: Fitted local regressions.\n    \"\"\"\n    if isinstance(smoothing_matrices, np.ndarray):\n        smoothing_matrix = smoothing_matrices[:, :, :, g]\n    else:\n        smoothing_matrix = list(smoothing_matrices.values())[g]\n\n    return torch.einsum(\n        \"ijk, jl -&gt; ikl\", torch.from_numpy(smoothing_matrix), torch.from_numpy(R_g)\n    ).numpy()\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.return_nonzero_groups","title":"<code>return_nonzero_groups()</code>","text":"<p>Return the group names of the nonzero groups.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str] | str: List of nonzero groups with their actual names if given.</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def return_nonzero_groups(self) -&gt; list[str]:\n    \"\"\"Return the group names of the nonzero groups.\n\n    Returns:\n        list[str] | str: List of nonzero groups with their actual names if given.\n    \"\"\"\n    if len(self.group_names) &gt; 0:\n        nonzero_groups = [\n            group\n            for group, zero_group in zip(self.group_names, self.zero_groups)\n            if not zero_group\n        ]\n    else:\n        nonzero_groups = [\"You neither provided any group names nor ran the model.\"]\n\n    return nonzero_groups\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.select_penalty","title":"<code>select_penalty(X_data, Y_data, nlambda=30, lambda_min_ratio=0.005, smoothers=None, precalculate_smooths=True, local_regression_method='kernel')</code>","text":"<p>GCV model selection procedure.</p> <p>Parameters:</p> Name Type Description Default <code>X_data</code> <code>ndarray</code> <p>description</p> required <code>Y_data</code> <code>ndarray</code> <p>description</p> required <code>nlambda</code> <code>int</code> <p>description. Defaults to 30.</p> <code>30</code> <code>lambda_min_ratio</code> <code>float</code> <p>description. Defaults to 5e-3.</p> <code>0.005</code> <code>precalculate_smooths</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>smoothers</code> <code>ndarray | dict[str, ndarray] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>local_regression_method</code> <code>str</code> <p>Defaults to \"loess\". Other options currently: \"kernel\".</p> <code>'kernel'</code> Source code in <code>gresit/model_selection.py</code> <pre><code>def select_penalty(\n    self,\n    X_data: np.ndarray | dict[str, np.ndarray],\n    Y_data: np.ndarray,\n    nlambda: int = 30,\n    lambda_min_ratio: float = 5e-3,\n    smoothers: np.ndarray | dict[str, np.ndarray] | None = None,\n    precalculate_smooths: bool = True,\n    local_regression_method: str = \"kernel\",\n) -&gt; None:\n    \"\"\"GCV model selection procedure.\n\n    Args:\n        X_data (np.ndarray): _description_\n        Y_data (np.ndarray): _description_\n        nlambda (int, optional): _description_. Defaults to 30.\n        lambda_min_ratio (float, optional): _description_. Defaults to 5e-3.\n        precalculate_smooths (bool, optional): _description_. Defaults to True.\n        smoothers (np.ndarray | dict[str, np.ndarray] | None): _description_. Defaults to None.\n        local_regression_method (str): Defaults to \"loess\". Other options currently: \"kernel\".\n    \"\"\"\n    # This is only ever be necessary if the function is called outside fit()\n    if smoothers is None:\n        smoothers = self.precalculate_smooths(\n            X_data=X_data, local_regression_method=local_regression_method\n        )\n\n    self._find_lambda_max_value(\n        X_data=X_data,\n        Y_data=Y_data,\n        smoothers=smoothers,\n        precalculate_smooths=precalculate_smooths,\n        local_regression_method=local_regression_method,\n    )\n    lambda_scale = np.exp(\n        np.linspace(start=np.log(1), stop=np.log(lambda_min_ratio), num=nlambda)\n    )\n    self.lambda_values = lambda_scale * self.lambda_max_value\n\n    gcv_values = np.empty(len(self.lambda_values))\n    f_hat_from_previous_lambda = self._init_functions(X_data=X_data)\n    current_min = np.inf\n    for i, lambda_value in enumerate(self.lambda_values):\n        self._progressBar(i, len(self.lambda_values) - 1, suffix=\"Finding optimal lambda\")\n        try:\n            self.block_coordinate_descent(\n                X_data=X_data,\n                Y_data=Y_data,\n                penalty=lambda_value,\n                precalculate_smooths=precalculate_smooths,\n                smoothers=smoothers,\n                warm_start_f_hat=f_hat_from_previous_lambda,\n                local_regression_method=local_regression_method,\n            )\n            gcv_values[i] = self.gcv(Y_data=Y_data)\n            f_hat_from_previous_lambda = self.f_g_hat\n            self.zero_group_history[lambda_value] = self.zero_groups\n\n            if gcv_values[i] &lt; current_min:\n                final_f_hat = self.f_g_hat  # Save for later\n                final_zero_groups = self.zero_groups  # Save for later\n                final_steps_till_convergence = self.steps_till_convergence\n                current_min = gcv_values[i]\n\n        except ConvergenceError:  # Stop if no convergence\n            if i == 0:  # If this is negative even max_lambda didn't converge.\n                raise ValueError(\n                    \"No lambda value converged. Something wrong with causal order?\"\n                )\n            break\n\n    self.chosen_lambda = self.lambda_values[np.argmin(gcv_values)]\n    self.f_g_hat = final_f_hat\n    self.zero_groups = final_zero_groups\n    self.steps_till_convergence = final_steps_till_convergence\n    self.gcv_values = gcv_values\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.smoother_direct_fit","title":"<code>smoother_direct_fit(g, d_g, R_g, X_data, local_regression_method)</code>","text":"<p>Direct fit of smoothing.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>int</code> <p>description</p> required <code>d_g</code> <code>int</code> <p>description</p> required <code>R_g</code> <code>ndarray</code> <p>description</p> required <code>X_data</code> <code>ndarray</code> <p>description</p> required <code>local_regression_method</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>_type_</code> <code>ndarray</code> <p>description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def smoother_direct_fit(\n    self,\n    g: int,\n    d_g: int,\n    R_g: np.ndarray,\n    X_data: np.ndarray | dict[str, np.ndarray],\n    local_regression_method: str,\n) -&gt; np.ndarray:\n    \"\"\"Direct fit of smoothing.\n\n    Args:\n        g (int): _description_\n        d_g (int): _description_\n        R_g (np.ndarray): _description_\n        X_data (np.ndarray): _description_\n        local_regression_method (str): _description_\n\n    Raises:\n        NotImplementedError: _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    if local_regression_method == \"loess\":\n        smooth_fit = self.loess_direct_fit(g=g, d_g=d_g, R_g=R_g, X_data=X_data)\n    elif local_regression_method == \"kernel\":\n        smooth_fit = self.gaussian_kernel_direct_fit(g=g, d_g=d_g, R_g=R_g, X_data=X_data)\n    else:\n        raise NotImplementedError(\"Smoothing method not implemented.\")\n    return smooth_fit\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.soft_thresholding_update","title":"<code>soft_thresholding_update(g, f_g, zero_groups, smooth_fit, s_g_ordered, penalty, m_opt)</code>","text":"<p>Soft-thresholding update.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>int</code> <p>description</p> required <code>f_g</code> <code>ndarray | dict[str, ndarray]</code> <p>description</p> required <code>zero_groups</code> <code>list[bool]</code> <p>description</p> required <code>smooth_fit</code> <code>ndarray</code> <p>description</p> required <code>s_g_ordered</code> <code>ndarray</code> <p>description</p> required <code>penalty</code> <code>float</code> <p>description</p> required <code>m_opt</code> <code>ndarray</code> <p>description</p> required <p>Returns:</p> Type Description <code>ndarray | dict[str, ndarray]</code> <p>np.ndarray | dict[str, np.ndarray]: description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def soft_thresholding_update(\n    self,\n    g: int,\n    f_g: np.ndarray | dict[str, np.ndarray],\n    zero_groups: list[bool],\n    smooth_fit: np.ndarray,\n    s_g_ordered: np.ndarray,\n    penalty: float,\n    m_opt: np.ndarray,\n) -&gt; np.ndarray | dict[str, np.ndarray]:\n    \"\"\"Soft-thresholding update.\n\n    Args:\n        g (int): _description_\n        f_g (np.ndarray | dict[str, np.ndarray]): _description_\n        zero_groups (list[bool]): _description_\n        smooth_fit (np.ndarray): _description_\n        s_g_ordered (np.ndarray): _description_\n        penalty (float): _description_\n        m_opt (np.ndarray): _description_\n\n    Returns:\n        np.ndarray | dict[str, np.ndarray]: _description_\n    \"\"\"\n    if isinstance(f_g, np.ndarray):\n        f_g_hat = f_g[:, :, g, :]\n    else:\n        f_g_hat = list(f_g.values())[g]\n\n    if zero_groups[g]:\n        f_g_hat = np.zeros(f_g_hat.shape)\n    else:\n        f_g_hat = self.update_loop(\n            f_g_hat=f_g_hat,\n            d_g=self.d_g_long[g],\n            smooth_fit=smooth_fit,\n            m_opt=m_opt,\n            s_g_ordered=s_g_ordered,\n            penalty=penalty,\n        )\n    if isinstance(f_g, np.ndarray):\n        f_g[:, :, g, :] = f_g_hat\n    else:\n        f_g[list(f_g.keys())[g]] = f_g_hat\n    return f_g\n</code></pre>"},{"location":"api/#gresit.model_selection.MURGS.update_loop","title":"<code>update_loop(f_g_hat, d_g, smooth_fit, s_g_ordered, penalty, m_opt)</code>","text":"<p>Inner update loop.</p> <p>Parameters:</p> Name Type Description Default <code>f_g_hat</code> <code>ndarray</code> <p>description</p> required <code>d_g</code> <code>int</code> <p>description</p> required <code>smooth_fit</code> <code>ndarray</code> <p>description</p> required <code>s_g_ordered</code> <code>ndarray</code> <p>description</p> required <code>penalty</code> <code>float</code> <p>description</p> required <code>m_opt</code> <code>ndarray</code> <p>description</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: description</p> Source code in <code>gresit/model_selection.py</code> <pre><code>def update_loop(\n    self,\n    f_g_hat: np.ndarray,\n    d_g: int,\n    smooth_fit: np.ndarray,\n    s_g_ordered: np.ndarray,\n    penalty: float,\n    m_opt: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Inner update loop.\n\n    Args:\n        f_g_hat (np.ndarray): _description_\n        d_g (int): _description_\n        smooth_fit (np.ndarray): _description_\n        s_g_ordered (np.ndarray): _description_\n        penalty (float): _description_\n        m_opt (np.ndarray): _description_\n\n    Returns:\n        np.ndarray: _description_\n    \"\"\"\n    for k in range(self.n_tasks):\n        for j in range(d_g):\n            # for each task check whether larger or smaller m*\n            if (k + 1) &gt; m_opt:\n                f_g_hat[:, j, k] = smooth_fit[:, j, k]\n            else:\n                s_g_sum = s_g_ordered[:m_opt].sum()\n                f_g_hat[:, j, k] = (\n                    1\n                    / m_opt\n                    * (s_g_sum - np.sqrt(d_g) * penalty)\n                    * smooth_fit[:, j, k]\n                    / s_g_ordered[k]\n                )\n            f_g_hat[:, j, k] -= f_g_hat[:, j, k].mean()\n    return f_g_hat\n</code></pre>"},{"location":"api/#simulation","title":"Simulation","text":"<p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"api/#gresit.synthetic_data.Equation","title":"<code>Equation</code>","text":"<p>Abstract class for non-linear equations.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class Equation(metaclass=ABCMeta):\n    \"\"\"Abstract class for non-linear equations.\"\"\"\n\n    def __init__(\n        self,\n        group_size: int,\n        input_dim: int,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n    ):\n        \"\"\"Abstract class for non-linear equations.\n\n        Args:\n            input_dim (int): parent dimension\n            group_size (int, optional): _description_. Defaults to 2.\n            rng (np.random.Generator, optional): Random number generator. Defaults to default rng.\n        \"\"\"\n        self.group_size = group_size\n        self.input_dim = input_dim\n        self.rng = rng\n\n    @abstractmethod\n    def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Abstract call method for equation.\"\"\"\n</code></pre>"},{"location":"api/#gresit.synthetic_data.Equation.__call__","title":"<code>__call__(x)</code>  <code>abstractmethod</code>","text":"<p>Abstract call method for equation.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>@abstractmethod\ndef __call__(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Abstract call method for equation.\"\"\"\n</code></pre>"},{"location":"api/#gresit.synthetic_data.Equation.__init__","title":"<code>__init__(group_size, input_dim, rng=np.random.default_rng(seed=2024))</code>","text":"<p>Abstract class for non-linear equations.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>parent dimension</p> required <code>group_size</code> <code>int</code> <p>description. Defaults to 2.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to default rng.</p> <code>default_rng(seed=2024)</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    group_size: int,\n    input_dim: int,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n):\n    \"\"\"Abstract class for non-linear equations.\n\n    Args:\n        input_dim (int): parent dimension\n        group_size (int, optional): _description_. Defaults to 2.\n        rng (np.random.Generator, optional): Random number generator. Defaults to default rng.\n    \"\"\"\n    self.group_size = group_size\n    self.input_dim = input_dim\n    self.rng = rng\n</code></pre>"},{"location":"api/#gresit.synthetic_data.FCNN","title":"<code>FCNN</code>","text":"<p>               Bases: <code>Equation</code></p> <p>A randomly initialized fully connected neural net.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class FCNN(Equation):\n    \"\"\"A randomly initialized fully connected neural net.\"\"\"\n\n    def __init__(\n        self,\n        group_size: int,\n        input_dim: int,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        hidden_layer: int = 10,\n    ) -&gt; None:\n        \"\"\"A randomly initialized fully connected neural net.\n\n        Args:\n            input_dim (int): parent dimension\n            group_size (int, optional): _description_. Defaults to 2.\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            hidden_layer (int, optional): Size of hidden dimension.\n        \"\"\"\n        super().__init__(group_size, input_dim, rng=rng)\n\n        layers = [\n            nn.Linear(self.group_size * self.input_dim, hidden_layer, dtype=torch.float32),\n            nn.Sigmoid(),\n            nn.Linear(hidden_layer, self.group_size, dtype=torch.float32),\n        ]\n        self.fcn = nn.Sequential(*layers)\n        self.fcn.apply(self.init_weights)\n\n    def init_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initializes the weights of the neural net.\n\n        Args:\n            m (nn.Module): The layer to be initialized\n        \"\"\"\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, mean=0, std=1)\n            nn.init.normal_(m.bias, mean=0, std=1)\n\n    def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Computes the right hand side of the equation.\n\n        Args:\n            x (np.ndarray): Input data\n\n        Returns:\n            np.ndarray: output data of size n x multioutput_dim\n        \"\"\"\n        X = torch.from_numpy(x).flatten(1, -1).float()\n\n        with torch.no_grad():\n            return self.fcn(X).detach().cpu().numpy()\n</code></pre>"},{"location":"api/#gresit.synthetic_data.FCNN.__call__","title":"<code>__call__(x)</code>","text":"<p>Computes the right hand side of the equation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: output data of size n x multioutput_dim</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the right hand side of the equation.\n\n    Args:\n        x (np.ndarray): Input data\n\n    Returns:\n        np.ndarray: output data of size n x multioutput_dim\n    \"\"\"\n    X = torch.from_numpy(x).flatten(1, -1).float()\n\n    with torch.no_grad():\n        return self.fcn(X).detach().cpu().numpy()\n</code></pre>"},{"location":"api/#gresit.synthetic_data.FCNN.__init__","title":"<code>__init__(group_size, input_dim, rng=np.random.default_rng(seed=2024), hidden_layer=10)</code>","text":"<p>A randomly initialized fully connected neural net.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>parent dimension</p> required <code>group_size</code> <code>int</code> <p>description. Defaults to 2.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>hidden_layer</code> <code>int</code> <p>Size of hidden dimension.</p> <code>10</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    group_size: int,\n    input_dim: int,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    hidden_layer: int = 10,\n) -&gt; None:\n    \"\"\"A randomly initialized fully connected neural net.\n\n    Args:\n        input_dim (int): parent dimension\n        group_size (int, optional): _description_. Defaults to 2.\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        hidden_layer (int, optional): Size of hidden dimension.\n    \"\"\"\n    super().__init__(group_size, input_dim, rng=rng)\n\n    layers = [\n        nn.Linear(self.group_size * self.input_dim, hidden_layer, dtype=torch.float32),\n        nn.Sigmoid(),\n        nn.Linear(hidden_layer, self.group_size, dtype=torch.float32),\n    ]\n    self.fcn = nn.Sequential(*layers)\n    self.fcn.apply(self.init_weights)\n</code></pre>"},{"location":"api/#gresit.synthetic_data.FCNN.init_weights","title":"<code>init_weights(m)</code>","text":"<p>Initializes the weights of the neural net.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Module</code> <p>The layer to be initialized</p> required Source code in <code>gresit/synthetic_data.py</code> <pre><code>def init_weights(self, m: nn.Module) -&gt; None:\n    \"\"\"Initializes the weights of the neural net.\n\n    Args:\n        m (nn.Module): The layer to be initialized\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, mean=0, std=1)\n        nn.init.normal_(m.bias, mean=0, std=1)\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GaussianProcesses","title":"<code>GaussianProcesses</code>","text":"<p>               Bases: <code>Equation</code></p> <p>A weighted sum of Gaussian processes.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class GaussianProcesses(Equation):\n    \"\"\"A weighted sum of Gaussian processes.\"\"\"\n\n    def __init__(\n        self,\n        group_size: int,\n        input_dim: int,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        n_gp: int = 5,\n        sigma: float = 0.3,\n    ) -&gt; None:\n        \"\"\"A weighted sum of Gaussian processes.\n\n        Args:\n            input_dim (int): parent dimension\n            group_size (int, optional): _description_. Defaults to 2.\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            n_gp (int): Number of processes\n            sigma (float): The standard deviation of GPs. Note that all GPS have the same std.\n\n        Raises:\n            ValueError: Gets thrown when input dimensions don't match.\n        \"\"\"\n        super().__init__(group_size, input_dim, rng=rng)\n\n        self.n_gp = n_gp\n\n        mean_vec = self.rng.random((self.group_size, self.input_dim, self.n_gp)) * 2.0 - 0.5\n        # Weights in GPs.\n        w1 = self.rng.random((self.group_size, self.input_dim, self.n_gp))\n        w1 /= w1.sum(axis=-1, keepdims=True)\n        # Weights for a linear aggregation of GPs.\n        w2 = self.rng.random((self.group_size, self.input_dim))\n        w2 /= w2.sum(axis=-1, keepdims=True)\n\n        (_, n1, _) = mean_vec.shape\n        (_, n2, _) = w1.shape\n        (_, n3) = w2.shape\n\n        if n1 != n2 or n1 != n3:\n            raise ValueError(\"Input variable dimensions need to match.\")\n\n        # add first axis corresponding to the number of samples.\n        self.mean_vec = mean_vec[np.newaxis]\n        self.w1 = w1[np.newaxis]\n        self.w2 = w2[np.newaxis]\n        self.sigma = sigma\n\n    @staticmethod\n    def from_params(\n        mean_vec: np.ndarray,\n        w1: np.ndarray,\n        w2: np.ndarray,\n        sigma: float = 0.3,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n    ) -&gt; \"GaussianProcesses\":\n        \"\"\"Creates a gaussian process from parameters.\n\n        Args:\n            mean_vec (np.ndarray): Mean vectors of Gaussian processes with\n                shape=(group_size, num_parents, num_kernels), where num_parents is\n                the dimension of the input variable and num_kernels is\n                the number of Gaussian kernels in one GP.\n            w1 (np.ndarray): Weights of GPs with shape=(num_parents, num_kernels).\n            w2 (np.ndarray): Weights for aggregating GPs with shape=(n,)\n            sigma (float): The standard deviation of GPs. Note that all GPS have the same std.\n            rng (np.random.Generator, optional): The random number generator to use.\n        \"\"\"\n        gp = GaussianProcesses(group_size=mean_vec.shape[0], input_dim=mean_vec.shape[1], rng=rng)\n\n        gp.mean_vec = mean_vec[np.newaxis]\n        gp.w1 = w1[np.newaxis]\n        gp.w2 = w2[np.newaxis]\n        gp.sigma = sigma\n        return gp\n\n    def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Depending on the shape of x dimension will be adjusted.\n\n        Args:\n            x (float | np.ndarray): Input data\n\n        Raises:\n            ValueError: Gets thrown when input dimensions don't match.\n\n        Returns:\n            float | np.ndarray: output data of size n x multioutput_dim\n        \"\"\"\n        kernel_list = []\n        for i in range(self.mean_vec.shape[-1]):\n            kernel_list.append(\n                self._gaussian_kernel_function(\n                    x_1=x, x_2=self.mean_vec[:, :, :, i], sigma=self.sigma\n                )\n            )\n\n        kernel = np.stack(kernel_list, axis=x.ndim)\n        if not kernel.shape == (*x.shape, self.mean_vec.shape[-1]):\n            raise ValueError(\n                f\"\"\"something went wrong, shape of kernel function should be \\\n                {(*x.shape, self.mean_vec.shape[-1])}, \\\n                but in fact it is {kernel.shape}.\"\"\"\n            )\n        # kernel should have shape #samples, #group_size, #parents #kernels\n        # Weighted sum of q Gaussian kernels.\n        gps: np.ndarray = (kernel * self.w1).sum(axis=-1)\n        # Weighted sum of GPs (doesn't do anythin if there is only one parent)\n        gp: np.ndarray = (gps * self.w2).sum(axis=-1)\n\n        if not gp.shape == (x.shape[0], x.shape[1]):\n            raise ValueError(\n                f\"\"\"something went wrong, shape of function output should be \\\n                {x.shape}, but in fact it is {gp.shape}.\"\"\"\n            )\n\n        return gp\n\n    def _gaussian_kernel_function(\n        self, x_1: np.ndarray, x_2: np.ndarray, sigma: np.ndarray | float\n    ) -&gt; np.ndarray:\n        \"\"\"Gaussian kernel.\"\"\"\n        return np.exp(-0.5 * np.square(x_1 - x_2) / np.square(sigma))\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GaussianProcesses.__call__","title":"<code>__call__(x)</code>","text":"<p>Depending on the shape of x dimension will be adjusted.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float | ndarray</code> <p>Input data</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Gets thrown when input dimensions don't match.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>float | np.ndarray: output data of size n x multioutput_dim</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __call__(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Depending on the shape of x dimension will be adjusted.\n\n    Args:\n        x (float | np.ndarray): Input data\n\n    Raises:\n        ValueError: Gets thrown when input dimensions don't match.\n\n    Returns:\n        float | np.ndarray: output data of size n x multioutput_dim\n    \"\"\"\n    kernel_list = []\n    for i in range(self.mean_vec.shape[-1]):\n        kernel_list.append(\n            self._gaussian_kernel_function(\n                x_1=x, x_2=self.mean_vec[:, :, :, i], sigma=self.sigma\n            )\n        )\n\n    kernel = np.stack(kernel_list, axis=x.ndim)\n    if not kernel.shape == (*x.shape, self.mean_vec.shape[-1]):\n        raise ValueError(\n            f\"\"\"something went wrong, shape of kernel function should be \\\n            {(*x.shape, self.mean_vec.shape[-1])}, \\\n            but in fact it is {kernel.shape}.\"\"\"\n        )\n    # kernel should have shape #samples, #group_size, #parents #kernels\n    # Weighted sum of q Gaussian kernels.\n    gps: np.ndarray = (kernel * self.w1).sum(axis=-1)\n    # Weighted sum of GPs (doesn't do anythin if there is only one parent)\n    gp: np.ndarray = (gps * self.w2).sum(axis=-1)\n\n    if not gp.shape == (x.shape[0], x.shape[1]):\n        raise ValueError(\n            f\"\"\"something went wrong, shape of function output should be \\\n            {x.shape}, but in fact it is {gp.shape}.\"\"\"\n        )\n\n    return gp\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GaussianProcesses.__init__","title":"<code>__init__(group_size, input_dim, rng=np.random.default_rng(seed=2024), n_gp=5, sigma=0.3)</code>","text":"<p>A weighted sum of Gaussian processes.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>parent dimension</p> required <code>group_size</code> <code>int</code> <p>description. Defaults to 2.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>n_gp</code> <code>int</code> <p>Number of processes</p> <code>5</code> <code>sigma</code> <code>float</code> <p>The standard deviation of GPs. Note that all GPS have the same std.</p> <code>0.3</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Gets thrown when input dimensions don't match.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    group_size: int,\n    input_dim: int,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    n_gp: int = 5,\n    sigma: float = 0.3,\n) -&gt; None:\n    \"\"\"A weighted sum of Gaussian processes.\n\n    Args:\n        input_dim (int): parent dimension\n        group_size (int, optional): _description_. Defaults to 2.\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        n_gp (int): Number of processes\n        sigma (float): The standard deviation of GPs. Note that all GPS have the same std.\n\n    Raises:\n        ValueError: Gets thrown when input dimensions don't match.\n    \"\"\"\n    super().__init__(group_size, input_dim, rng=rng)\n\n    self.n_gp = n_gp\n\n    mean_vec = self.rng.random((self.group_size, self.input_dim, self.n_gp)) * 2.0 - 0.5\n    # Weights in GPs.\n    w1 = self.rng.random((self.group_size, self.input_dim, self.n_gp))\n    w1 /= w1.sum(axis=-1, keepdims=True)\n    # Weights for a linear aggregation of GPs.\n    w2 = self.rng.random((self.group_size, self.input_dim))\n    w2 /= w2.sum(axis=-1, keepdims=True)\n\n    (_, n1, _) = mean_vec.shape\n    (_, n2, _) = w1.shape\n    (_, n3) = w2.shape\n\n    if n1 != n2 or n1 != n3:\n        raise ValueError(\"Input variable dimensions need to match.\")\n\n    # add first axis corresponding to the number of samples.\n    self.mean_vec = mean_vec[np.newaxis]\n    self.w1 = w1[np.newaxis]\n    self.w2 = w2[np.newaxis]\n    self.sigma = sigma\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GaussianProcesses.from_params","title":"<code>from_params(mean_vec, w1, w2, sigma=0.3, rng=np.random.default_rng(seed=2024))</code>  <code>staticmethod</code>","text":"<p>Creates a gaussian process from parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean_vec</code> <code>ndarray</code> <p>Mean vectors of Gaussian processes with shape=(group_size, num_parents, num_kernels), where num_parents is the dimension of the input variable and num_kernels is the number of Gaussian kernels in one GP.</p> required <code>w1</code> <code>ndarray</code> <p>Weights of GPs with shape=(num_parents, num_kernels).</p> required <code>w2</code> <code>ndarray</code> <p>Weights for aggregating GPs with shape=(n,)</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of GPs. Note that all GPS have the same std.</p> <code>0.3</code> <code>rng</code> <code>Generator</code> <p>The random number generator to use.</p> <code>default_rng(seed=2024)</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>@staticmethod\ndef from_params(\n    mean_vec: np.ndarray,\n    w1: np.ndarray,\n    w2: np.ndarray,\n    sigma: float = 0.3,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n) -&gt; \"GaussianProcesses\":\n    \"\"\"Creates a gaussian process from parameters.\n\n    Args:\n        mean_vec (np.ndarray): Mean vectors of Gaussian processes with\n            shape=(group_size, num_parents, num_kernels), where num_parents is\n            the dimension of the input variable and num_kernels is\n            the number of Gaussian kernels in one GP.\n        w1 (np.ndarray): Weights of GPs with shape=(num_parents, num_kernels).\n        w2 (np.ndarray): Weights for aggregating GPs with shape=(n,)\n        sigma (float): The standard deviation of GPs. Note that all GPS have the same std.\n        rng (np.random.Generator, optional): The random number generator to use.\n    \"\"\"\n    gp = GaussianProcesses(group_size=mean_vec.shape[0], input_dim=mean_vec.shape[1], rng=rng)\n\n    gp.mean_vec = mean_vec[np.newaxis]\n    gp.w1 = w1[np.newaxis]\n    gp.w2 = w2[np.newaxis]\n    gp.sigma = sigma\n    return gp\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenChainedData","title":"<code>GenChainedData</code>","text":"<p>               Bases: <code>GenData</code></p> <p>Class to generate chain DAG with nonlinear data following multivariate ANM (mANM).</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class GenChainedData(GenData):\n    \"\"\"Class to generate chain DAG with nonlinear data following multivariate ANM (mANM).\"\"\"\n\n    def __init__(\n        self,\n        number_of_nodes: int = 15,\n        equation_cls: type[Equation] = GaussianProcesses,\n        equation_kwargs: dict[str, Any] | None = None,\n        group_size: int = 2,\n        edge_density: float = 0.2,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        snr: float = 1.0,\n        noise_distribution: str = \"gaussian\",\n    ) -&gt; None:\n        \"\"\"Initiate the layered DAG.\n\n        Args:\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            number_of_nodes (int, optional): _description_. Defaults to 15.\n            equation_cls (Equation): The type of the equation that should be used.\n            equation_kwargs (dict): Arguments for equation.\n            group_size (int, optional): Number of entries in groups. Defaults to 2.\n            edge_density (float, optional): _description_. Defaults to 0.2.\n            snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n            noise_distribution (str, optional): Which distribution to choose for the noise.\n                defaults to Gaussian noise. Options include: `lognormal`\n        \"\"\"\n        super().__init__(\n            number_of_nodes=number_of_nodes,\n            equation_cls=equation_cls,\n            equation_kwargs=equation_kwargs,\n            group_size=group_size,\n            edge_density=edge_density,\n            rng=rng,\n            snr=snr,\n            noise_distribution=noise_distribution,\n        )\n        self.dag = DAG()\n        self._initiate()\n        self.causal_order = self.dag.causal_order\n\n    def _make_dag(self) -&gt; None:\n        \"\"\"Generate an chain Directed Acyclic Graph (DAG).\"\"\"\n        # Assign a random topological ordering\n        nodes = [f\"X_{i}\" for i in range(self.number_of_nodes)]\n        self.rng.shuffle(nodes)\n        self.causal_order = nodes\n        chain_edges = [(nodes[i], nodes[i + 1]) for i in range(self.number_of_nodes - 1)]\n        G = DAG(nodes=nodes, edges=chain_edges)\n        for i in range(self.number_of_nodes):\n            for j in range(i + 2, self.number_of_nodes):\n                if self.rng.random() &lt; self.edge_density:\n                    G.add_edge(edge=(nodes[i], nodes[j]))\n        self.dag = G\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenChainedData.__init__","title":"<code>__init__(number_of_nodes=15, equation_cls=GaussianProcesses, equation_kwargs=None, group_size=2, edge_density=0.2, rng=np.random.default_rng(seed=2024), snr=1.0, noise_distribution='gaussian')</code>","text":"<p>Initiate the layered DAG.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>number_of_nodes</code> <code>int</code> <p>description. Defaults to 15.</p> <code>15</code> <code>equation_cls</code> <code>Equation</code> <p>The type of the equation that should be used.</p> <code>GaussianProcesses</code> <code>equation_kwargs</code> <code>dict</code> <p>Arguments for equation.</p> <code>None</code> <code>group_size</code> <code>int</code> <p>Number of entries in groups. Defaults to 2.</p> <code>2</code> <code>edge_density</code> <code>float</code> <p>description. Defaults to 0.2.</p> <code>0.2</code> <code>snr</code> <code>float</code> <p>Signal to noise ratio. Defaults to 1.0.</p> <code>1.0</code> <code>noise_distribution</code> <code>str</code> <p>Which distribution to choose for the noise. defaults to Gaussian noise. Options include: <code>lognormal</code></p> <code>'gaussian'</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    number_of_nodes: int = 15,\n    equation_cls: type[Equation] = GaussianProcesses,\n    equation_kwargs: dict[str, Any] | None = None,\n    group_size: int = 2,\n    edge_density: float = 0.2,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    snr: float = 1.0,\n    noise_distribution: str = \"gaussian\",\n) -&gt; None:\n    \"\"\"Initiate the layered DAG.\n\n    Args:\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        number_of_nodes (int, optional): _description_. Defaults to 15.\n        equation_cls (Equation): The type of the equation that should be used.\n        equation_kwargs (dict): Arguments for equation.\n        group_size (int, optional): Number of entries in groups. Defaults to 2.\n        edge_density (float, optional): _description_. Defaults to 0.2.\n        snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n        noise_distribution (str, optional): Which distribution to choose for the noise.\n            defaults to Gaussian noise. Options include: `lognormal`\n    \"\"\"\n    super().__init__(\n        number_of_nodes=number_of_nodes,\n        equation_cls=equation_cls,\n        equation_kwargs=equation_kwargs,\n        group_size=group_size,\n        edge_density=edge_density,\n        rng=rng,\n        snr=snr,\n        noise_distribution=noise_distribution,\n    )\n    self.dag = DAG()\n    self._initiate()\n    self.causal_order = self.dag.causal_order\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenData","title":"<code>GenData</code>","text":"<p>Parent class to GenDate classes.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class GenData:\n    \"\"\"Parent class to GenDate classes.\"\"\"\n\n    def __init__(\n        self,\n        number_of_nodes: int = 15,\n        equation_cls: type[Equation] = GaussianProcesses,\n        equation_kwargs: dict[str, Any] | None = None,\n        group_size: int = 2,\n        edge_density: float = 0.2,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        snr: float = 1.0,\n        noise_distribution: str = \"gaussian\",\n    ) -&gt; None:\n        \"\"\"Initiate parent class DAG.\n\n        Args:\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            number_of_nodes (int, optional): _description_. Defaults to 15.\n            equation_cls (Equation): The type of the equation that should be used.\n            equation_kwargs (dict): Arguments for equation.\n            group_size (int, optional): Number of entries in groups. Defaults to 2.\n            edge_density (float, optional): _description_. Defaults to 0.2.\n            snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n            noise_distribution (str, optional): Which distribution to choose for the noise.\n                defaults to Gaussian noise. Options include: `lognormal`\n        \"\"\"\n        self.number_of_nodes = number_of_nodes\n        self.group_size = group_size\n        self.edge_density = edge_density\n        self.rng = rng\n        self.equation_cls = equation_cls\n        self.equation_kwargs = equation_kwargs\n        self.dag = DAG()\n        self.causal_order: list[str] = []\n        self.snr = snr\n        self.noise_distribution = noise_distribution\n\n    def _initiate(self) -&gt; None:\n        \"\"\"Initiate DAG and random functions.\"\"\"\n        self._make_dag()\n        self._initiate_ANM()\n\n    def _make_dag(self) -&gt; None:\n        \"\"\"Make DAG.\n\n        Raises:\n            NotImplementedError: Needs to be overwritten.\n        \"\"\"\n        raise NotImplementedError\n\n    def _initiate_ANM(self) -&gt; None:\n        \"\"\"Initiate the ANM equations.\"\"\"\n        equations: dict[str, MultiOutputANM] = {}\n        for node in self.dag.nodes:\n            parents = self.dag.parents(of_node=node)\n            equations[node] = MultiOutputANM(\n                input_dim=len(parents),\n                equation_cls=self.equation_cls,\n                equation_kwargs=self.equation_kwargs,\n                group_size=self.group_size,\n                rng=self.rng,\n                snr=self.snr,\n            )\n\n        self.equations = equations\n\n    def _generate_random_correlation_matrix(self, group_size: int) -&gt; np.ndarray:\n        \"\"\"Generate random pd correlation matrix.\n\n        Args:\n            group_size (int): dimension of matrix.\n\n        Returns:\n            np.ndarray: correlation matrix.\n        \"\"\"\n        L = np.tril(self.rng.uniform(-0.8, 0.8, (group_size, group_size)), k=-1)\n        np.fill_diagonal(L, 1)  # Set diagonal to 1\n\n        # Compute symmetric positive definite matrix\n        Sigma = L @ L.T\n\n        # Normalize diagonal to 1\n        D = np.sqrt(np.diag(Sigma))\n        Sigma = Sigma / np.outer(D, D)\n\n        return Sigma\n\n    def generate_data(self, num_samples: int = 1000) -&gt; tuple[dict[str, np.ndarray], np.ndarray]:\n        \"\"\"Sample data from the layered DAG.\"\"\"\n        noise_data = []\n        for _ in self.dag.nodes:\n            mean = self.rng.uniform(-0.8, 0.8, self.group_size)\n            corr = self._generate_random_correlation_matrix(self.group_size)\n            if self.noise_distribution == \"gaussian\":\n                noise_data.append(\n                    self.rng.multivariate_normal(mean=mean, cov=corr, size=num_samples)\n                )\n            else:\n                noise_data.append(\n                    np.exp(self.rng.multivariate_normal(mean=mean, cov=corr, size=num_samples))\n                )\n        reshaped_noise = np.moveaxis(np.asanyarray(noise_data), 0, -1)\n\n        data = np.zeros(reshaped_noise.shape)\n        data_dict = {}\n        for i, node in enumerate(self.causal_order):\n            parents = self.dag.parents(of_node=node)\n            parent_indices = [self.dag.causal_order.index(parent) for parent in parents]\n            data[:, :, i] = self.equations[node].apply_rhs(\n                parent_data=data[:, :, parent_indices], noise_data=reshaped_noise[:, :, i]\n            )\n            data[:, :, i] -= data[:, :, i].mean(axis=0)\n            data[:, :, i] /= data[:, :, i].std(axis=0)\n            data_dict[node] = data[:, :, i]\n        return data_dict, data\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenData.__init__","title":"<code>__init__(number_of_nodes=15, equation_cls=GaussianProcesses, equation_kwargs=None, group_size=2, edge_density=0.2, rng=np.random.default_rng(seed=2024), snr=1.0, noise_distribution='gaussian')</code>","text":"<p>Initiate parent class DAG.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>number_of_nodes</code> <code>int</code> <p>description. Defaults to 15.</p> <code>15</code> <code>equation_cls</code> <code>Equation</code> <p>The type of the equation that should be used.</p> <code>GaussianProcesses</code> <code>equation_kwargs</code> <code>dict</code> <p>Arguments for equation.</p> <code>None</code> <code>group_size</code> <code>int</code> <p>Number of entries in groups. Defaults to 2.</p> <code>2</code> <code>edge_density</code> <code>float</code> <p>description. Defaults to 0.2.</p> <code>0.2</code> <code>snr</code> <code>float</code> <p>Signal to noise ratio. Defaults to 1.0.</p> <code>1.0</code> <code>noise_distribution</code> <code>str</code> <p>Which distribution to choose for the noise. defaults to Gaussian noise. Options include: <code>lognormal</code></p> <code>'gaussian'</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    number_of_nodes: int = 15,\n    equation_cls: type[Equation] = GaussianProcesses,\n    equation_kwargs: dict[str, Any] | None = None,\n    group_size: int = 2,\n    edge_density: float = 0.2,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    snr: float = 1.0,\n    noise_distribution: str = \"gaussian\",\n) -&gt; None:\n    \"\"\"Initiate parent class DAG.\n\n    Args:\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        number_of_nodes (int, optional): _description_. Defaults to 15.\n        equation_cls (Equation): The type of the equation that should be used.\n        equation_kwargs (dict): Arguments for equation.\n        group_size (int, optional): Number of entries in groups. Defaults to 2.\n        edge_density (float, optional): _description_. Defaults to 0.2.\n        snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n        noise_distribution (str, optional): Which distribution to choose for the noise.\n            defaults to Gaussian noise. Options include: `lognormal`\n    \"\"\"\n    self.number_of_nodes = number_of_nodes\n    self.group_size = group_size\n    self.edge_density = edge_density\n    self.rng = rng\n    self.equation_cls = equation_cls\n    self.equation_kwargs = equation_kwargs\n    self.dag = DAG()\n    self.causal_order: list[str] = []\n    self.snr = snr\n    self.noise_distribution = noise_distribution\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenData.generate_data","title":"<code>generate_data(num_samples=1000)</code>","text":"<p>Sample data from the layered DAG.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def generate_data(self, num_samples: int = 1000) -&gt; tuple[dict[str, np.ndarray], np.ndarray]:\n    \"\"\"Sample data from the layered DAG.\"\"\"\n    noise_data = []\n    for _ in self.dag.nodes:\n        mean = self.rng.uniform(-0.8, 0.8, self.group_size)\n        corr = self._generate_random_correlation_matrix(self.group_size)\n        if self.noise_distribution == \"gaussian\":\n            noise_data.append(\n                self.rng.multivariate_normal(mean=mean, cov=corr, size=num_samples)\n            )\n        else:\n            noise_data.append(\n                np.exp(self.rng.multivariate_normal(mean=mean, cov=corr, size=num_samples))\n            )\n    reshaped_noise = np.moveaxis(np.asanyarray(noise_data), 0, -1)\n\n    data = np.zeros(reshaped_noise.shape)\n    data_dict = {}\n    for i, node in enumerate(self.causal_order):\n        parents = self.dag.parents(of_node=node)\n        parent_indices = [self.dag.causal_order.index(parent) for parent in parents]\n        data[:, :, i] = self.equations[node].apply_rhs(\n            parent_data=data[:, :, parent_indices], noise_data=reshaped_noise[:, :, i]\n        )\n        data[:, :, i] -= data[:, :, i].mean(axis=0)\n        data[:, :, i] /= data[:, :, i].std(axis=0)\n        data_dict[node] = data[:, :, i]\n    return data_dict, data\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenERData","title":"<code>GenERData</code>","text":"<p>               Bases: <code>GenData</code></p> <p>Class to generate general nonlinear data following Erdos-Renyi (ER) graph.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class GenERData(GenData):\n    \"\"\"Class to generate general nonlinear data following Erdos-Renyi (ER) graph.\"\"\"\n\n    def __init__(\n        self,\n        number_of_nodes: int = 15,\n        equation_cls: type[Equation] = GaussianProcesses,\n        equation_kwargs: dict[str, Any] | None = None,\n        group_size: int = 2,\n        edge_density: float = 0.2,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        snr: float = 1.0,\n        noise_distribution: str = \"gaussian\",\n    ) -&gt; None:\n        \"\"\"Initiate the ER DAG.\n\n        Args:\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            number_of_nodes (int, optional): _description_. Defaults to 15.\n            equation_cls (Equation): The type of the equation that should be used.\n            equation_kwargs (dict): Arguments for equation.\n            group_size (int, optional): Number of entries in groups. Defaults to 2.\n            edge_density (float, optional): _description_. Defaults to 0.2.\n            snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n            noise_distribution (str, optional): Which distribution to choose for the noise.\n                defaults to Gaussian noise. Options include: `lognormal`\n        \"\"\"\n        super().__init__(\n            number_of_nodes=number_of_nodes,\n            equation_cls=equation_cls,\n            equation_kwargs=equation_kwargs,\n            group_size=group_size,\n            edge_density=edge_density,\n            rng=rng,\n            snr=snr,\n            noise_distribution=noise_distribution,\n        )\n        self.dag = DAG()\n        self._initiate()\n        self.causal_order = self.dag.causal_order\n\n    def _make_dag(self) -&gt; None:\n        \"\"\"Generate an Erd\u0151s-R\u00e9nyi Directed Acyclic Graph (DAG).\"\"\"\n        # Assign a random topological ordering\n        nodes = [f\"X_{i}\" for i in range(self.number_of_nodes)]\n        self.rng.shuffle(nodes)\n\n        G = DAG(nodes=nodes)\n\n        # Add edges based on Erd\u0151s-R\u00e9nyi model\n        for i, j in combinations(range(self.number_of_nodes), 2):\n            # Ensure edges go from lower to higher index\n            if self.rng.random() &lt; self.edge_density:\n                G.add_edge(edge=(nodes[i], nodes[j]))\n\n        self.dag = G\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenERData.__init__","title":"<code>__init__(number_of_nodes=15, equation_cls=GaussianProcesses, equation_kwargs=None, group_size=2, edge_density=0.2, rng=np.random.default_rng(seed=2024), snr=1.0, noise_distribution='gaussian')</code>","text":"<p>Initiate the ER DAG.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>number_of_nodes</code> <code>int</code> <p>description. Defaults to 15.</p> <code>15</code> <code>equation_cls</code> <code>Equation</code> <p>The type of the equation that should be used.</p> <code>GaussianProcesses</code> <code>equation_kwargs</code> <code>dict</code> <p>Arguments for equation.</p> <code>None</code> <code>group_size</code> <code>int</code> <p>Number of entries in groups. Defaults to 2.</p> <code>2</code> <code>edge_density</code> <code>float</code> <p>description. Defaults to 0.2.</p> <code>0.2</code> <code>snr</code> <code>float</code> <p>Signal to noise ratio. Defaults to 1.0.</p> <code>1.0</code> <code>noise_distribution</code> <code>str</code> <p>Which distribution to choose for the noise. defaults to Gaussian noise. Options include: <code>lognormal</code></p> <code>'gaussian'</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    number_of_nodes: int = 15,\n    equation_cls: type[Equation] = GaussianProcesses,\n    equation_kwargs: dict[str, Any] | None = None,\n    group_size: int = 2,\n    edge_density: float = 0.2,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    snr: float = 1.0,\n    noise_distribution: str = \"gaussian\",\n) -&gt; None:\n    \"\"\"Initiate the ER DAG.\n\n    Args:\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        number_of_nodes (int, optional): _description_. Defaults to 15.\n        equation_cls (Equation): The type of the equation that should be used.\n        equation_kwargs (dict): Arguments for equation.\n        group_size (int, optional): Number of entries in groups. Defaults to 2.\n        edge_density (float, optional): _description_. Defaults to 0.2.\n        snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n        noise_distribution (str, optional): Which distribution to choose for the noise.\n            defaults to Gaussian noise. Options include: `lognormal`\n    \"\"\"\n    super().__init__(\n        number_of_nodes=number_of_nodes,\n        equation_cls=equation_cls,\n        equation_kwargs=equation_kwargs,\n        group_size=group_size,\n        edge_density=edge_density,\n        rng=rng,\n        snr=snr,\n        noise_distribution=noise_distribution,\n    )\n    self.dag = DAG()\n    self._initiate()\n    self.causal_order = self.dag.causal_order\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenLayeredData","title":"<code>GenLayeredData</code>","text":"<p>               Bases: <code>GenData</code></p> <p>Class to generate general nonlinear data following multivariate ANM (mANM).</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class GenLayeredData(GenData):\n    \"\"\"Class to generate general nonlinear data following multivariate ANM (mANM).\"\"\"\n\n    def __init__(\n        self,\n        number_of_nodes: int = 15,\n        number_of_layers: int = 3,\n        equation_cls: type[Equation] = GaussianProcesses,\n        equation_kwargs: dict[str, Any] | None = None,\n        group_size: int = 2,\n        edge_density: float = 0.2,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        snr: float = 1.0,\n        noise_distribution: str = \"gaussian\",\n    ) -&gt; None:\n        \"\"\"Initiate the layered DAG.\n\n        Args:\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            number_of_nodes (int, optional): _description_. Defaults to 15.\n            number_of_layers (int, optional): _description_. Defaults to 3.\n            equation_cls (Equation): The type of the equation that should be used.\n            equation_kwargs (dict): Arguments for equation.\n            group_size (int, optional): Number of entries in groups. Defaults to 2.\n            edge_density (float, optional): _description_. Defaults to 0.2.\n            snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n            noise_distribution (str, optional): Which distribution to choose for the noise.\n                defaults to Gaussian noise. Options include: `lognormal`\n        \"\"\"\n        super().__init__(\n            number_of_nodes=number_of_nodes,\n            equation_cls=equation_cls,\n            equation_kwargs=equation_kwargs,\n            group_size=group_size,\n            edge_density=edge_density,\n            rng=rng,\n            snr=snr,\n            noise_distribution=noise_distribution,\n        )\n        self.number_of_layers = number_of_layers\n        self.dag: LayeredDAG = LayeredDAG()\n        self._initiate()\n        self.layering = self.dag.layering\n\n    def _divide_equal(self, number: int, pieces: int) -&gt; list[int]:\n        \"\"\"Divides an integer into a specified number of pieces, distributing the remainder.\n\n        Args:\n            number: The integer to divide.\n            pieces: The desired number of pieces.\n\n        Returns:\n            A list containing the values of each piece.\n        \"\"\"\n        base_size = number // pieces\n        remainder = number % pieces\n        return [base_size + 1 if i &lt; remainder else base_size for i in range(pieces)]\n\n    def all_causal_orderings(self) -&gt; list[list[str]]:\n        \"\"\"Valid causal ordering of the layered DAG.\n\n        Raises:\n            ValueError: If there are no nodes or edges.\n\n        Returns:\n            list[list[str]]: list of causal orders.\n        \"\"\"\n        if not self.dag.nodes:\n            raise ValueError(\"There are no nodes in the graph\")\n\n        if not self.layering:\n            raise ValueError(\"Layering must be provided.\")\n\n        all_within_orders = [\n            list(nx.all_topological_sorts(self.dag.layer_induced_subgraph(layer).to_networkx()))\n            for layer in self.layering.values()\n        ]\n\n        all_orderings_tupled = list(product(*all_within_orders))\n        all_orderings = []\n        for order_tuple in all_orderings_tupled:\n            all_orderings.append([x for xs in [portion for portion in order_tuple] for x in xs])\n        return all_orderings\n\n    def _make_dag(self) -&gt; None:\n        \"\"\"Create a layered DAG.\"\"\"\n        nodes_per_layer = self._divide_equal(self.number_of_nodes, self.number_of_layers)\n\n        layering = defaultdict(list)\n        for i, layer_i in enumerate(nodes_per_layer):\n            for j in range(layer_i):\n                layering[f\"L_{i}\"].append(f\"X_{i}_{j}\")\n\n        for list_of_nodes in layering.values():\n            self.rng.shuffle(list_of_nodes)\n\n        self.dag.layering = layering\n\n        causal_ordering = [x for xs in [k for k in layering.values()] for x in xs]\n        self.causal_order = causal_ordering\n        # Add edges based on Erd\u0151s-R\u00e9nyi model\n\n        self.dag.add_nodes_from(causal_ordering)\n\n        for i in range(len(causal_ordering)):\n            for j in range(i + 1, len(causal_ordering)):\n                if self.rng.random() &lt; self.edge_density:\n                    self.dag.add_edge(edge=(causal_ordering[i], causal_ordering[j]))\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenLayeredData.__init__","title":"<code>__init__(number_of_nodes=15, number_of_layers=3, equation_cls=GaussianProcesses, equation_kwargs=None, group_size=2, edge_density=0.2, rng=np.random.default_rng(seed=2024), snr=1.0, noise_distribution='gaussian')</code>","text":"<p>Initiate the layered DAG.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>number_of_nodes</code> <code>int</code> <p>description. Defaults to 15.</p> <code>15</code> <code>number_of_layers</code> <code>int</code> <p>description. Defaults to 3.</p> <code>3</code> <code>equation_cls</code> <code>Equation</code> <p>The type of the equation that should be used.</p> <code>GaussianProcesses</code> <code>equation_kwargs</code> <code>dict</code> <p>Arguments for equation.</p> <code>None</code> <code>group_size</code> <code>int</code> <p>Number of entries in groups. Defaults to 2.</p> <code>2</code> <code>edge_density</code> <code>float</code> <p>description. Defaults to 0.2.</p> <code>0.2</code> <code>snr</code> <code>float</code> <p>Signal to noise ratio. Defaults to 1.0.</p> <code>1.0</code> <code>noise_distribution</code> <code>str</code> <p>Which distribution to choose for the noise. defaults to Gaussian noise. Options include: <code>lognormal</code></p> <code>'gaussian'</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    number_of_nodes: int = 15,\n    number_of_layers: int = 3,\n    equation_cls: type[Equation] = GaussianProcesses,\n    equation_kwargs: dict[str, Any] | None = None,\n    group_size: int = 2,\n    edge_density: float = 0.2,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    snr: float = 1.0,\n    noise_distribution: str = \"gaussian\",\n) -&gt; None:\n    \"\"\"Initiate the layered DAG.\n\n    Args:\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        number_of_nodes (int, optional): _description_. Defaults to 15.\n        number_of_layers (int, optional): _description_. Defaults to 3.\n        equation_cls (Equation): The type of the equation that should be used.\n        equation_kwargs (dict): Arguments for equation.\n        group_size (int, optional): Number of entries in groups. Defaults to 2.\n        edge_density (float, optional): _description_. Defaults to 0.2.\n        snr (float, optional): Signal to noise ratio. Defaults to 1.0.\n        noise_distribution (str, optional): Which distribution to choose for the noise.\n            defaults to Gaussian noise. Options include: `lognormal`\n    \"\"\"\n    super().__init__(\n        number_of_nodes=number_of_nodes,\n        equation_cls=equation_cls,\n        equation_kwargs=equation_kwargs,\n        group_size=group_size,\n        edge_density=edge_density,\n        rng=rng,\n        snr=snr,\n        noise_distribution=noise_distribution,\n    )\n    self.number_of_layers = number_of_layers\n    self.dag: LayeredDAG = LayeredDAG()\n    self._initiate()\n    self.layering = self.dag.layering\n</code></pre>"},{"location":"api/#gresit.synthetic_data.GenLayeredData.all_causal_orderings","title":"<code>all_causal_orderings()</code>","text":"<p>Valid causal ordering of the layered DAG.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no nodes or edges.</p> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: list of causal orders.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def all_causal_orderings(self) -&gt; list[list[str]]:\n    \"\"\"Valid causal ordering of the layered DAG.\n\n    Raises:\n        ValueError: If there are no nodes or edges.\n\n    Returns:\n        list[list[str]]: list of causal orders.\n    \"\"\"\n    if not self.dag.nodes:\n        raise ValueError(\"There are no nodes in the graph\")\n\n    if not self.layering:\n        raise ValueError(\"Layering must be provided.\")\n\n    all_within_orders = [\n        list(nx.all_topological_sorts(self.dag.layer_induced_subgraph(layer).to_networkx()))\n        for layer in self.layering.values()\n    ]\n\n    all_orderings_tupled = list(product(*all_within_orders))\n    all_orderings = []\n    for order_tuple in all_orderings_tupled:\n        all_orderings.append([x for xs in [portion for portion in order_tuple] for x in xs])\n    return all_orderings\n</code></pre>"},{"location":"api/#gresit.synthetic_data.MultiOutputANM","title":"<code>MultiOutputANM</code>","text":"<p>Class to construct nonlinear multi-outcome data that follows an ANM.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>class MultiOutputANM:\n    \"\"\"Class to construct nonlinear multi-outcome data that follows an ANM.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        equation_cls: type[Equation] = GaussianProcesses,\n        equation_kwargs: dict[str, Any] | None = None,\n        group_size: int = 2,\n        snr: float = 1.0,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n    ) -&gt; None:\n        \"\"\"Initiates the ANM object.\n\n        Args:\n            input_dim (int): parent dimension\n            group_size (int, optional): _description_. Defaults to 2.\n            rng (np.random.Generator, optional): Random number generator. Defaults to None.\n            equation_cls (Equation): The type of the equation that should be used.\n            equation_kwargs (dict): Arguments for equation.\n            snr (float, optional): Signal-to-noise ratio. Defaults to 1.0.\n        \"\"\"\n        self.input_dim = input_dim\n        self.group_size = group_size\n        self.snr = snr\n        self.rng = rng\n        self._lambda = self.snr / (1.0 + self.snr)\n        self.equation_cls = equation_cls\n        if not equation_kwargs:\n            equation_kwargs = {}\n        self.equation_kwargs = equation_kwargs\n        if self.input_dim &gt; 0:\n            self.f_nonlinear = self.generate_nonlinear_map()\n\n    def generate_nonlinear_map(self) -&gt; Equation:\n        \"\"\"Nonlinear function.\"\"\"\n        return self.equation_cls(\n            group_size=self.group_size,\n            input_dim=self.input_dim,\n            rng=self.rng,\n            **self.equation_kwargs,\n        )\n\n    def apply_rhs(self, parent_data: np.ndarray | None, noise_data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"See whether this works.\"\"\"\n        noise_standard_dev = 1 / np.std(noise_data, axis=0)\n        noise_std = noise_standard_dev * noise_data\n\n        if self.input_dim &gt; 0:\n            y_interim = self.f_nonlinear(parent_data)\n            y_standard_dev = 1 / np.std(y_interim, axis=0)\n            y_std = y_interim * y_standard_dev\n            y = np.sqrt(self._lambda) * y_std + np.sqrt(1.0 - self._lambda) * noise_std\n            y = y - np.mean(y, axis=0)\n        else:\n            y = noise_std\n        return y\n</code></pre>"},{"location":"api/#gresit.synthetic_data.MultiOutputANM.__init__","title":"<code>__init__(input_dim, equation_cls=GaussianProcesses, equation_kwargs=None, group_size=2, snr=1.0, rng=np.random.default_rng(seed=2024))</code>","text":"<p>Initiates the ANM object.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>parent dimension</p> required <code>group_size</code> <code>int</code> <p>description. Defaults to 2.</p> <code>2</code> <code>rng</code> <code>Generator</code> <p>Random number generator. Defaults to None.</p> <code>default_rng(seed=2024)</code> <code>equation_cls</code> <code>Equation</code> <p>The type of the equation that should be used.</p> <code>GaussianProcesses</code> <code>equation_kwargs</code> <code>dict</code> <p>Arguments for equation.</p> <code>None</code> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    equation_cls: type[Equation] = GaussianProcesses,\n    equation_kwargs: dict[str, Any] | None = None,\n    group_size: int = 2,\n    snr: float = 1.0,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n) -&gt; None:\n    \"\"\"Initiates the ANM object.\n\n    Args:\n        input_dim (int): parent dimension\n        group_size (int, optional): _description_. Defaults to 2.\n        rng (np.random.Generator, optional): Random number generator. Defaults to None.\n        equation_cls (Equation): The type of the equation that should be used.\n        equation_kwargs (dict): Arguments for equation.\n        snr (float, optional): Signal-to-noise ratio. Defaults to 1.0.\n    \"\"\"\n    self.input_dim = input_dim\n    self.group_size = group_size\n    self.snr = snr\n    self.rng = rng\n    self._lambda = self.snr / (1.0 + self.snr)\n    self.equation_cls = equation_cls\n    if not equation_kwargs:\n        equation_kwargs = {}\n    self.equation_kwargs = equation_kwargs\n    if self.input_dim &gt; 0:\n        self.f_nonlinear = self.generate_nonlinear_map()\n</code></pre>"},{"location":"api/#gresit.synthetic_data.MultiOutputANM.apply_rhs","title":"<code>apply_rhs(parent_data, noise_data)</code>","text":"<p>See whether this works.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def apply_rhs(self, parent_data: np.ndarray | None, noise_data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"See whether this works.\"\"\"\n    noise_standard_dev = 1 / np.std(noise_data, axis=0)\n    noise_std = noise_standard_dev * noise_data\n\n    if self.input_dim &gt; 0:\n        y_interim = self.f_nonlinear(parent_data)\n        y_standard_dev = 1 / np.std(y_interim, axis=0)\n        y_std = y_interim * y_standard_dev\n        y = np.sqrt(self._lambda) * y_std + np.sqrt(1.0 - self._lambda) * noise_std\n        y = y - np.mean(y, axis=0)\n    else:\n        y = noise_std\n    return y\n</code></pre>"},{"location":"api/#gresit.synthetic_data.MultiOutputANM.generate_nonlinear_map","title":"<code>generate_nonlinear_map()</code>","text":"<p>Nonlinear function.</p> Source code in <code>gresit/synthetic_data.py</code> <pre><code>def generate_nonlinear_map(self) -&gt; Equation:\n    \"\"\"Nonlinear function.\"\"\"\n    return self.equation_cls(\n        group_size=self.group_size,\n        input_dim=self.input_dim,\n        rng=self.rng,\n        **self.equation_kwargs,\n    )\n</code></pre>"},{"location":"api/#gresit.simulation_utils.BenchMarker","title":"<code>BenchMarker</code>","text":"<p>Class to instantiate and run simulations.</p> Source code in <code>gresit/simulation_utils.py</code> <pre><code>class BenchMarker:\n    \"\"\"Class to instantiate and run simulations.\"\"\"\n\n    @staticmethod\n    def _emptygraph(size: int) -&gt; np.ndarray:\n        return np.zeros((size, size))\n\n    def _cpdag_dag_processing(\n        self,\n        gt: pd.DataFrame,\n        learned_graph: GRAPH,\n        cpdag_strategy: str = \"random_dag\",\n    ) -&gt; tuple[pd.DataFrame, list[str] | None]:\n        \"\"\"Converts CPDAGs to DAGs according to CPDAG strategy.\n\n        If PDAG is not a proper CPDAG, and orientations lead to cycles,\n        we don't do anything.\n\n        Args:\n            gt (pd.DataFrame): ground truth adjacency matrix\n            learned_graph (GRAPH): learned graph\n            cpdag_strategy (str, optional): Conversion strategy. Defaults to \"random_dag\".\n\n        Raises:\n            NotImplementedError: _description_\n            ValueError: _description_\n\n        Returns:\n            tuple[pd.DataFrame, list[str] | None]: Tuple containing amat and causal order (or None).\n        \"\"\"\n        learned_dag: PDAG | DAG\n        if isinstance(learned_graph, PDAG):\n            if cpdag_strategy == \"random_dag\":\n                try:\n                    learned_dag = learned_graph.to_random_dag()\n                except ValueError:\n                    learned_dag = learned_graph\n            elif cpdag_strategy == \"best_dag\":\n                try:\n                    all_dags = learned_graph.to_allDAGs()\n                    original_order = gt.columns\n                    shd_array = np.full(len(all_dags), np.inf)\n                    for i, each_dag in enumerate(all_dags):\n                        shd_array[i] = shd(\n                            gt.to_numpy(dtype=np.int8),\n                            each_dag.adjacency_matrix.reindex(\n                                index=original_order, columns=original_order\n                            ).to_numpy(dtype=np.int8),\n                        )[1]\n                    learned_dag = all_dags[shd_array.argmin()]\n                except ValueError:\n                    learned_dag = learned_graph\n            else:\n                raise NotImplementedError()\n\n            learned_adjacency_matrix = learned_dag.adjacency_matrix\n            learned_causal_order = learned_dag.causal_order\n\n        elif isinstance(learned_graph, DAG):\n            learned_adjacency_matrix = learned_graph.adjacency_matrix\n            learned_causal_order = learned_graph.causal_order\n\n        else:\n            raise ValueError(\"Something went wrong, result is neither `DAG` nor `CPDAG`.\")\n\n        return learned_adjacency_matrix, learned_causal_order\n\n    def run_benchmark(\n        self,\n        params: ExperimentParams,\n        num_runs: int = 30,\n        metrics: list[str] = [\"shd\", \"sid\", \"ancestor_aid\", \"ancester_ordering_aid\"],\n        cpdag_strategy: str = \"random_dag\",\n    ) -&gt; defaultdict[str, defaultdict[str, list[int | float | None]]]:\n        \"\"\"Benchmark run.\n\n        Args:\n            num_runs (int, optional): _description_. Defaults to 30.\n            params (ExperimenParams): Params to benchmark\n            metrics (list[str], optional): _description_. Defaults to\n                [\"shd\", \"sid\", \"ancestor_aid\", \"ancester_ordering_aid\"].\n            cpdag_strategy (str): How to convert CPDAG to DAG to enable fair comparison.\n                Options are `random_dag` (default) and `best_dag`. In the former, a random\n                DAG is chosen from the MEC, in the latter the SHD is calculated for all DAGs\n                in the MEC and the best one is chosen.\n        \"\"\"\n        results: defaultdict[str, defaultdict[str, list[int | float | None]]] = defaultdict(\n            partial(defaultdict, list)\n        )\n        for run in range(num_runs):\n            # Generate data to also generate a new DAG\n            try:\n                ldat = params.make_data()\n                data, _ = ldat.generate_data(num_samples=params.number_of_samples)\n\n                layering = getattr(ldat.dag, \"layering\", None)\n                gt = ldat.dag.adjacency_matrix\n\n                for algo, name in params:\n                    learned_graph = algo.learn_graph(\n                        data_dict=data,\n                        layering=layering,\n                    )\n\n                    learned_adjacency_matrix, learned_causal_order = self._cpdag_dag_processing(\n                        gt=gt, learned_graph=learned_graph, cpdag_strategy=cpdag_strategy\n                    )\n\n                    for metric in metrics:\n                        _, resulting_metric = _metric_mapping[metric](\n                            gt,\n                            learned_adjacency_matrix,\n                            learned_causal_order,\n                        )\n\n                        if isinstance(resulting_metric, np.ndarray):\n                            resulting_metric = None\n\n                        results[name][metric].append(resulting_metric)\n            except Exception as e:\n                print(f\"Error in run {run}: {e}. Skipping this run.\")\n                continue\n        return results\n\n    def write_results(\n        self, results: defaultdict[str, defaultdict[str, list[int | float | None]]], path: str\n    ) -&gt; None:\n        \"\"\"Write results to JSON files with time signature.\n\n        Args:\n            results (defaultdict[str, defaultdict[str, list[int | float | None]]]): Result\n                dict from a benchmark run.\n            path (str): Destination path where to save files.\n        \"\"\"\n        now = datetime.now()\n        current_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n        with open(path + f\"/results_{current_time}.json\", \"w\", encoding=\"utf-8\") as outfile:\n            json.dump(results, outfile)\n</code></pre>"},{"location":"api/#gresit.simulation_utils.BenchMarker.run_benchmark","title":"<code>run_benchmark(params, num_runs=30, metrics=['shd', 'sid', 'ancestor_aid', 'ancester_ordering_aid'], cpdag_strategy='random_dag')</code>","text":"<p>Benchmark run.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>description. Defaults to 30.</p> <code>30</code> <code>params</code> <code>ExperimenParams</code> <p>Params to benchmark</p> required <code>metrics</code> <code>list[str]</code> <p>description. Defaults to [\"shd\", \"sid\", \"ancestor_aid\", \"ancester_ordering_aid\"].</p> <code>['shd', 'sid', 'ancestor_aid', 'ancester_ordering_aid']</code> <code>cpdag_strategy</code> <code>str</code> <p>How to convert CPDAG to DAG to enable fair comparison. Options are <code>random_dag</code> (default) and <code>best_dag</code>. In the former, a random DAG is chosen from the MEC, in the latter the SHD is calculated for all DAGs in the MEC and the best one is chosen.</p> <code>'random_dag'</code> Source code in <code>gresit/simulation_utils.py</code> <pre><code>def run_benchmark(\n    self,\n    params: ExperimentParams,\n    num_runs: int = 30,\n    metrics: list[str] = [\"shd\", \"sid\", \"ancestor_aid\", \"ancester_ordering_aid\"],\n    cpdag_strategy: str = \"random_dag\",\n) -&gt; defaultdict[str, defaultdict[str, list[int | float | None]]]:\n    \"\"\"Benchmark run.\n\n    Args:\n        num_runs (int, optional): _description_. Defaults to 30.\n        params (ExperimenParams): Params to benchmark\n        metrics (list[str], optional): _description_. Defaults to\n            [\"shd\", \"sid\", \"ancestor_aid\", \"ancester_ordering_aid\"].\n        cpdag_strategy (str): How to convert CPDAG to DAG to enable fair comparison.\n            Options are `random_dag` (default) and `best_dag`. In the former, a random\n            DAG is chosen from the MEC, in the latter the SHD is calculated for all DAGs\n            in the MEC and the best one is chosen.\n    \"\"\"\n    results: defaultdict[str, defaultdict[str, list[int | float | None]]] = defaultdict(\n        partial(defaultdict, list)\n    )\n    for run in range(num_runs):\n        # Generate data to also generate a new DAG\n        try:\n            ldat = params.make_data()\n            data, _ = ldat.generate_data(num_samples=params.number_of_samples)\n\n            layering = getattr(ldat.dag, \"layering\", None)\n            gt = ldat.dag.adjacency_matrix\n\n            for algo, name in params:\n                learned_graph = algo.learn_graph(\n                    data_dict=data,\n                    layering=layering,\n                )\n\n                learned_adjacency_matrix, learned_causal_order = self._cpdag_dag_processing(\n                    gt=gt, learned_graph=learned_graph, cpdag_strategy=cpdag_strategy\n                )\n\n                for metric in metrics:\n                    _, resulting_metric = _metric_mapping[metric](\n                        gt,\n                        learned_adjacency_matrix,\n                        learned_causal_order,\n                    )\n\n                    if isinstance(resulting_metric, np.ndarray):\n                        resulting_metric = None\n\n                    results[name][metric].append(resulting_metric)\n        except Exception as e:\n            print(f\"Error in run {run}: {e}. Skipping this run.\")\n            continue\n    return results\n</code></pre>"},{"location":"api/#gresit.simulation_utils.BenchMarker.write_results","title":"<code>write_results(results, path)</code>","text":"<p>Write results to JSON files with time signature.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>defaultdict[str, defaultdict[str, list[int | float | None]]]</code> <p>Result dict from a benchmark run.</p> required <code>path</code> <code>str</code> <p>Destination path where to save files.</p> required Source code in <code>gresit/simulation_utils.py</code> <pre><code>def write_results(\n    self, results: defaultdict[str, defaultdict[str, list[int | float | None]]], path: str\n) -&gt; None:\n    \"\"\"Write results to JSON files with time signature.\n\n    Args:\n        results (defaultdict[str, defaultdict[str, list[int | float | None]]]): Result\n            dict from a benchmark run.\n        path (str): Destination path where to save files.\n    \"\"\"\n    now = datetime.now()\n    current_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n    with open(path + f\"/results_{current_time}.json\", \"w\", encoding=\"utf-8\") as outfile:\n        json.dump(results, outfile)\n</code></pre>"},{"location":"api/#gresit.simulation_utils.draw_result_boxplots","title":"<code>draw_result_boxplots(result_dict, title='', file_path=None, file_name='example_name')</code>","text":"<p>Draw full set of result boxplots.</p> <p>Parameters:</p> Name Type Description Default <code>result_dict</code> <code>dict</code> <p>description</p> required <code>title</code> <code>str</code> <p>title of the plot.</p> <code>''</code> <code>file_path</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>file_name</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>'example_name'</code> Source code in <code>gresit/simulation_utils.py</code> <pre><code>def draw_result_boxplots(\n    result_dict: defaultdict[str, defaultdict[str, list[float | None]]],\n    title: str = \"\",\n    file_path: str | None = None,\n    file_name: str | None = \"example_name\",\n) -&gt; None:\n    \"\"\"Draw full set of result boxplots.\n\n    Args:\n        result_dict (dict): _description_\n        title (str, optional): title of the plot.\n        file_path (str | None, optional): _description_. Defaults to None.\n        file_name (str | None, optional): _description_. Defaults to None.\n    \"\"\"\n    num_plots = len(result_dict[list(result_dict.keys())[0]].keys())\n    MAX_PLOT_PER_ROW = 3\n    if num_plots &gt; MAX_PLOT_PER_ROW:\n        num_rows = 2\n        num_cols = int(np.ceil(num_plots / num_rows))\n\n    fig, ax = plt.subplots(\n        figsize=(18, 4.2),\n        nrows=num_rows,\n        ncols=num_cols,\n        sharex=False,\n        sharey=True,\n    )\n\n    valid_axes = [axi for axi in ax.flat[:] if axi in fig.axes]\n    if (num_plots % 2) != 0:\n        fig.delaxes(ax[0, -1])\n        valid_axes = [axi for axi in ax.flat[:] if axi in fig.axes]\n\n    for axi, metric in zip(\n        valid_axes,\n        result_dict[list(result_dict.keys())[0]].keys(),\n    ):\n        _draw_full_boxplot(\n            result_dict=result_dict,\n            position=axi,\n            title=\"\",\n            x_label=_metric_names[metric],\n            metric=metric,\n        )\n\n    plt.figtext(\n        0.16,\n        1.01,\n        r\"$\\leftarrow$ lower is better\",\n        horizontalalignment=\"right\",\n        fontsize=\"x-small\",\n    )\n    plt.figtext(\n        0.41,\n        1.01,\n        r\"$\\rightarrow$ higher is better\",\n        horizontalalignment=\"right\",\n        fontsize=\"x-small\",\n    )\n\n    fig.tight_layout()\n    fig.suptitle(title)\n    fig.subplots_adjust(top=0.88)\n    if file_path is not None:\n        plt.savefig(f\"{file_path}/{file_name}.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/#utils","title":"Utils","text":"<p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Loss functions for training the model.</p> <p><code>loss_hsic()</code> taken from https://github.com/danielgreenfeld3/XIC/blob/master/hsic.py as used in paper https://arxiv.org/abs/1910.00270</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p> <p>Utility classes and functions related to gresit.</p> <p>Copyright (c) 2025 Robert Bosch GmbH</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"api/#gresit.losses.GaussianKernelMatrix","title":"<code>GaussianKernelMatrix(x, sigma=None)</code>","text":"<p>Get gaussian kernel matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>description</p> required <code>sigma</code> <code>float</code> <p>description. Defaults the median heuristic when None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>_type_</code> <code>Tensor</code> <p>description</p> Source code in <code>gresit/losses.py</code> <pre><code>def GaussianKernelMatrix(x: torch.Tensor, sigma: float | None = None) -&gt; torch.Tensor:\n    \"\"\"Get gaussian kernel matrix.\n\n    Args:\n        x (torch.Tensor): _description_\n        sigma (float, optional): _description_. Defaults the median heuristic when None.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    pairwise_distances_ = pairwise_distances(x)\n    if sigma is None:\n        sigma = _median_heuristic(pairwise_distances_)\n    return torch.exp(-pairwise_distances_ / sigma)\n</code></pre>"},{"location":"api/#gresit.losses.HSIC","title":"<code>HSIC(x, y, s_x=None, s_y=None, device='cpu')</code>","text":"<p>Get test statistic.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>description</p> required <code>y</code> <code>Tensor</code> <p>description</p> required <code>s_x</code> <code>float</code> <p>description. Defaults to None.</p> <code>None</code> <code>s_y</code> <code>float</code> <p>description. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>description. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def HSIC(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    s_x: float | None = None,\n    s_y: float | None = None,\n    device: str = \"cpu\",\n) -&gt; torch.Tensor:\n    \"\"\"Get test statistic.\n\n    Args:\n        x (torch.Tensor): _description_\n        y (torch.Tensor): _description_\n        s_x (float, optional): _description_. Defaults to None.\n        s_y (float, optional): _description_. Defaults to None.\n        device (str, optional): _description_. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    m = x.shape[0]\n    K = GaussianKernelMatrix(x, s_x)\n    L = GaussianKernelMatrix(y, s_y)\n\n    H = torch.eye(m) - 1.0 / m * torch.ones((m, m))\n    H = H.float().to(device)\n    return torch.trace(torch.mm(L, torch.mm(H, torch.mm(K, H)))) / ((m - 1) ** 2)\n</code></pre>"},{"location":"api/#gresit.losses.loss_disco","title":"<code>loss_disco(x, y_pred, y_true, device='cpu')</code>","text":"<p>Get UCRT loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>description</p> required <code>y_pred</code> <code>Tensor</code> <p>description</p> required <code>y_true</code> <code>Tensor</code> <p>description</p> required <code>device</code> <code>str</code> <p>description. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def loss_disco(\n    x: torch.Tensor,\n    y_pred: torch.Tensor,\n    y_true: torch.Tensor,\n    device: str = \"cpu\",\n) -&gt; torch.Tensor:\n    \"\"\"Get UCRT loss.\n\n    Args:\n        x (torch.Tensor): _description_\n        y_pred (torch.Tensor): _description_\n        y_true (torch.Tensor): _description_\n        device (str, optional): _description_. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    r = y_pred - y_true\n    return u_distance_cov_squared(a=x, b=r, device=device)\n</code></pre>"},{"location":"api/#gresit.losses.loss_hsic","title":"<code>loss_hsic(x, y_pred, y_true, device='cpu')</code>","text":"<p>Get HSIC loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>description</p> required <code>y_pred</code> <code>Tensor</code> <p>description</p> required <code>y_true</code> <code>Tensor</code> <p>description</p> required <code>device</code> <code>str</code> <p>description. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def loss_hsic(\n    x: torch.Tensor,\n    y_pred: torch.Tensor,\n    y_true: torch.Tensor,\n    device: str = \"cpu\",\n) -&gt; torch.Tensor:\n    \"\"\"Get HSIC loss.\n\n    Args:\n        x (torch.Tensor): _description_\n        y_pred (torch.Tensor): _description_\n        y_true (torch.Tensor): _description_\n        device (str, optional): _description_. Defaults to \"cpu\".\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    r = y_pred - y_true\n    return HSIC(x=x, y=r, device=device)\n</code></pre>"},{"location":"api/#gresit.losses.loss_mse","title":"<code>loss_mse(x, y_pred, y_true, device='cpu')</code>","text":"<p>MSE loss.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def loss_mse(\n    x: torch.Tensor,\n    y_pred: torch.Tensor,\n    y_true: torch.Tensor,\n    device: str = \"cpu\",\n) -&gt; torch.Tensor:\n    \"\"\"MSE loss.\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    _ = x  # Ignored on purpose, kept for compatibility\n    return ((y_pred - y_true) ** 2).mean()\n</code></pre>"},{"location":"api/#gresit.losses.pairwise_distances","title":"<code>pairwise_distances(x)</code>","text":"<p>Get pairwise distance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>description</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def pairwise_distances(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get pairwise distance.\n\n    Args:\n        x (torch.Tensor): _description_\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    instances_norm = torch.sum(x**2, -1).reshape((-1, 1))\n    return -2 * torch.mm(x, x.t()) + instances_norm + instances_norm.t()\n</code></pre>"},{"location":"api/#gresit.losses.u_distance_cov_squared","title":"<code>u_distance_cov_squared(a, b, device='cpu', robust=False)</code>","text":"<p>Unbiased squared distance covariance.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Tensor</code> <p>description</p> required <code>b</code> <code>Tensor</code> <p>description</p> required <code>device</code> <code>str</code> <p>device to run on.</p> <code>'cpu'</code> <code>robust</code> <code>bool</code> <p>if you want to make sure that the joint finite first moment condition for the distance covariance holds, setting <code>robust</code> to <code>True</code> transforms data to univariate ranks.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: description</p> Source code in <code>gresit/losses.py</code> <pre><code>def u_distance_cov_squared(\n    a: torch.Tensor, b: torch.Tensor, device: str = \"cpu\", robust: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Unbiased squared distance covariance.\n\n    Args:\n        a (torch.Tensor): _description_\n        b (torch.Tensor): _description_\n        device (str): device to run on.\n        robust (bool): if you want to make sure that the joint finite first moment\n            condition for the distance covariance holds, setting `robust` to `True`\n            transforms data to univariate ranks.\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    if robust:\n        a = _univariate_ranks(a)\n        b = _univariate_ranks(b)\n    n = a.shape[0]\n    a_distance = _pairwise_distance_matrix(a)\n    b_distance = _pairwise_distance_matrix(b)\n\n    a_ij = _u_centered_matrix(a_distance)\n    b_ij = _u_centered_matrix(b_distance)\n\n    return (a_ij * b_ij).sum(dtype=torch.float32) / (n * (n - 3))\n</code></pre>"},{"location":"api/#gresit.regression_techniques.BoostedRegressionTrees","title":"<code>BoostedRegressionTrees</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>Boosted multi-outcome regression trees.</p> <p>This is simply a wrapper around the xgboost <code>XGBRegressor</code> class.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>class BoostedRegressionTrees(MultiRegressor):\n    \"\"\"Boosted multi-outcome regression trees.\n\n    This is simply a wrapper around the xgboost `XGBRegressor` class.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the `XGBRegressor` object.\"\"\"\n        self.clf = xgb.XGBRegressor(tree_method=\"hist\", multi_strategy=\"multi_output_tree\")\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        \"\"\"Fit boosted multi-outcome regression trees to training data.\n\n        Args:\n            X (np.ndarray): training data predictors\n            Y (np.ndarray): training data (multi-) targets\n        \"\"\"\n        self.clf.fit(X, Y)\n        self._X_test = X\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Predict using fitted boosted multi-outcome regression trees.\n\n        Args:\n            X_test (np.ndarray): Test data to predict on.\n\n        Returns:\n            np.ndarray: Predicted values.\n        \"\"\"\n        if X_test is None:\n            X_test = self._X_test\n        return self.clf.predict(X_test)\n</code></pre>"},{"location":"api/#gresit.regression_techniques.BoostedRegressionTrees.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the <code>XGBRegressor</code> object.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the `XGBRegressor` object.\"\"\"\n    self.clf = xgb.XGBRegressor(tree_method=\"hist\", multi_strategy=\"multi_output_tree\")\n</code></pre>"},{"location":"api/#gresit.regression_techniques.BoostedRegressionTrees.fit","title":"<code>fit(X, Y)</code>","text":"<p>Fit boosted multi-outcome regression trees to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data predictors</p> required <code>Y</code> <code>ndarray</code> <p>training data (multi-) targets</p> required Source code in <code>gresit/regression_techniques.py</code> <pre><code>def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n    \"\"\"Fit boosted multi-outcome regression trees to training data.\n\n    Args:\n        X (np.ndarray): training data predictors\n        Y (np.ndarray): training data (multi-) targets\n    \"\"\"\n    self.clf.fit(X, Y)\n    self._X_test = X\n</code></pre>"},{"location":"api/#gresit.regression_techniques.BoostedRegressionTrees.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Predict using fitted boosted multi-outcome regression trees.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data to predict on.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted values.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Predict using fitted boosted multi-outcome regression trees.\n\n    Args:\n        X_test (np.ndarray): Test data to predict on.\n\n    Returns:\n        np.ndarray: Predicted values.\n    \"\"\"\n    if X_test is None:\n        X_test = self._X_test\n    return self.clf.predict(X_test)\n</code></pre>"},{"location":"api/#gresit.regression_techniques.CurdsWhey","title":"<code>CurdsWhey</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>Breiman and Friedman's curds and whey multivariate regression model.</p> <p>When regression problem is of multivariate nature and the outcome variables are related among another, more accurate predictions may be obtained by using a linear combination of the OLS predictors.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>class CurdsWhey(MultiRegressor):\n    \"\"\"Breiman and Friedman's curds and whey multivariate regression model.\n\n    When regression problem is of multivariate nature and the outcome variables are\n    related among another, more accurate predictions may be obtained by using a linear combination\n    of the OLS predictors.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes C&amp;W linear shrinkage method.\"\"\"\n        self._ols: sklearn.linear_model.LinearRegression\n        self._T: np.ndarray\n        self._D: np.ndarray\n        self._response_transformed: StandardScaler\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        \"\"\"Fits the C&amp;W model to the multivariate X and Y.\n\n        Args:\n            X (np.ndarray): Matrix of predictors\n            Y (np.ndarray): Matrix of responses\n        \"\"\"\n        n = X.shape[0]\n        p = X.shape[1]\n        r = p / n\n        q = Y.shape[1]\n\n        Y_transform = StandardScaler().fit(Y)\n        Y_std = Y_transform.transform(Y)\n\n        cca = CCA(n_components=q)\n        cca.fit(X, Y_std)\n        X_c, Y_c = cca.transform(X, Y_std)\n        c_k = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=q)\n        T = cca.y_rotations_\n\n        # Computing Diagonal Shrinkage\n        denom_1 = (c_k**2) * ((1 - r) ** 2)\n        denom_2 = (1 - c_k**2) * (r**2)\n        numer = (1 - r) * (c_k**2 - r)\n        ds = numer / (denom_1 + denom_2)\n        ds[ds &lt; 0] = 0\n        D = np.diag(ds)\n\n        # Predicting values\n        ols_cw = LinearRegression().fit(X, Y_std)\n        self._ols = ols_cw\n        self._T = T\n        self._D = D\n        self._response_transformed = Y_transform\n        self._X_test = X\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Predict response matrix based on X data.\n\n        Args:\n            X_test (np.ndarray): Matrix of responses\n\n        Returns:\n            np.ndarray: predicted values.\n        \"\"\"\n        if X_test is None:\n            X_test = self._X_test\n        Y_tilde = self._ols.predict(X=X_test)\n        return self._response_transformed.inverse_transform(\n            Y_tilde @ self._T @ self._D @ np.linalg.inv(self._T)\n        )\n</code></pre>"},{"location":"api/#gresit.regression_techniques.CurdsWhey.__init__","title":"<code>__init__()</code>","text":"<p>Initializes C&amp;W linear shrinkage method.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes C&amp;W linear shrinkage method.\"\"\"\n    self._ols: sklearn.linear_model.LinearRegression\n    self._T: np.ndarray\n    self._D: np.ndarray\n    self._response_transformed: StandardScaler\n</code></pre>"},{"location":"api/#gresit.regression_techniques.CurdsWhey.fit","title":"<code>fit(X, Y)</code>","text":"<p>Fits the C&amp;W model to the multivariate X and Y.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Matrix of predictors</p> required <code>Y</code> <code>ndarray</code> <p>Matrix of responses</p> required Source code in <code>gresit/regression_techniques.py</code> <pre><code>def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n    \"\"\"Fits the C&amp;W model to the multivariate X and Y.\n\n    Args:\n        X (np.ndarray): Matrix of predictors\n        Y (np.ndarray): Matrix of responses\n    \"\"\"\n    n = X.shape[0]\n    p = X.shape[1]\n    r = p / n\n    q = Y.shape[1]\n\n    Y_transform = StandardScaler().fit(Y)\n    Y_std = Y_transform.transform(Y)\n\n    cca = CCA(n_components=q)\n    cca.fit(X, Y_std)\n    X_c, Y_c = cca.transform(X, Y_std)\n    c_k = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=q)\n    T = cca.y_rotations_\n\n    # Computing Diagonal Shrinkage\n    denom_1 = (c_k**2) * ((1 - r) ** 2)\n    denom_2 = (1 - c_k**2) * (r**2)\n    numer = (1 - r) * (c_k**2 - r)\n    ds = numer / (denom_1 + denom_2)\n    ds[ds &lt; 0] = 0\n    D = np.diag(ds)\n\n    # Predicting values\n    ols_cw = LinearRegression().fit(X, Y_std)\n    self._ols = ols_cw\n    self._T = T\n    self._D = D\n    self._response_transformed = Y_transform\n    self._X_test = X\n</code></pre>"},{"location":"api/#gresit.regression_techniques.CurdsWhey.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Predict response matrix based on X data.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Matrix of responses</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: predicted values.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Predict response matrix based on X data.\n\n    Args:\n        X_test (np.ndarray): Matrix of responses\n\n    Returns:\n        np.ndarray: predicted values.\n    \"\"\"\n    if X_test is None:\n        X_test = self._X_test\n    Y_tilde = self._ols.predict(X=X_test)\n    return self._response_transformed.inverse_transform(\n        Y_tilde @ self._T @ self._D @ np.linalg.inv(self._T)\n    )\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor","title":"<code>MultiRegressor</code>","text":"<p>Abstract class for multivariate regression.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>class MultiRegressor(metaclass=ABCMeta):\n    \"\"\"Abstract class for multivariate regression.\"\"\"\n\n    def __init__(\n        self,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        **kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Base class for regressors.\"\"\"\n        self.rng = rng\n\n    @abstractmethod\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        \"\"\"Fit the model.\n\n        Args:\n            X (np.ndarray): Matrix of predictors\n            Y (np.ndarray): Matrix of responses\n        \"\"\"\n\n    @abstractmethod\n    def predict(self, X_test: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict given data matrix.\n\n        Args:\n            X_test (np.ndarray): data matrix to predict on.\n\n        Returns:\n            np.ndarray: (Matrix of) predicted values\n        \"\"\"\n\n    def mse(self, Y_test: np.ndarray, X_test: np.ndarray) -&gt; float:\n        \"\"\"Mean squared error.\n\n        Args:\n            Y_test (np.ndarray): Test response\n            X_test (np.ndarray): Test predictors\n\n        Returns:\n            float: MSE\n        \"\"\"\n        Yhat = self.predict(X_test)\n        mse: float = (np.square(Y_test - Yhat) / np.prod(Y_test.shape)).mean()\n        return mse\n\n    def standardize(self, a: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Standardize data.\n\n        Args:\n            a (np.ndarray): _description_\n\n        Returns:\n            np.ndarray: _description_\n        \"\"\"\n        return (a - np.mean(a, axis=0)) / np.std(a, axis=0)\n\n    def split_and_standardize(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        test_size: float = 0.2,\n    ) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Train test split and standardize data.\n\n        Args:\n            X (np.ndarray): predictors\n            Y (np.ndarray): targets\n            test_size (float): Size of test data\n\n        Returns:\n            tuple[np.ndarray]: X_train, X_test, Y_train, Y_test\n        \"\"\"\n        X_train, X_test, Y_train, Y_test = train_test_split(\n            X, Y, test_size=test_size, random_state=2024\n        )\n\n        mean = np.mean(X_train, axis=0)\n        std = np.std(X_train, axis=0)\n        X_train = (X_train - mean) / std\n        X_test = (X_test - mean) / std\n\n        return X_train, X_test, Y_train, Y_test\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.__init__","title":"<code>__init__(rng=np.random.default_rng(seed=2024), **kwargs)</code>","text":"<p>Base class for regressors.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __init__(\n    self,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    **kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Base class for regressors.\"\"\"\n    self.rng = rng\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.fit","title":"<code>fit(X, Y)</code>  <code>abstractmethod</code>","text":"<p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Matrix of predictors</p> required <code>Y</code> <code>ndarray</code> <p>Matrix of responses</p> required Source code in <code>gresit/regression_techniques.py</code> <pre><code>@abstractmethod\ndef fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n    \"\"\"Fit the model.\n\n    Args:\n        X (np.ndarray): Matrix of predictors\n        Y (np.ndarray): Matrix of responses\n    \"\"\"\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.mse","title":"<code>mse(Y_test, X_test)</code>","text":"<p>Mean squared error.</p> <p>Parameters:</p> Name Type Description Default <code>Y_test</code> <code>ndarray</code> <p>Test response</p> required <code>X_test</code> <code>ndarray</code> <p>Test predictors</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>MSE</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def mse(self, Y_test: np.ndarray, X_test: np.ndarray) -&gt; float:\n    \"\"\"Mean squared error.\n\n    Args:\n        Y_test (np.ndarray): Test response\n        X_test (np.ndarray): Test predictors\n\n    Returns:\n        float: MSE\n    \"\"\"\n    Yhat = self.predict(X_test)\n    mse: float = (np.square(Y_test - Yhat) / np.prod(Y_test.shape)).mean()\n    return mse\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.predict","title":"<code>predict(X_test)</code>  <code>abstractmethod</code>","text":"<p>Predict given data matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>data matrix to predict on.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (Matrix of) predicted values</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>@abstractmethod\ndef predict(self, X_test: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict given data matrix.\n\n    Args:\n        X_test (np.ndarray): data matrix to predict on.\n\n    Returns:\n        np.ndarray: (Matrix of) predicted values\n    \"\"\"\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.split_and_standardize","title":"<code>split_and_standardize(X, Y, test_size=0.2)</code>","text":"<p>Train test split and standardize data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>predictors</p> required <code>Y</code> <code>ndarray</code> <p>targets</p> required <code>test_size</code> <code>float</code> <p>Size of test data</p> <code>0.2</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>tuple[np.ndarray]: X_train, X_test, Y_train, Y_test</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def split_and_standardize(\n    self,\n    X: np.ndarray,\n    Y: np.ndarray,\n    test_size: float = 0.2,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Train test split and standardize data.\n\n    Args:\n        X (np.ndarray): predictors\n        Y (np.ndarray): targets\n        test_size (float): Size of test data\n\n    Returns:\n        tuple[np.ndarray]: X_train, X_test, Y_train, Y_test\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(\n        X, Y, test_size=test_size, random_state=2024\n    )\n\n    mean = np.mean(X_train, axis=0)\n    std = np.std(X_train, axis=0)\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n\n    return X_train, X_test, Y_train, Y_test\n</code></pre>"},{"location":"api/#gresit.regression_techniques.MultiRegressor.standardize","title":"<code>standardize(a)</code>","text":"<p>Standardize data.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>description</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: description</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def standardize(self, a: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Standardize data.\n\n    Args:\n        a (np.ndarray): _description_\n\n    Returns:\n        np.ndarray: _description_\n    \"\"\"\n    return (a - np.mean(a, axis=0)) / np.std(a, axis=0)\n</code></pre>"},{"location":"api/#gresit.regression_techniques.ReducedRankRegressor","title":"<code>ReducedRankRegressor</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>Kernel Reduced Rank Ridge Regression.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>class ReducedRankRegressor(MultiRegressor):  # , BaseEstimator):\n    \"\"\"Kernel Reduced Rank Ridge Regression.\"\"\"\n\n    def __init__(self, rank: int, alpha: np.float64 = 1.0) -&gt; None:\n        \"\"\"Initializes the model.\n\n        Args:\n            rank (int): Rank constraint.\n            alpha (np.float64, optional): Regularization parameter. Defaults to 1.0.\n        \"\"\"\n        self.rank = rank\n        self.alpha = alpha\n        self._P_rr: np.ndarray\n        self._Q_fr: np.ndarray\n        self._X_train: np.ndarray\n        self._X_test: np.ndarray\n        self._Y_test: np.ndarray | None = None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Method print.\"\"\"\n        return f\"kernel Reduced Rank Ridge Regression \\\n            by Mukherjee (rank = f{self.rank})\"\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        \"\"\"Fit kRRR model to data.\n\n        Args:\n            X (np.ndarray): training data predictors\n            Y (np.ndarray): training (multivariate) response\n        \"\"\"\n        K_X: np.ndarray = np.dot(X, X.T)\n        tmp_1 = self.alpha * np.identity(K_X.shape[0]) + K_X\n        Q_fr = np.linalg.solve(tmp_1, Y)\n        P_fr = np.linalg.eig(np.dot(Y.T, np.dot(K_X, Q_fr)))[1].real\n        P_rr = np.dot(P_fr[:, 0 : self.rank], P_fr[:, 0 : self.rank].T)\n\n        self._Q_fr = Q_fr\n        self._P_rr = P_rr\n        self._X_train = X\n        self._X_test = X\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Predict fitted kRRR model.\n\n        Args:\n            X_test (np.ndarray): Test data to predict on.\n\n        Returns:\n            np.ndarray: Predicted values.\n        \"\"\"\n        if X_test is None:\n            X_test = self._X_test\n        K_Xx = np.dot(X_test, self._X_train.T)\n\n        return np.dot(K_Xx, np.dot(self._Q_fr, self._P_rr))\n</code></pre>"},{"location":"api/#gresit.regression_techniques.ReducedRankRegressor.__init__","title":"<code>__init__(rank, alpha=1.0)</code>","text":"<p>Initializes the model.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Rank constraint.</p> required <code>alpha</code> <code>float64</code> <p>Regularization parameter. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __init__(self, rank: int, alpha: np.float64 = 1.0) -&gt; None:\n    \"\"\"Initializes the model.\n\n    Args:\n        rank (int): Rank constraint.\n        alpha (np.float64, optional): Regularization parameter. Defaults to 1.0.\n    \"\"\"\n    self.rank = rank\n    self.alpha = alpha\n    self._P_rr: np.ndarray\n    self._Q_fr: np.ndarray\n    self._X_train: np.ndarray\n    self._X_test: np.ndarray\n    self._Y_test: np.ndarray | None = None\n</code></pre>"},{"location":"api/#gresit.regression_techniques.ReducedRankRegressor.__str__","title":"<code>__str__()</code>","text":"<p>Method print.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Method print.\"\"\"\n    return f\"kernel Reduced Rank Ridge Regression \\\n        by Mukherjee (rank = f{self.rank})\"\n</code></pre>"},{"location":"api/#gresit.regression_techniques.ReducedRankRegressor.fit","title":"<code>fit(X, Y)</code>","text":"<p>Fit kRRR model to data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data predictors</p> required <code>Y</code> <code>ndarray</code> <p>training (multivariate) response</p> required Source code in <code>gresit/regression_techniques.py</code> <pre><code>def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n    \"\"\"Fit kRRR model to data.\n\n    Args:\n        X (np.ndarray): training data predictors\n        Y (np.ndarray): training (multivariate) response\n    \"\"\"\n    K_X: np.ndarray = np.dot(X, X.T)\n    tmp_1 = self.alpha * np.identity(K_X.shape[0]) + K_X\n    Q_fr = np.linalg.solve(tmp_1, Y)\n    P_fr = np.linalg.eig(np.dot(Y.T, np.dot(K_X, Q_fr)))[1].real\n    P_rr = np.dot(P_fr[:, 0 : self.rank], P_fr[:, 0 : self.rank].T)\n\n    self._Q_fr = Q_fr\n    self._P_rr = P_rr\n    self._X_train = X\n    self._X_test = X\n</code></pre>"},{"location":"api/#gresit.regression_techniques.ReducedRankRegressor.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Predict fitted kRRR model.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>Test data to predict on.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted values.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Predict fitted kRRR model.\n\n    Args:\n        X_test (np.ndarray): Test data to predict on.\n\n    Returns:\n        np.ndarray: Predicted values.\n    \"\"\"\n    if X_test is None:\n        X_test = self._X_test\n    K_Xx = np.dot(X_test, self._X_train.T)\n\n    return np.dot(K_Xx, np.dot(self._Q_fr, self._P_rr))\n</code></pre>"},{"location":"api/#gresit.regression_techniques.SimultaneousLinearModel","title":"<code>SimultaneousLinearModel</code>","text":"<p>               Bases: <code>MultiRegressor</code></p> <p>Class for performing multivariate linear Regression.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>class SimultaneousLinearModel(MultiRegressor):\n    \"\"\"Class for performing multivariate linear Regression.\"\"\"\n\n    def __init__(\n        self,\n        rng: np.random.Generator = np.random.default_rng(seed=2024),\n        alpha: float = 0.1,\n    ) -&gt; None:\n        \"\"\"Initializes with a ridge penalty equal to 0.1.\n\n        Args:\n            rng (np.random.Generator): A random generator\n            alpha (float, optional): Penalty term. Defaults to 0.1.\n        \"\"\"\n        super().__init__(rng)\n\n        self.alpha: float = alpha\n        self.reg: Ridge\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        \"\"\"Fit multivariate linear regression.\n\n        Args:\n            X (np.ndarray): Matrix of predictors\n            Y (np.ndarray): Matrix of responses\n        \"\"\"\n        self._X_test: np.ndarray = X\n        self.reg = Ridge(alpha=self.alpha).fit(X, Y)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Class description.\"\"\"\n        return \"Multivariate Linear Regression\"\n\n    def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n        \"\"\"Predict given data matrix.\n\n        Args:\n            X_test (np.ndarray): data matrix to predict on.\n\n        Returns:\n            np.ndarray: (Matrix of) predicted values\n        \"\"\"\n        if X_test is None:\n            X_test = self._X_test\n        return self.reg.predict(X_test)\n</code></pre>"},{"location":"api/#gresit.regression_techniques.SimultaneousLinearModel.__init__","title":"<code>__init__(rng=np.random.default_rng(seed=2024), alpha=0.1)</code>","text":"<p>Initializes with a ridge penalty equal to 0.1.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>A random generator</p> <code>default_rng(seed=2024)</code> <code>alpha</code> <code>float</code> <p>Penalty term. Defaults to 0.1.</p> <code>0.1</code> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __init__(\n    self,\n    rng: np.random.Generator = np.random.default_rng(seed=2024),\n    alpha: float = 0.1,\n) -&gt; None:\n    \"\"\"Initializes with a ridge penalty equal to 0.1.\n\n    Args:\n        rng (np.random.Generator): A random generator\n        alpha (float, optional): Penalty term. Defaults to 0.1.\n    \"\"\"\n    super().__init__(rng)\n\n    self.alpha: float = alpha\n    self.reg: Ridge\n</code></pre>"},{"location":"api/#gresit.regression_techniques.SimultaneousLinearModel.__str__","title":"<code>__str__()</code>","text":"<p>Class description.</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Class description.\"\"\"\n    return \"Multivariate Linear Regression\"\n</code></pre>"},{"location":"api/#gresit.regression_techniques.SimultaneousLinearModel.fit","title":"<code>fit(X, Y)</code>","text":"<p>Fit multivariate linear regression.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Matrix of predictors</p> required <code>Y</code> <code>ndarray</code> <p>Matrix of responses</p> required Source code in <code>gresit/regression_techniques.py</code> <pre><code>def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n    \"\"\"Fit multivariate linear regression.\n\n    Args:\n        X (np.ndarray): Matrix of predictors\n        Y (np.ndarray): Matrix of responses\n    \"\"\"\n    self._X_test: np.ndarray = X\n    self.reg = Ridge(alpha=self.alpha).fit(X, Y)\n</code></pre>"},{"location":"api/#gresit.regression_techniques.SimultaneousLinearModel.predict","title":"<code>predict(X_test=None)</code>","text":"<p>Predict given data matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>ndarray</code> <p>data matrix to predict on.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (Matrix of) predicted values</p> Source code in <code>gresit/regression_techniques.py</code> <pre><code>def predict(self, X_test: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"Predict given data matrix.\n\n    Args:\n        X_test (np.ndarray): data matrix to predict on.\n\n    Returns:\n        np.ndarray: (Matrix of) predicted values\n    \"\"\"\n    if X_test is None:\n        X_test = self._X_test\n    return self.reg.predict(X_test)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG","title":"<code>DAG</code>","text":"<p>               Bases: <code>GRAPH</code></p> <p>General class for dealing with directed acyclic graph i.e.</p> <p>graphs that are directed and must not contain any cycles.</p> Source code in <code>gresit/graphs.py</code> <pre><code>class DAG(GRAPH):\n    \"\"\"General class for dealing with directed acyclic graph i.e.\n\n    graphs that are directed and must not contain any cycles.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: list[str] | None = None,\n        edges: list[tuple[str, str]] | None = None,\n    ) -&gt; None:\n        \"\"\"DAG constructor.\n\n        Args:\n            nodes (list[str] | None, optional): Nodes. Defaults to None.\n            edges (list[tuple[str,str]] | None, optional): Edges. Defaults to None.\n        \"\"\"\n        if nodes is None:\n            nodes = []\n        if edges is None:\n            edges = []\n\n        self._nodes: set[str] = set(nodes)\n        self._edges: set[tuple[str, str]] = set()\n        self._parents: defaultdict[str, set[str]] = defaultdict(set)\n        self._children: defaultdict[str, set[str]] = defaultdict(set)\n        self._random_state: np.random.Generator = np.random.default_rng(seed=2023)\n\n        for edge in edges:\n            self._add_edge(*edge)\n\n    def _add_node(self, node: str) -&gt; None:\n        self._nodes.add(node)\n\n    def _add_edge(self, i: str, j: str) -&gt; None:\n        self._nodes.add(i)\n        self._nodes.add(j)\n        self._edges.add((i, j))\n\n        # Check if graph is acyclic\n        if not self.is_acyclic():\n            raise ValueError(\n                \"The edge set you provided \\\n                induces one or more cycles.\\\n                Check your input!\"\n            )\n\n        self._children[i].add(j)\n        self._parents[j].add(i)\n\n    @property\n    def random_state(self) -&gt; np.random.Generator:\n        \"\"\"Current random state.\n\n        Returns:\n            np.random.Generator: Generator object.\n        \"\"\"\n        return self._random_state\n\n    @random_state.setter\n    def random_state(self, r: np.random.Generator) -&gt; None:\n        if not isinstance(r, np.random.Generator):\n            raise AssertionError(\"Specify numpy random number generator object!\")\n        self._random_state = r\n\n    def add_edge(self, edge: tuple[str, str]) -&gt; None:\n        \"\"\"Add edge to DAG.\n\n        Args:\n            edge (tuple[str, str]): Edge to add\n        \"\"\"\n        self._add_edge(*edge)\n\n    def add_node(self, node: str) -&gt; None:\n        \"\"\"Add node to DAG.\n\n        Args:\n            node (str): node to add\n        \"\"\"\n        self._add_node(node)\n\n    def add_edges_from(self, edges: list[tuple[str, str]]) -&gt; None:\n        \"\"\"Add multiple edges to DAG.\n\n        Args:\n            edges (list[tuple[str, str]]): Edges to add\n        \"\"\"\n        for edge in edges:\n            self.add_edge(edge=edge)\n\n    def add_nodes_from(self, nodes: list[str]) -&gt; None:\n        \"\"\"Add multiple nodes to DAG.\n\n        Args:\n            nodes (list[str]): nodes to add\n        \"\"\"\n        for node in nodes:\n            self.add_node(node)\n\n    def children(self, of_node: str) -&gt; list[str]:\n        \"\"\"Gives all children of node `node`.\n\n        Args:\n            of_node (str): node in current DAG.\n\n        Returns:\n            list: of children.\n        \"\"\"\n        if of_node in self._children.keys():\n            return list(self._children[of_node])\n        else:\n            return []\n\n    def parents(self, of_node: str) -&gt; list[str]:\n        \"\"\"Gives all parents of node `node`.\n\n        Args:\n            of_node (str): node in current DAG.\n\n        Returns:\n            list: of parents.\n        \"\"\"\n        if of_node in self._parents.keys():\n            return list(self._parents[of_node])\n        else:\n            return []\n\n    def induced_subgraph(self, nodes: list[str]) -&gt; DAG:\n        \"\"\"Returns the induced subgraph on the nodes in `nodes`.\n\n        Args:\n            nodes (list[str]): List of nodes.\n\n        Returns:\n            DAG: Induced subgraph.\n        \"\"\"\n        edges = [(i, j) for i, j in self.edges if i in nodes and j in nodes]\n        return DAG(nodes=nodes, edges=edges)\n\n    def is_adjacent(self, i: str, j: str) -&gt; bool:\n        \"\"\"Return True if the graph contains an directed edge between i and j.\n\n        Args:\n            i (str): node i.\n            j (str): node j.\n\n        Returns:\n            bool: True if i-&gt;j or i&lt;-j\n        \"\"\"\n        return (j, i) in self.edges or (i, j) in self.edges\n\n    def is_clique(self, potential_clique: set[str]) -&gt; bool:\n        \"\"\"Check every pair of node X potential_clique is adjacent.\"\"\"\n        return all(self.is_adjacent(i, j) for i, j in combinations(potential_clique, 2))\n\n    def is_acyclic(self) -&gt; bool:\n        \"\"\"Check if the graph is acyclic.\n\n        Returns:\n            bool: True if graph is acyclic.\n        \"\"\"\n        nx_dag = self.to_networkx()\n        acyclic: bool = nx.is_directed_acyclic_graph(nx_dag)\n        return acyclic\n\n    @classmethod\n    def from_pandas_adjacency(cls, pd_amat: pd.DataFrame, *args: Any, **kwargs: Any) -&gt; DAG:\n        \"\"\"Build DAG from a Pandas adjacency matrix.\n\n        Args:\n            pd_amat (pd.DataFrame): input adjacency matrix.\n            args (Any): Additional arguments.\n            kwargs (Any): Additional arguments.\n\n        Returns:\n            DAG\n        \"\"\"\n        assert pd_amat.shape[0] == pd_amat.shape[1]\n        nodes = pd_amat.columns\n\n        all_connections = []\n        start, end = np.where(pd_amat != 0)\n        for idx, _ in enumerate(start):\n            all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n        temp = [set(i) for i in all_connections]\n        temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n\n        dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n        return DAG(nodes=nodes, edges=dir_edges)\n\n    def remove_edge(self, i: str, j: str) -&gt; None:\n        \"\"\"Removes edge in question.\n\n        Args:\n            i (str): tail\n            j (str): head\n\n        Raises:\n            AssertionError: if edge does not exist\n        \"\"\"\n        if (i, j) not in self.edges:\n            raise AssertionError(\"Edge does not exist in current DAG\")\n\n        self._edges.discard((i, j))\n        self._children[i].discard(j)\n        self._parents[j].discard(i)\n\n    def remove_node(self, node: str) -&gt; None:\n        \"\"\"Remove a node from the graph.\"\"\"\n        self._nodes.remove(node)\n\n        self._edges = {(i, j) for i, j in self._edges if node not in (i, j)}\n\n        for child in self._children[node]:\n            self._parents[child].remove(node)\n\n        for parent in self._parents[node]:\n            self._children[parent].remove(node)\n\n        self._parents.pop(node, \"I was never here\")\n        self._children.pop(node, \"I was never here\")\n\n    @property\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Returns adjacency matrix.\n\n        The i,jth entry being one indicates that there is an edge\n        from i to j. A zero indicates that there is no edge.\n\n        Returns:\n            pd.DataFrame: adjacency matrix\n        \"\"\"\n        amat = pd.DataFrame(\n            np.zeros([self.num_nodes, self.num_nodes]),\n            index=self.nodes,\n            columns=self.nodes,\n        )\n        for edge in self.edges:\n            amat.loc[edge] = 1\n        return amat\n\n    def vstructs(self) -&gt; set[tuple[str, str]]:\n        \"\"\"Retrieve v-structures.\n\n        Returns:\n            set: set of all v-structures\n        \"\"\"\n        vstructures = set()\n        for node in self._nodes:\n            for p1, p2 in combinations(self._parents[node], 2):\n                if p1 not in self._parents[p2] and p2 not in self._parents[p1]:\n                    vstructures.add((p1, node))\n                    vstructures.add((p2, node))\n        return vstructures\n\n    def copy(self) -&gt; DAG:\n        \"\"\"Return a copy of the graph.\"\"\"\n        return DAG(nodes=list(self._nodes), edges=list(self._edges))\n\n    def show(self) -&gt; None:\n        \"\"\"Plot DAG.\"\"\"\n        graph = self.to_networkx()\n        pos = nx.circular_layout(graph)\n        nx.draw(graph, pos=pos, with_labels=True)\n\n    def to_networkx(self) -&gt; nx.DiGraph:\n        \"\"\"Convert to networkx graph.\n\n        Returns:\n            nx.MultiDiGraph: Graph with directed and undirected edges.\n        \"\"\"\n        nx_dag = nx.DiGraph()\n        nx_dag.add_nodes_from(self.nodes)\n        nx_dag.add_edges_from(self.edges)\n\n        return nx_dag\n\n    @property\n    def nodes(self) -&gt; list[str]:\n        \"\"\"Get all nods in current DAG.\n\n        Returns:\n            list: list of nodes.\n        \"\"\"\n        return sorted(list(self._nodes))\n\n    @property\n    def num_nodes(self) -&gt; int:\n        \"\"\"Number of nodes in current DAG.\n\n        Returns:\n            int: Number of nodes\n        \"\"\"\n        return len(self._nodes)\n\n    @property\n    def num_edges(self) -&gt; int:\n        \"\"\"Number of directed edges in current DAG.\n\n        Returns:\n            int: Number of directed edges\n        \"\"\"\n        return len(self._edges)\n\n    @property\n    def sparsity(self) -&gt; float:\n        \"\"\"Sparsity of the graph.\n\n        Returns:\n            float: in [0,1]\n        \"\"\"\n        s = self.num_nodes\n        return self.num_edges / s / (s - 1) * 2\n\n    @property\n    def edges(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Gives all directed edges in current DAG.\n\n        Returns:\n            list[tuple[str,str]]: List of directed edges.\n        \"\"\"\n        return list(self._edges)\n\n    @property\n    def causal_order(self) -&gt; list[str]:\n        \"\"\"Returns the causal order of the current graph.\n\n        Note that this order is in general not unique.\n\n        Returns:\n            list[str]: Causal order\n        \"\"\"\n        return list(nx.lexicographical_topological_sort(self.to_networkx()))\n\n    @property\n    def sink_nodes(self) -&gt; list[str]:\n        \"\"\"Returns all sink nodes, i.e.\n\n        nodes with no descendents in particular no children.\n\n        Returns:\n            list[str]: list of sink nodes.\n        \"\"\"\n        return [\n            s\n            for b, s in zip([self.children(of_node=node) == [] for node in self.nodes], self.nodes)\n            if b\n        ]\n\n    @property\n    def source_nodes(self) -&gt; list[str]:\n        \"\"\"Returns all source nodes, i.e.\n\n        nodes with no ancesters in particular no parents.\n\n        Returns:\n            list[str]: list of sink nodes.\n        \"\"\"\n        return [\n            s\n            for b, s in zip([self.parents(of_node=node) == [] for node in self.nodes], self.nodes)\n            if b\n        ]\n\n    @property\n    def max_in_degree(self) -&gt; int:\n        \"\"\"Maximum in-degree of the graph.\n\n        Returns:\n            int: Maximum in-degree\n        \"\"\"\n        return max(len(self._parents[node]) for node in self._nodes)\n\n    @property\n    def max_out_degree(self) -&gt; int:\n        \"\"\"Maximum out-degree of the graph.\n\n        Returns:\n            int: Maximum out-degree\n        \"\"\"\n        return max(len(self._children[node]) for node in self._nodes)\n\n    @classmethod\n    def from_nx(cls, nx_dag: nx.DiGraph, *args: Any, **kwargs: Any) -&gt; DAG:\n        \"\"\"Convert to DAG from nx.DiGraph.\n\n        Args:\n            nx_dag (nx.DiGraph): DAG in question.\n            args (Any): additional arguments\n            kwargs (Any): additional arguments\n\n        Raises:\n            TypeError: If DAG is not nx.DiGraph\n\n        Returns:\n            DAG\n        \"\"\"\n        if not isinstance(nx_dag, nx.DiGraph):\n            raise TypeError(\"DAG must be of type nx.DiGraph\")\n        return DAG(nodes=list(nx_dag.nodes), edges=list(nx_dag.edges))\n\n    def to_cpdag(self) -&gt; PDAG:\n        \"\"\"Convert DAG to CPDAG.\n\n        Returns:\n            PDAG: CPDAG representing the MEC.\n        \"\"\"\n        return dag2cpdag(dag=self.to_networkx())\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Returns adjacency matrix.</p> <p>The i,jth entry being one indicates that there is an edge from i to j. A zero indicates that there is no edge.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: adjacency matrix</p>"},{"location":"api/#gresit.graphs.DAG.causal_order","title":"<code>causal_order</code>  <code>property</code>","text":"<p>Returns the causal order of the current graph.</p> <p>Note that this order is in general not unique.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Causal order</p>"},{"location":"api/#gresit.graphs.DAG.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Gives all directed edges in current DAG.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str,str]]: List of directed edges.</p>"},{"location":"api/#gresit.graphs.DAG.max_in_degree","title":"<code>max_in_degree</code>  <code>property</code>","text":"<p>Maximum in-degree of the graph.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Maximum in-degree</p>"},{"location":"api/#gresit.graphs.DAG.max_out_degree","title":"<code>max_out_degree</code>  <code>property</code>","text":"<p>Maximum out-degree of the graph.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Maximum out-degree</p>"},{"location":"api/#gresit.graphs.DAG.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Get all nods in current DAG.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>list of nodes.</p>"},{"location":"api/#gresit.graphs.DAG.num_edges","title":"<code>num_edges</code>  <code>property</code>","text":"<p>Number of directed edges in current DAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of directed edges</p>"},{"location":"api/#gresit.graphs.DAG.num_nodes","title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Number of nodes in current DAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of nodes</p>"},{"location":"api/#gresit.graphs.DAG.random_state","title":"<code>random_state</code>  <code>property</code> <code>writable</code>","text":"<p>Current random state.</p> <p>Returns:</p> Type Description <code>Generator</code> <p>np.random.Generator: Generator object.</p>"},{"location":"api/#gresit.graphs.DAG.sink_nodes","title":"<code>sink_nodes</code>  <code>property</code>","text":"<p>Returns all sink nodes, i.e.</p> <p>nodes with no descendents in particular no children.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of sink nodes.</p>"},{"location":"api/#gresit.graphs.DAG.source_nodes","title":"<code>source_nodes</code>  <code>property</code>","text":"<p>Returns all source nodes, i.e.</p> <p>nodes with no ancesters in particular no parents.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of sink nodes.</p>"},{"location":"api/#gresit.graphs.DAG.sparsity","title":"<code>sparsity</code>  <code>property</code>","text":"<p>Sparsity of the graph.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>in [0,1]</p>"},{"location":"api/#gresit.graphs.DAG.__init__","title":"<code>__init__(nodes=None, edges=None)</code>","text":"<p>DAG constructor.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str] | None</code> <p>Nodes. Defaults to None.</p> <code>None</code> <code>edges</code> <code>list[tuple[str, str]] | None</code> <p>Edges. Defaults to None.</p> <code>None</code> Source code in <code>gresit/graphs.py</code> <pre><code>def __init__(\n    self,\n    nodes: list[str] | None = None,\n    edges: list[tuple[str, str]] | None = None,\n) -&gt; None:\n    \"\"\"DAG constructor.\n\n    Args:\n        nodes (list[str] | None, optional): Nodes. Defaults to None.\n        edges (list[tuple[str,str]] | None, optional): Edges. Defaults to None.\n    \"\"\"\n    if nodes is None:\n        nodes = []\n    if edges is None:\n        edges = []\n\n    self._nodes: set[str] = set(nodes)\n    self._edges: set[tuple[str, str]] = set()\n    self._parents: defaultdict[str, set[str]] = defaultdict(set)\n    self._children: defaultdict[str, set[str]] = defaultdict(set)\n    self._random_state: np.random.Generator = np.random.default_rng(seed=2023)\n\n    for edge in edges:\n        self._add_edge(*edge)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.add_edge","title":"<code>add_edge(edge)</code>","text":"<p>Add edge to DAG.</p> <p>Parameters:</p> Name Type Description Default <code>edge</code> <code>tuple[str, str]</code> <p>Edge to add</p> required Source code in <code>gresit/graphs.py</code> <pre><code>def add_edge(self, edge: tuple[str, str]) -&gt; None:\n    \"\"\"Add edge to DAG.\n\n    Args:\n        edge (tuple[str, str]): Edge to add\n    \"\"\"\n    self._add_edge(*edge)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.add_edges_from","title":"<code>add_edges_from(edges)</code>","text":"<p>Add multiple edges to DAG.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>list[tuple[str, str]]</code> <p>Edges to add</p> required Source code in <code>gresit/graphs.py</code> <pre><code>def add_edges_from(self, edges: list[tuple[str, str]]) -&gt; None:\n    \"\"\"Add multiple edges to DAG.\n\n    Args:\n        edges (list[tuple[str, str]]): Edges to add\n    \"\"\"\n    for edge in edges:\n        self.add_edge(edge=edge)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.add_node","title":"<code>add_node(node)</code>","text":"<p>Add node to DAG.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node to add</p> required Source code in <code>gresit/graphs.py</code> <pre><code>def add_node(self, node: str) -&gt; None:\n    \"\"\"Add node to DAG.\n\n    Args:\n        node (str): node to add\n    \"\"\"\n    self._add_node(node)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.add_nodes_from","title":"<code>add_nodes_from(nodes)</code>","text":"<p>Add multiple nodes to DAG.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>nodes to add</p> required Source code in <code>gresit/graphs.py</code> <pre><code>def add_nodes_from(self, nodes: list[str]) -&gt; None:\n    \"\"\"Add multiple nodes to DAG.\n\n    Args:\n        nodes (list[str]): nodes to add\n    \"\"\"\n    for node in nodes:\n        self.add_node(node)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.children","title":"<code>children(of_node)</code>","text":"<p>Gives all children of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>of_node</code> <code>str</code> <p>node in current DAG.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>of children.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def children(self, of_node: str) -&gt; list[str]:\n    \"\"\"Gives all children of node `node`.\n\n    Args:\n        of_node (str): node in current DAG.\n\n    Returns:\n        list: of children.\n    \"\"\"\n    if of_node in self._children.keys():\n        return list(self._children[of_node])\n    else:\n        return []\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the graph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def copy(self) -&gt; DAG:\n    \"\"\"Return a copy of the graph.\"\"\"\n    return DAG(nodes=list(self._nodes), edges=list(self._edges))\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.from_nx","title":"<code>from_nx(nx_dag, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert to DAG from nx.DiGraph.</p> <p>Parameters:</p> Name Type Description Default <code>nx_dag</code> <code>DiGraph</code> <p>DAG in question.</p> required <code>args</code> <code>Any</code> <p>additional arguments</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>additional arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If DAG is not nx.DiGraph</p> <p>Returns:</p> Type Description <code>DAG</code> <p>DAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>@classmethod\ndef from_nx(cls, nx_dag: nx.DiGraph, *args: Any, **kwargs: Any) -&gt; DAG:\n    \"\"\"Convert to DAG from nx.DiGraph.\n\n    Args:\n        nx_dag (nx.DiGraph): DAG in question.\n        args (Any): additional arguments\n        kwargs (Any): additional arguments\n\n    Raises:\n        TypeError: If DAG is not nx.DiGraph\n\n    Returns:\n        DAG\n    \"\"\"\n    if not isinstance(nx_dag, nx.DiGraph):\n        raise TypeError(\"DAG must be of type nx.DiGraph\")\n    return DAG(nodes=list(nx_dag.nodes), edges=list(nx_dag.edges))\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.from_pandas_adjacency","title":"<code>from_pandas_adjacency(pd_amat, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build DAG from a Pandas adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pd_amat</code> <code>DataFrame</code> <p>input adjacency matrix.</p> required <code>args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DAG</code> <p>DAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>@classmethod\ndef from_pandas_adjacency(cls, pd_amat: pd.DataFrame, *args: Any, **kwargs: Any) -&gt; DAG:\n    \"\"\"Build DAG from a Pandas adjacency matrix.\n\n    Args:\n        pd_amat (pd.DataFrame): input adjacency matrix.\n        args (Any): Additional arguments.\n        kwargs (Any): Additional arguments.\n\n    Returns:\n        DAG\n    \"\"\"\n    assert pd_amat.shape[0] == pd_amat.shape[1]\n    nodes = pd_amat.columns\n\n    all_connections = []\n    start, end = np.where(pd_amat != 0)\n    for idx, _ in enumerate(start):\n        all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n    temp = [set(i) for i in all_connections]\n    temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n\n    dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n    return DAG(nodes=nodes, edges=dir_edges)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.induced_subgraph","title":"<code>induced_subgraph(nodes)</code>","text":"<p>Returns the induced subgraph on the nodes in <code>nodes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>List of nodes.</p> required <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>Induced subgraph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def induced_subgraph(self, nodes: list[str]) -&gt; DAG:\n    \"\"\"Returns the induced subgraph on the nodes in `nodes`.\n\n    Args:\n        nodes (list[str]): List of nodes.\n\n    Returns:\n        DAG: Induced subgraph.\n    \"\"\"\n    edges = [(i, j) for i, j in self.edges if i in nodes and j in nodes]\n    return DAG(nodes=nodes, edges=edges)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.is_acyclic","title":"<code>is_acyclic()</code>","text":"<p>Check if the graph is acyclic.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if graph is acyclic.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def is_acyclic(self) -&gt; bool:\n    \"\"\"Check if the graph is acyclic.\n\n    Returns:\n        bool: True if graph is acyclic.\n    \"\"\"\n    nx_dag = self.to_networkx()\n    acyclic: bool = nx.is_directed_acyclic_graph(nx_dag)\n    return acyclic\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.is_adjacent","title":"<code>is_adjacent(i, j)</code>","text":"<p>Return True if the graph contains an directed edge between i and j.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>str</code> <p>node i.</p> required <code>j</code> <code>str</code> <p>node j.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if i-&gt;j or i&lt;-j</p> Source code in <code>gresit/graphs.py</code> <pre><code>def is_adjacent(self, i: str, j: str) -&gt; bool:\n    \"\"\"Return True if the graph contains an directed edge between i and j.\n\n    Args:\n        i (str): node i.\n        j (str): node j.\n\n    Returns:\n        bool: True if i-&gt;j or i&lt;-j\n    \"\"\"\n    return (j, i) in self.edges or (i, j) in self.edges\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.is_clique","title":"<code>is_clique(potential_clique)</code>","text":"<p>Check every pair of node X potential_clique is adjacent.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def is_clique(self, potential_clique: set[str]) -&gt; bool:\n    \"\"\"Check every pair of node X potential_clique is adjacent.\"\"\"\n    return all(self.is_adjacent(i, j) for i, j in combinations(potential_clique, 2))\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.parents","title":"<code>parents(of_node)</code>","text":"<p>Gives all parents of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>of_node</code> <code>str</code> <p>node in current DAG.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>of parents.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def parents(self, of_node: str) -&gt; list[str]:\n    \"\"\"Gives all parents of node `node`.\n\n    Args:\n        of_node (str): node in current DAG.\n\n    Returns:\n        list: of parents.\n    \"\"\"\n    if of_node in self._parents.keys():\n        return list(self._parents[of_node])\n    else:\n        return []\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.remove_edge","title":"<code>remove_edge(i, j)</code>","text":"<p>Removes edge in question.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>str</code> <p>tail</p> required <code>j</code> <code>str</code> <p>head</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>if edge does not exist</p> Source code in <code>gresit/graphs.py</code> <pre><code>def remove_edge(self, i: str, j: str) -&gt; None:\n    \"\"\"Removes edge in question.\n\n    Args:\n        i (str): tail\n        j (str): head\n\n    Raises:\n        AssertionError: if edge does not exist\n    \"\"\"\n    if (i, j) not in self.edges:\n        raise AssertionError(\"Edge does not exist in current DAG\")\n\n    self._edges.discard((i, j))\n    self._children[i].discard(j)\n    self._parents[j].discard(i)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.remove_node","title":"<code>remove_node(node)</code>","text":"<p>Remove a node from the graph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def remove_node(self, node: str) -&gt; None:\n    \"\"\"Remove a node from the graph.\"\"\"\n    self._nodes.remove(node)\n\n    self._edges = {(i, j) for i, j in self._edges if node not in (i, j)}\n\n    for child in self._children[node]:\n        self._parents[child].remove(node)\n\n    for parent in self._parents[node]:\n        self._children[parent].remove(node)\n\n    self._parents.pop(node, \"I was never here\")\n    self._children.pop(node, \"I was never here\")\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.show","title":"<code>show()</code>","text":"<p>Plot DAG.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Plot DAG.\"\"\"\n    graph = self.to_networkx()\n    pos = nx.circular_layout(graph)\n    nx.draw(graph, pos=pos, with_labels=True)\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.to_cpdag","title":"<code>to_cpdag()</code>","text":"<p>Convert DAG to CPDAG.</p> <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>CPDAG representing the MEC.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_cpdag(self) -&gt; PDAG:\n    \"\"\"Convert DAG to CPDAG.\n\n    Returns:\n        PDAG: CPDAG representing the MEC.\n    \"\"\"\n    return dag2cpdag(dag=self.to_networkx())\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Convert to networkx graph.</p> <p>Returns:</p> Type Description <code>DiGraph</code> <p>nx.MultiDiGraph: Graph with directed and undirected edges.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_networkx(self) -&gt; nx.DiGraph:\n    \"\"\"Convert to networkx graph.\n\n    Returns:\n        nx.MultiDiGraph: Graph with directed and undirected edges.\n    \"\"\"\n    nx_dag = nx.DiGraph()\n    nx_dag.add_nodes_from(self.nodes)\n    nx_dag.add_edges_from(self.edges)\n\n    return nx_dag\n</code></pre>"},{"location":"api/#gresit.graphs.DAG.vstructs","title":"<code>vstructs()</code>","text":"<p>Retrieve v-structures.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set[tuple[str, str]]</code> <p>set of all v-structures</p> Source code in <code>gresit/graphs.py</code> <pre><code>def vstructs(self) -&gt; set[tuple[str, str]]:\n    \"\"\"Retrieve v-structures.\n\n    Returns:\n        set: set of all v-structures\n    \"\"\"\n    vstructures = set()\n    for node in self._nodes:\n        for p1, p2 in combinations(self._parents[node], 2):\n            if p1 not in self._parents[p2] and p2 not in self._parents[p1]:\n                vstructures.add((p1, node))\n                vstructures.add((p2, node))\n    return vstructures\n</code></pre>"},{"location":"api/#gresit.graphs.GRAPH","title":"<code>GRAPH</code>","text":"<p>Abstract base class for all Graphs in current project.</p> Source code in <code>gresit/graphs.py</code> <pre><code>class GRAPH(metaclass=ABCMeta):\n    \"\"\"Abstract base class for all Graphs in current project.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Init ABC.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Return adjacency matrix.\n\n        Raises:\n            AssertionError: _description_\n            AssertionError: _description_\n            ValueError: _description_\n            AssertionError: _description_\n            AssertionError: _description_\n            TypeError: _description_\n\n        Returns:\n            pd.DataFrame: Adjacency matrix of underlying graph.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def causal_order(self) -&gt; list[str] | None:\n        \"\"\"Return causal order.\n\n        Raises:\n            AssertionError: _description_\n            AssertionError: _description_\n            ValueError: _description_\n            AssertionError: _description_\n            AssertionError: _description_\n            TypeError: _description_\n\n        Returns:\n            list[str] | None: Causal order of underlying graph.\n                None if not a DAG.\n        \"\"\"\n</code></pre>"},{"location":"api/#gresit.graphs.GRAPH.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return adjacency matrix.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>ValueError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>TypeError</code> <p>description</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Adjacency matrix of underlying graph.</p>"},{"location":"api/#gresit.graphs.GRAPH.causal_order","title":"<code>causal_order</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return causal order.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>ValueError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <code>TypeError</code> <p>description</p> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>list[str] | None: Causal order of underlying graph. None if not a DAG.</p>"},{"location":"api/#gresit.graphs.GRAPH.__init__","title":"<code>__init__()</code>","text":"<p>Init ABC.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Init ABC.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG","title":"<code>LayeredDAG</code>","text":"<p>               Bases: <code>DAG</code></p> <p>Class to construct Layered DAGs.</p> <p>Layered DAGs <code>L</code> are DAGs where the Nodes <code>V</code> follow some natural layering. In other words, no edge can ever point into any of the earlier layers.</p> Source code in <code>gresit/graphs.py</code> <pre><code>class LayeredDAG(DAG):\n    \"\"\"Class to construct Layered DAGs.\n\n    Layered DAGs `L` are DAGs where the Nodes `V` follow some natural layering.\n    In other words, no edge can ever point into any of the earlier layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: list[str] | None = None,\n        edges: list[tuple[str, str]] | None = None,\n        layering: dict[str, list[str]] | None = None,\n    ) -&gt; None:\n        \"\"\"Layered DAG constructor.\n\n        Args:\n            nodes (list[str] | None, optional): Nodes of LDAG. Defaults to None.\n            edges (list[tuple[str, str]] | None, optional): Edges of LDAG. Defaults to None.\n            layering (dict[str, list[str]] | None, optional): Layering. Defaults to None.\n        \"\"\"\n        self._layering = layering\n        super().__init__(nodes=nodes, edges=edges)\n\n    def _add_edge(self, i: str, j: str) -&gt; None:\n        if not self.layering:\n            raise ValueError(\"Layering must be provided before adding edges.\")\n\n        self._nodes.add(i)\n        self._nodes.add(j)\n        self._edges.add((i, j))\n\n        # Check if graph is acyclic\n        if not self.is_acyclic():\n            raise ValueError(\n                \"The edge set you provided \\\n                induces one or more cycles.\\\n                Check your input!\"\n            )\n\n        # Check if edge is allowed due to layering\n        if not self._is_allowed(edge=(i, j)):\n            raise ValueError(\n                \"The edge set you provided \\\n                does not agree with the layering.\\\n                Check your input!\"\n            )\n\n        self._children[i].add(j)\n        self._parents[j].add(i)\n\n    @property\n    def layering(self) -&gt; dict[str, list[str]] | None:\n        \"\"\"Current layering dict.\n\n        Returns:\n            dict[str, list[str]]: Layering\n        \"\"\"\n        return self._layering\n\n    @layering.setter\n    def layering(self, la: np.random.Generator) -&gt; None:\n        if not isinstance(la, dict):\n            raise AssertionError(\"Layering must be a dictionary!\")\n        self._layering = la\n\n    def _is_allowed(self, edge: tuple[str, str]) -&gt; bool:\n        if not self.layering:\n            raise ValueError(\"Layering must be provided before adding edges.\")\n        i, j = edge\n        layers = list(self.layering.keys())\n        for layer, nodes in self.layering.items():\n            if i in nodes:\n                i_layer = layers.index(layer)\n            if j in nodes:\n                j_layer = layers.index(layer)\n        return i_layer &lt;= j_layer\n\n    def layer_induced_subgraph(self, nodes: list[str]) -&gt; DAG:\n        \"\"\"Returns the induced subgraph on the nodes in `nodes`.\n\n        Args:\n            nodes (list[str]): List of nodes.\n\n        Returns:\n            DAG: Induced subgraph.\n        \"\"\"\n        if self.layering is not None and not any(\n            [nodes == layer for layer in self.layering.values()]\n        ):\n            raise ValueError(\"Nodes you provide must correspond to a layer.\")\n        edges = [(i, j) for i, j in self.edges if i in nodes and j in nodes]\n        return DAG(nodes=nodes, edges=edges)\n\n    @classmethod\n    def from_pandas_adjacency(\n        cls, pd_amat: pd.DataFrame, layering: dict[str, list[str]]\n    ) -&gt; LayeredDAG:\n        \"\"\"Build LayeredDAG from a Pandas adjacency matrix.\n\n        Args:\n            pd_amat (pd.DataFrame): input adjacency matrix.\n            layering (dict[str, list[str]]): layering of nodes.\n\n        Returns:\n            LayeredDAG\n        \"\"\"\n        assert pd_amat.shape[0] == pd_amat.shape[1]\n        nodes = pd_amat.columns\n\n        all_connections = []\n        start, end = np.where(pd_amat != 0)\n        for idx, _ in enumerate(start):\n            all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n        temp = [set(i) for i in all_connections]\n        temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n\n        dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n        return LayeredDAG(nodes=nodes, edges=dir_edges, layering=layering)\n\n    def copy(self) -&gt; LayeredDAG:\n        \"\"\"Return a copy of the graph.\"\"\"\n        return LayeredDAG(nodes=list(self._nodes), edges=list(self._edges), layering=self.layering)\n\n    @classmethod\n    def from_nx(cls, nx_dag: nx.DiGraph, layering: dict[str, list[str]]) -&gt; LayeredDAG:\n        \"\"\"Convert to DAG from nx.DiGraph.\n\n        Args:\n            nx_dag (nx.DiGraph): DAG in question.\n            layering (dict[str, list[str]]): layering of nodes.\n\n        Raises:\n            TypeError: If DAG is not nx.DiGraph\n\n        Returns:\n            LayeredDAG\n        \"\"\"\n        if not isinstance(nx_dag, nx.DiGraph):\n            raise TypeError(\"DAG must be of type nx.DiGraph\")\n        return LayeredDAG(nodes=list(nx_dag.nodes), edges=list(nx_dag.edges), layering=layering)\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG.layering","title":"<code>layering</code>  <code>property</code> <code>writable</code>","text":"<p>Current layering dict.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]] | None</code> <p>dict[str, list[str]]: Layering</p>"},{"location":"api/#gresit.graphs.LayeredDAG.__init__","title":"<code>__init__(nodes=None, edges=None, layering=None)</code>","text":"<p>Layered DAG constructor.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str] | None</code> <p>Nodes of LDAG. Defaults to None.</p> <code>None</code> <code>edges</code> <code>list[tuple[str, str]] | None</code> <p>Edges of LDAG. Defaults to None.</p> <code>None</code> <code>layering</code> <code>dict[str, list[str]] | None</code> <p>Layering. Defaults to None.</p> <code>None</code> Source code in <code>gresit/graphs.py</code> <pre><code>def __init__(\n    self,\n    nodes: list[str] | None = None,\n    edges: list[tuple[str, str]] | None = None,\n    layering: dict[str, list[str]] | None = None,\n) -&gt; None:\n    \"\"\"Layered DAG constructor.\n\n    Args:\n        nodes (list[str] | None, optional): Nodes of LDAG. Defaults to None.\n        edges (list[tuple[str, str]] | None, optional): Edges of LDAG. Defaults to None.\n        layering (dict[str, list[str]] | None, optional): Layering. Defaults to None.\n    \"\"\"\n    self._layering = layering\n    super().__init__(nodes=nodes, edges=edges)\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the graph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def copy(self) -&gt; LayeredDAG:\n    \"\"\"Return a copy of the graph.\"\"\"\n    return LayeredDAG(nodes=list(self._nodes), edges=list(self._edges), layering=self.layering)\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG.from_nx","title":"<code>from_nx(nx_dag, layering)</code>  <code>classmethod</code>","text":"<p>Convert to DAG from nx.DiGraph.</p> <p>Parameters:</p> Name Type Description Default <code>nx_dag</code> <code>DiGraph</code> <p>DAG in question.</p> required <code>layering</code> <code>dict[str, list[str]]</code> <p>layering of nodes.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If DAG is not nx.DiGraph</p> <p>Returns:</p> Type Description <code>LayeredDAG</code> <p>LayeredDAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>@classmethod\ndef from_nx(cls, nx_dag: nx.DiGraph, layering: dict[str, list[str]]) -&gt; LayeredDAG:\n    \"\"\"Convert to DAG from nx.DiGraph.\n\n    Args:\n        nx_dag (nx.DiGraph): DAG in question.\n        layering (dict[str, list[str]]): layering of nodes.\n\n    Raises:\n        TypeError: If DAG is not nx.DiGraph\n\n    Returns:\n        LayeredDAG\n    \"\"\"\n    if not isinstance(nx_dag, nx.DiGraph):\n        raise TypeError(\"DAG must be of type nx.DiGraph\")\n    return LayeredDAG(nodes=list(nx_dag.nodes), edges=list(nx_dag.edges), layering=layering)\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG.from_pandas_adjacency","title":"<code>from_pandas_adjacency(pd_amat, layering)</code>  <code>classmethod</code>","text":"<p>Build LayeredDAG from a Pandas adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pd_amat</code> <code>DataFrame</code> <p>input adjacency matrix.</p> required <code>layering</code> <code>dict[str, list[str]]</code> <p>layering of nodes.</p> required <p>Returns:</p> Type Description <code>LayeredDAG</code> <p>LayeredDAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>@classmethod\ndef from_pandas_adjacency(\n    cls, pd_amat: pd.DataFrame, layering: dict[str, list[str]]\n) -&gt; LayeredDAG:\n    \"\"\"Build LayeredDAG from a Pandas adjacency matrix.\n\n    Args:\n        pd_amat (pd.DataFrame): input adjacency matrix.\n        layering (dict[str, list[str]]): layering of nodes.\n\n    Returns:\n        LayeredDAG\n    \"\"\"\n    assert pd_amat.shape[0] == pd_amat.shape[1]\n    nodes = pd_amat.columns\n\n    all_connections = []\n    start, end = np.where(pd_amat != 0)\n    for idx, _ in enumerate(start):\n        all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n    temp = [set(i) for i in all_connections]\n    temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n\n    dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n    return LayeredDAG(nodes=nodes, edges=dir_edges, layering=layering)\n</code></pre>"},{"location":"api/#gresit.graphs.LayeredDAG.layer_induced_subgraph","title":"<code>layer_induced_subgraph(nodes)</code>","text":"<p>Returns the induced subgraph on the nodes in <code>nodes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>List of nodes.</p> required <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>Induced subgraph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def layer_induced_subgraph(self, nodes: list[str]) -&gt; DAG:\n    \"\"\"Returns the induced subgraph on the nodes in `nodes`.\n\n    Args:\n        nodes (list[str]): List of nodes.\n\n    Returns:\n        DAG: Induced subgraph.\n    \"\"\"\n    if self.layering is not None and not any(\n        [nodes == layer for layer in self.layering.values()]\n    ):\n        raise ValueError(\"Nodes you provide must correspond to a layer.\")\n    edges = [(i, j) for i, j in self.edges if i in nodes and j in nodes]\n    return DAG(nodes=nodes, edges=edges)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG","title":"<code>PDAG</code>","text":"<p>               Bases: <code>GRAPH</code></p> <p>Class for dealing with partially directed graph i.e.</p> <p>graphs that contain both directed and undirected edges.</p> Source code in <code>gresit/graphs.py</code> <pre><code>class PDAG(GRAPH):\n    \"\"\"Class for dealing with partially directed graph i.e.\n\n    graphs that contain both directed and undirected edges.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: list[str] | None = None,\n        dir_edges: list[tuple[str, str]] | None = None,\n        undir_edges: list[tuple[str, str]] | None = None,\n    ) -&gt; None:\n        \"\"\"PDAG constructor.\n\n        Args:\n            nodes (list[str] | None, optional): Nodes in the PDAG. Defaults to None.\n            dir_edges (list[tuple[str,str]] | None, optional): directed edges. Defaults to None.\n            undir_edges (list[tuple[str,str]] | None, optional): undirected edges. Defaults to None.\n        \"\"\"\n        if nodes is None:\n            nodes = []\n        if dir_edges is None:\n            dir_edges = []\n        if undir_edges is None:\n            undir_edges = []\n\n        self._nodes = set(nodes)\n        self._undir_edges: set[tuple[str, str]] = set()\n        self._dir_edges: set[tuple[str, str]] = set()\n        self._parents: defaultdict[str, set[str]] = defaultdict(set)\n        self._children: defaultdict[str, set[str]] = defaultdict(set)\n        self._neighbors: defaultdict[str, set[str]] = defaultdict(set)\n        self._undirected_neighbors: defaultdict[str, set[str]] = defaultdict(set)\n\n        for dir_edge in dir_edges:\n            self._add_dir_edge(*dir_edge)\n        for unir_edge in undir_edges:\n            self._add_undir_edge(*unir_edge)\n\n    def _add_dir_edge(self, i: str, j: str) -&gt; None:\n        self._nodes.add(i)\n        self._nodes.add(j)\n        self._dir_edges.add((i, j))\n\n        self._neighbors[i].add(j)\n        self._neighbors[j].add(i)\n\n        self._children[i].add(j)\n        self._parents[j].add(i)\n\n    def _add_undir_edge(self, i: str, j: str) -&gt; None:\n        self._nodes.add(i)\n        self._nodes.add(j)\n        self._undir_edges.add((i, j))\n\n        self._neighbors[i].add(j)\n        self._neighbors[j].add(i)\n\n        self._undirected_neighbors[i].add(j)\n        self._undirected_neighbors[j].add(i)\n\n    def children(self, node: str) -&gt; set[str]:\n        \"\"\"Gives all children of node `node`.\n\n        Args:\n            node (str): node in current PDAG.\n\n        Returns:\n            set: set of children.\n        \"\"\"\n        if node in self._children.keys():\n            return self._children[node]\n        else:\n            return set()\n\n    def parents(self, node: str) -&gt; set[str]:\n        \"\"\"Gives all parents of node `node`.\n\n        Args:\n            node (str): node in current PDAG.\n\n        Returns:\n            set: set of parents.\n        \"\"\"\n        if node in self._parents.keys():\n            return self._parents[node]\n        else:\n            return set()\n\n    def neighbors(self, node: str) -&gt; set[str]:\n        \"\"\"Gives all neighbors of node `node`.\n\n        Args:\n            node (str): node in current PDAG.\n\n        Returns:\n            set: set of neighbors.\n        \"\"\"\n        if node in self._neighbors.keys():\n            return self._neighbors[node]\n        else:\n            return set()\n\n    def undir_neighbors(self, node: str) -&gt; set[str]:\n        \"\"\"Gives all undirected neighbors of node `node`.\n\n        Args:\n            node (str): node in current PDAG.\n\n        Returns:\n            set: set of undirected neighbors.\n        \"\"\"\n        if node in self._undirected_neighbors.keys():\n            return self._undirected_neighbors[node]\n        else:\n            return set()\n\n    def is_adjacent(self, i: str, j: str) -&gt; bool:\n        \"\"\"Return True if the graph contains an directed or undirected edge between i and j.\n\n        Args:\n            i (str): node i.\n            j (str): node j.\n\n        Returns:\n            bool: True if i-j or i-&gt;j or i&lt;-j\n        \"\"\"\n        return any(\n            (\n                (j, i) in self.dir_edges or (j, i) in self.undir_edges,\n                (i, j) in self.dir_edges or (i, j) in self.undir_edges,\n            )\n        )\n\n    def is_clique(self, potential_clique: set[str]) -&gt; bool:\n        \"\"\"Check every pair of node X potential_clique is adjacent.\"\"\"\n        return all(self.is_adjacent(i, j) for i, j in combinations(potential_clique, 2))\n\n    @classmethod\n    def from_pandas_adjacency(cls, pd_amat: pd.DataFrame) -&gt; PDAG:\n        \"\"\"Build PDAG from a Pandas adjacency matrix.\n\n        Args:\n            pd_amat (pd.DataFrame): input adjacency matrix.\n\n        Returns:\n            PDAG\n        \"\"\"\n        assert pd_amat.shape[0] == pd_amat.shape[1]\n        nodes = pd_amat.columns\n\n        all_connections = []\n        start, end = np.where(pd_amat != 0)\n        for idx, _ in enumerate(start):\n            all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n        temp = [set(i) for i in all_connections]\n        temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n        undir_edges = [tuple(item) for item in set(frozenset(item) for item in temp2)]\n\n        dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n        return PDAG(nodes=nodes, dir_edges=dir_edges, undir_edges=undir_edges)\n\n    def remove_edge(self, i: str, j: str) -&gt; None:\n        \"\"\"Removes edge in question.\n\n        Args:\n            i (str): tail\n            j (str): head\n\n        Raises:\n            AssertionError: if edge does not exist\n        \"\"\"\n        if (i, j) not in self.dir_edges and (i, j) not in self.undir_edges:\n            raise AssertionError(\"Edge does not exist in current PDAG\")\n\n        self._undir_edges.discard((i, j))\n        self._dir_edges.discard((i, j))\n        self._children[i].discard(j)\n        self._parents[j].discard(i)\n        self._neighbors[i].discard(j)\n        self._neighbors[j].discard(i)\n        self._undirected_neighbors[i].discard(j)\n        self._undirected_neighbors[j].discard(i)\n\n    def undir_to_dir_edge(self, tail: str, head: str) -&gt; None:\n        \"\"\"Takes a undirected edge and turns it into a directed one.\n\n        tail indicates the starting node of the edge and head the end node, i.e.\n        tail -&gt; head.\n\n        Args:\n            tail (str): starting node\n            head (str): end node\n\n        Raises:\n            AssertionError: if edge does not exist or is not undirected.\n        \"\"\"\n        if (tail, head) not in self.undir_edges and (\n            head,\n            tail,\n        ) not in self.undir_edges:\n            raise AssertionError(\"Edge seems not to be undirected or even there at all.\")\n        self._undir_edges.discard((tail, head))\n        self._undir_edges.discard((head, tail))\n        self._neighbors[tail].discard(head)\n        self._neighbors[head].discard(tail)\n        self._undirected_neighbors[tail].discard(head)\n        self._undirected_neighbors[head].discard(tail)\n\n        self._add_dir_edge(i=tail, j=head)\n\n    def remove_node(self, node: str) -&gt; None:\n        \"\"\"Remove a node from the graph.\n\n        Args:\n            node (str): node to remove\n        \"\"\"\n        self._nodes.remove(node)\n\n        self._dir_edges = {(i, j) for i, j in self._dir_edges if node not in (i, j)}\n\n        self._undir_edges = {(i, j) for i, j in self._undir_edges if node not in (i, j)}\n\n        for child in self._children[node]:\n            self._parents[child].remove(node)\n            self._neighbors[child].remove(node)\n\n        for parent in self._parents[node]:\n            self._children[parent].remove(node)\n            self._neighbors[parent].remove(node)\n\n        for u_nbr in self._undirected_neighbors[node]:\n            self._undirected_neighbors[u_nbr].remove(node)\n            self._neighbors[u_nbr].remove(node)\n\n        self._parents.pop(node, \"I was never here\")\n        self._children.pop(node, \"I was never here\")\n        self._neighbors.pop(node, \"I was never here\")\n        self._undirected_neighbors.pop(node, \"I was never here\")\n\n    def to_dag(self) -&gt; nx.DiGraph:\n        r\"\"\"Algorithm as described in Chickering (2002).\n\n            1. From PDAG P create DAG G containing all directed edges from P\n            2. Repeat the following: Select node v in P s.t.\n                i. v has no outgoing edges (children) i.e. \\\\(ch(v) = \\\\emptyset \\\\)\n\n                ii. \\\\(neigh(v) \\\\neq \\\\emptyset\\\\)\n                    Then \\\\( (pa(v) \\\\cup (neigh(v) \\\\) form a clique.\n                    For each v that is in a clique and is part of an undirected edge in P\n                    i.e. w - v, insert a directed edge w -&gt; v in G.\n                    Remove v and all incident edges from P and continue with next node.\n                    Until all nodes have been deleted from P.\n\n        Returns:\n            nx.DiGraph: DAG that belongs to the MEC implied by the PDAG\n        \"\"\"\n        pdag = self.copy()\n\n        dag = nx.DiGraph()\n        dag.add_nodes_from(pdag.nodes)\n        dag.add_edges_from(pdag.dir_edges)\n\n        if pdag.num_undir_edges == 0:\n            return dag\n        else:\n            while pdag.num_nodes &gt; 0:\n                # find node with (1) no directed outgoing edges and\n                #                (2) the set of undirected neighbors is either empty or\n                #                    undirected neighbors + parents of X are a clique\n                found = False\n                for node in pdag.nodes:\n                    children = pdag.children(node)\n                    neighbors = pdag.neighbors(node)\n                    # pdag._undirected_neighbors[node]\n                    parents = pdag.parents(node)\n                    potential_clique_members = neighbors.union(parents)\n\n                    is_clique = pdag.is_clique(potential_clique_members)\n\n                    if not children and (not neighbors or is_clique):\n                        found = True\n                        # add all edges of node as outgoing edges to dag\n                        for edge in pdag.undir_edges:\n                            if node in edge:\n                                incident_node = set(edge) - {node}\n                                dag.add_edge(*incident_node, node)\n\n                        pdag.remove_node(node)\n                        break\n\n                if not found:\n                    logger.warning(\"PDAG not extendible: Random DAG on skeleton drawn.\")\n\n                    dag = nx.from_pandas_adjacency(self._amat_to_dag(), create_using=nx.DiGraph)\n\n                    break\n\n            return dag\n\n    @property\n    def adjacency_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Returns adjacency matrix.\n\n        The i,jth entry being one indicates that there is an edge\n        from i to j. A zero indicates that there is no edge.\n\n        Returns:\n            pd.DataFrame: adjacency matrix\n        \"\"\"\n        amat = pd.DataFrame(\n            np.zeros([self.num_nodes, self.num_nodes]),\n            index=self.nodes,\n            columns=self.nodes,\n        )\n        for edge in self.dir_edges:\n            amat.loc[edge] = 1\n        for edge in self.undir_edges:\n            amat.loc[edge] = amat.loc[edge[::-1]] = 1\n        return amat\n\n    @property\n    def causal_order(self) -&gt; None:\n        \"\"\"Causal order is None.\n\n        This is because PDAGs only allow for a partial causal order.\n\n        Returns:\n            None: None\n        \"\"\"\n        return None\n\n    def _amat_to_dag(self) -&gt; pd.DataFrame:\n        \"\"\"Transform the adjacency matrix of an PDAG to the adjacency matrix.\n\n            of SOME DAG in the Markov equivalence class.\n\n        Returns:\n            pd.DataFrame: DAG, a member of the MEC.\n        \"\"\"\n        pdag_amat = self.adjacency_matrix.to_numpy()\n\n        p = pdag_amat.shape[0]\n        ## amat to skel\n        skel = pdag_amat + pdag_amat.T\n        skel[np.where(skel &gt; 1)] = 1\n        ## permute skel\n        permute_ord = np.random.choice(a=p, size=p, replace=False)\n        skel = skel[:, permute_ord][permute_ord]\n\n        ## skel to dag\n        for i in range(1, p):\n            for j in range(0, i + 1):\n                if skel[i, j] == 1:\n                    skel[i, j] = 0\n\n        ## inverse permutation\n        i_ord = np.sort(permute_ord)\n        skel = skel[:, i_ord][i_ord]\n        return pd.DataFrame(\n            skel,\n            index=self.adjacency_matrix.index,\n            columns=self.adjacency_matrix.columns,\n        )\n\n    def vstructs(self) -&gt; set[tuple[str, str]]:\n        \"\"\"Retrieve v-structures.\n\n        Returns:\n            set: set of all v-structures\n        \"\"\"\n        vstructures = set()\n        for node in self._nodes:\n            for p1, p2 in combinations(self._parents[node], 2):\n                if p1 not in self._parents[p2] and p2 not in self._parents[p1]:\n                    vstructures.add((p1, node))\n                    vstructures.add((p2, node))\n        return vstructures\n\n    def copy(self) -&gt; PDAG:\n        \"\"\"Return a copy of the graph.\"\"\"\n        return PDAG(\n            nodes=list(self._nodes),\n            dir_edges=list(self._dir_edges),\n            undir_edges=list(self._undir_edges),\n        )\n\n    def show(self) -&gt; None:\n        \"\"\"Plot PDAG.\"\"\"\n        graph = self.to_networkx()\n        pos = nx.circular_layout(graph)\n        nx.draw(graph, pos=pos, with_labels=True)\n\n    def to_networkx(self) -&gt; nx.MultiDiGraph:\n        \"\"\"Convert to networkx graph.\n\n        Returns:\n            nx.MultiDiGraph: Graph with directed and undirected edges.\n        \"\"\"\n        nx_pdag = nx.MultiDiGraph()\n        nx_pdag.add_nodes_from(self.nodes)\n        nx_pdag.add_edges_from(self.dir_edges)\n        for edge in self.undir_edges:\n            nx_pdag.add_edge(*edge)\n            nx_pdag.add_edge(*edge[::-1])\n\n        return nx_pdag\n\n    def _meek_mec_enumeration(self, pdag: PDAG, dag_list: list[DAG]) -&gt; None:\n        \"\"\"Apply Meek's MEC enumeration algorithm.\n\n        Args:\n            pdag (PDAG): partially directed graph in question.\n            dag_list (list): list of currently found DAGs.\n\n        References:\n            Wien\u00f6bst, Marcel, et al. \"Efficient enumeration of Markov equivalent DAGs.\"\n            Proceedings of the AAAI Conference on Artificial Intelligence.\n            Vol. 37. No. 10. 2023.\n        \"\"\"\n        g_copy = pdag.copy()\n        g_copy = self._apply_meek_rules(g_copy)  # Apply Meek rules\n\n        undir_edges = g_copy.undir_edges\n        if undir_edges:\n            i, j = undir_edges[0]  # Take first undirected edge\n\n        if not g_copy.undir_edges:\n            # makes sure that flaoting nodes are preserved\n            new_member = DAG()\n            new_member.add_nodes_from(g_copy.nodes)\n            new_member.add_edges_from(g_copy.dir_edges)\n            dag_list.append(new_member)\n            return  # Add DAG to current list\n\n        # Recursion first orientation:\n        g_copy.undir_to_dir_edge(i, j)\n        self._meek_mec_enumeration(pdag=g_copy, dag_list=dag_list)\n        g_copy.remove_edge(i, j)\n\n        # Recursion second orientation\n        g_copy._add_dir_edge(j, i)\n        self._meek_mec_enumeration(pdag=g_copy, dag_list=dag_list)\n\n    def to_allDAGs(self) -&gt; list[DAG]:\n        \"\"\"Recursion algorithm which recursively applies the following steps.\n\n            1. Orient the first undirected edge found.\n            2. Apply Meek rules.\n            3. Recurse with each direction of the oriented edge.\n        This corresponds to Algorithm 2 in Wien\u00f6bst et al. (2023).\n\n        References:\n            Wien\u00f6bst, Marcel, et al. \"Efficient enumeration of Markov equivalent DAGs.\"\n            Proceedings of the AAAI Conference on Artificial Intelligence.\n            Vol. 37. No. 10. 2023.\n        \"\"\"\n        all_dags: list[DAG] = []\n        self._meek_mec_enumeration(pdag=self, dag_list=all_dags)\n        return all_dags\n\n    # use Meek's cpdag2alldag\n    def _apply_meek_rules(self, G: PDAG) -&gt; PDAG:\n        \"\"\"Apply all four Meek rules to a PDAG turning it into a CPDAG.\n\n        Args:\n            G (PDAG): PDAG to complete\n\n        Returns:\n            PDAG: completed PDAG.\n        \"\"\"\n        # Apply Meek Rules\n        cpdag = G.copy()\n        cpdag = rule_1(pdag=cpdag)\n        cpdag = rule_2(pdag=cpdag)\n        cpdag = rule_3(pdag=cpdag)\n        cpdag = rule_4(pdag=cpdag)\n        return cpdag\n\n    def to_random_dag(self) -&gt; DAG:\n        \"\"\"Provides a random DAG residing in the MEC.\n\n        Returns:\n            nx.DiGraph: random DAG living in MEC\n        \"\"\"\n        to_dag_candidate = self.copy()\n\n        while to_dag_candidate.num_undir_edges &gt; 0:\n            chosen_edge = to_dag_candidate.undir_edges[\n                np.random.choice(to_dag_candidate.num_undir_edges)\n            ]\n            choose_orientation = [chosen_edge, chosen_edge[::-1]]\n            node_i, node_j = choose_orientation[np.random.choice(len(choose_orientation))]\n\n            to_dag_candidate.undir_to_dir_edge(tail=node_i, head=node_j)\n            to_dag_candidate = to_dag_candidate._apply_meek_rules(G=to_dag_candidate)\n\n        return DAG.from_pandas_adjacency(to_dag_candidate.adjacency_matrix)\n\n    @property\n    def nodes(self) -&gt; list[str]:\n        \"\"\"Get all nods in current PDAG.\n\n        Returns:\n            list: list of nodes.\n        \"\"\"\n        return sorted(list(self._nodes))\n\n    @property\n    def num_nodes(self) -&gt; int:\n        \"\"\"Number of nodes in current PDAG.\n\n        Returns:\n            int: Number of nodes\n        \"\"\"\n        return len(self._nodes)\n\n    @property\n    def num_undir_edges(self) -&gt; int:\n        \"\"\"Number of undirected edges in current PDAG.\n\n        Returns:\n            int: Number of undirected edges\n        \"\"\"\n        return len(self._undir_edges)\n\n    @property\n    def num_dir_edges(self) -&gt; int:\n        \"\"\"Number of directed edges in current PDAG.\n\n        Returns:\n            int: Number of directed edges\n        \"\"\"\n        return len(self._dir_edges)\n\n    @property\n    def num_adjacencies(self) -&gt; int:\n        \"\"\"Number of adjacent nodes in current PDAG.\n\n        Returns:\n            int: Number of adjacent nodes\n        \"\"\"\n        return self.num_undir_edges + self.num_dir_edges\n\n    @property\n    def undir_edges(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Gives all undirected edges in current PDAG.\n\n        Returns:\n            list[tuple[str,str]]: List of undirected edges.\n        \"\"\"\n        return list(self._undir_edges)\n\n    @property\n    def dir_edges(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Gives all directed edges in current PDAG.\n\n        Returns:\n            list[tuple[str,str]]: List of directed edges.\n        \"\"\"\n        return list(self._dir_edges)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.adjacency_matrix","title":"<code>adjacency_matrix</code>  <code>property</code>","text":"<p>Returns adjacency matrix.</p> <p>The i,jth entry being one indicates that there is an edge from i to j. A zero indicates that there is no edge.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: adjacency matrix</p>"},{"location":"api/#gresit.graphs.PDAG.causal_order","title":"<code>causal_order</code>  <code>property</code>","text":"<p>Causal order is None.</p> <p>This is because PDAGs only allow for a partial causal order.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>None</p>"},{"location":"api/#gresit.graphs.PDAG.dir_edges","title":"<code>dir_edges</code>  <code>property</code>","text":"<p>Gives all directed edges in current PDAG.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str,str]]: List of directed edges.</p>"},{"location":"api/#gresit.graphs.PDAG.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Get all nods in current PDAG.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>list of nodes.</p>"},{"location":"api/#gresit.graphs.PDAG.num_adjacencies","title":"<code>num_adjacencies</code>  <code>property</code>","text":"<p>Number of adjacent nodes in current PDAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of adjacent nodes</p>"},{"location":"api/#gresit.graphs.PDAG.num_dir_edges","title":"<code>num_dir_edges</code>  <code>property</code>","text":"<p>Number of directed edges in current PDAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of directed edges</p>"},{"location":"api/#gresit.graphs.PDAG.num_nodes","title":"<code>num_nodes</code>  <code>property</code>","text":"<p>Number of nodes in current PDAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of nodes</p>"},{"location":"api/#gresit.graphs.PDAG.num_undir_edges","title":"<code>num_undir_edges</code>  <code>property</code>","text":"<p>Number of undirected edges in current PDAG.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of undirected edges</p>"},{"location":"api/#gresit.graphs.PDAG.undir_edges","title":"<code>undir_edges</code>  <code>property</code>","text":"<p>Gives all undirected edges in current PDAG.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str,str]]: List of undirected edges.</p>"},{"location":"api/#gresit.graphs.PDAG.__init__","title":"<code>__init__(nodes=None, dir_edges=None, undir_edges=None)</code>","text":"<p>PDAG constructor.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str] | None</code> <p>Nodes in the PDAG. Defaults to None.</p> <code>None</code> <code>dir_edges</code> <code>list[tuple[str, str]] | None</code> <p>directed edges. Defaults to None.</p> <code>None</code> <code>undir_edges</code> <code>list[tuple[str, str]] | None</code> <p>undirected edges. Defaults to None.</p> <code>None</code> Source code in <code>gresit/graphs.py</code> <pre><code>def __init__(\n    self,\n    nodes: list[str] | None = None,\n    dir_edges: list[tuple[str, str]] | None = None,\n    undir_edges: list[tuple[str, str]] | None = None,\n) -&gt; None:\n    \"\"\"PDAG constructor.\n\n    Args:\n        nodes (list[str] | None, optional): Nodes in the PDAG. Defaults to None.\n        dir_edges (list[tuple[str,str]] | None, optional): directed edges. Defaults to None.\n        undir_edges (list[tuple[str,str]] | None, optional): undirected edges. Defaults to None.\n    \"\"\"\n    if nodes is None:\n        nodes = []\n    if dir_edges is None:\n        dir_edges = []\n    if undir_edges is None:\n        undir_edges = []\n\n    self._nodes = set(nodes)\n    self._undir_edges: set[tuple[str, str]] = set()\n    self._dir_edges: set[tuple[str, str]] = set()\n    self._parents: defaultdict[str, set[str]] = defaultdict(set)\n    self._children: defaultdict[str, set[str]] = defaultdict(set)\n    self._neighbors: defaultdict[str, set[str]] = defaultdict(set)\n    self._undirected_neighbors: defaultdict[str, set[str]] = defaultdict(set)\n\n    for dir_edge in dir_edges:\n        self._add_dir_edge(*dir_edge)\n    for unir_edge in undir_edges:\n        self._add_undir_edge(*unir_edge)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.children","title":"<code>children(node)</code>","text":"<p>Gives all children of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node in current PDAG.</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>set of children.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def children(self, node: str) -&gt; set[str]:\n    \"\"\"Gives all children of node `node`.\n\n    Args:\n        node (str): node in current PDAG.\n\n    Returns:\n        set: set of children.\n    \"\"\"\n    if node in self._children.keys():\n        return self._children[node]\n    else:\n        return set()\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the graph.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def copy(self) -&gt; PDAG:\n    \"\"\"Return a copy of the graph.\"\"\"\n    return PDAG(\n        nodes=list(self._nodes),\n        dir_edges=list(self._dir_edges),\n        undir_edges=list(self._undir_edges),\n    )\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.from_pandas_adjacency","title":"<code>from_pandas_adjacency(pd_amat)</code>  <code>classmethod</code>","text":"<p>Build PDAG from a Pandas adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pd_amat</code> <code>DataFrame</code> <p>input adjacency matrix.</p> required <p>Returns:</p> Type Description <code>PDAG</code> <p>PDAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>@classmethod\ndef from_pandas_adjacency(cls, pd_amat: pd.DataFrame) -&gt; PDAG:\n    \"\"\"Build PDAG from a Pandas adjacency matrix.\n\n    Args:\n        pd_amat (pd.DataFrame): input adjacency matrix.\n\n    Returns:\n        PDAG\n    \"\"\"\n    assert pd_amat.shape[0] == pd_amat.shape[1]\n    nodes = pd_amat.columns\n\n    all_connections = []\n    start, end = np.where(pd_amat != 0)\n    for idx, _ in enumerate(start):\n        all_connections.append((pd_amat.columns[start[idx]], pd_amat.columns[end[idx]]))\n\n    temp = [set(i) for i in all_connections]\n    temp2 = [arc for arc in all_connections if temp.count(set(arc)) &gt; 1]\n    undir_edges = [tuple(item) for item in set(frozenset(item) for item in temp2)]\n\n    dir_edges = [edge for edge in all_connections if edge not in temp2]\n\n    return PDAG(nodes=nodes, dir_edges=dir_edges, undir_edges=undir_edges)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.is_adjacent","title":"<code>is_adjacent(i, j)</code>","text":"<p>Return True if the graph contains an directed or undirected edge between i and j.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>str</code> <p>node i.</p> required <code>j</code> <code>str</code> <p>node j.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if i-j or i-&gt;j or i&lt;-j</p> Source code in <code>gresit/graphs.py</code> <pre><code>def is_adjacent(self, i: str, j: str) -&gt; bool:\n    \"\"\"Return True if the graph contains an directed or undirected edge between i and j.\n\n    Args:\n        i (str): node i.\n        j (str): node j.\n\n    Returns:\n        bool: True if i-j or i-&gt;j or i&lt;-j\n    \"\"\"\n    return any(\n        (\n            (j, i) in self.dir_edges or (j, i) in self.undir_edges,\n            (i, j) in self.dir_edges or (i, j) in self.undir_edges,\n        )\n    )\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.is_clique","title":"<code>is_clique(potential_clique)</code>","text":"<p>Check every pair of node X potential_clique is adjacent.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def is_clique(self, potential_clique: set[str]) -&gt; bool:\n    \"\"\"Check every pair of node X potential_clique is adjacent.\"\"\"\n    return all(self.is_adjacent(i, j) for i, j in combinations(potential_clique, 2))\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.neighbors","title":"<code>neighbors(node)</code>","text":"<p>Gives all neighbors of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node in current PDAG.</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>set of neighbors.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def neighbors(self, node: str) -&gt; set[str]:\n    \"\"\"Gives all neighbors of node `node`.\n\n    Args:\n        node (str): node in current PDAG.\n\n    Returns:\n        set: set of neighbors.\n    \"\"\"\n    if node in self._neighbors.keys():\n        return self._neighbors[node]\n    else:\n        return set()\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.parents","title":"<code>parents(node)</code>","text":"<p>Gives all parents of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node in current PDAG.</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>set of parents.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def parents(self, node: str) -&gt; set[str]:\n    \"\"\"Gives all parents of node `node`.\n\n    Args:\n        node (str): node in current PDAG.\n\n    Returns:\n        set: set of parents.\n    \"\"\"\n    if node in self._parents.keys():\n        return self._parents[node]\n    else:\n        return set()\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.remove_edge","title":"<code>remove_edge(i, j)</code>","text":"<p>Removes edge in question.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>str</code> <p>tail</p> required <code>j</code> <code>str</code> <p>head</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>if edge does not exist</p> Source code in <code>gresit/graphs.py</code> <pre><code>def remove_edge(self, i: str, j: str) -&gt; None:\n    \"\"\"Removes edge in question.\n\n    Args:\n        i (str): tail\n        j (str): head\n\n    Raises:\n        AssertionError: if edge does not exist\n    \"\"\"\n    if (i, j) not in self.dir_edges and (i, j) not in self.undir_edges:\n        raise AssertionError(\"Edge does not exist in current PDAG\")\n\n    self._undir_edges.discard((i, j))\n    self._dir_edges.discard((i, j))\n    self._children[i].discard(j)\n    self._parents[j].discard(i)\n    self._neighbors[i].discard(j)\n    self._neighbors[j].discard(i)\n    self._undirected_neighbors[i].discard(j)\n    self._undirected_neighbors[j].discard(i)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.remove_node","title":"<code>remove_node(node)</code>","text":"<p>Remove a node from the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node to remove</p> required Source code in <code>gresit/graphs.py</code> <pre><code>def remove_node(self, node: str) -&gt; None:\n    \"\"\"Remove a node from the graph.\n\n    Args:\n        node (str): node to remove\n    \"\"\"\n    self._nodes.remove(node)\n\n    self._dir_edges = {(i, j) for i, j in self._dir_edges if node not in (i, j)}\n\n    self._undir_edges = {(i, j) for i, j in self._undir_edges if node not in (i, j)}\n\n    for child in self._children[node]:\n        self._parents[child].remove(node)\n        self._neighbors[child].remove(node)\n\n    for parent in self._parents[node]:\n        self._children[parent].remove(node)\n        self._neighbors[parent].remove(node)\n\n    for u_nbr in self._undirected_neighbors[node]:\n        self._undirected_neighbors[u_nbr].remove(node)\n        self._neighbors[u_nbr].remove(node)\n\n    self._parents.pop(node, \"I was never here\")\n    self._children.pop(node, \"I was never here\")\n    self._neighbors.pop(node, \"I was never here\")\n    self._undirected_neighbors.pop(node, \"I was never here\")\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.show","title":"<code>show()</code>","text":"<p>Plot PDAG.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Plot PDAG.\"\"\"\n    graph = self.to_networkx()\n    pos = nx.circular_layout(graph)\n    nx.draw(graph, pos=pos, with_labels=True)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.to_allDAGs","title":"<code>to_allDAGs()</code>","text":"<p>Recursion algorithm which recursively applies the following steps.</p> <pre><code>1. Orient the first undirected edge found.\n2. Apply Meek rules.\n3. Recurse with each direction of the oriented edge.\n</code></pre> <p>This corresponds to Algorithm 2 in Wien\u00f6bst et al. (2023).</p> References <p>Wien\u00f6bst, Marcel, et al. \"Efficient enumeration of Markov equivalent DAGs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 10. 2023.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_allDAGs(self) -&gt; list[DAG]:\n    \"\"\"Recursion algorithm which recursively applies the following steps.\n\n        1. Orient the first undirected edge found.\n        2. Apply Meek rules.\n        3. Recurse with each direction of the oriented edge.\n    This corresponds to Algorithm 2 in Wien\u00f6bst et al. (2023).\n\n    References:\n        Wien\u00f6bst, Marcel, et al. \"Efficient enumeration of Markov equivalent DAGs.\"\n        Proceedings of the AAAI Conference on Artificial Intelligence.\n        Vol. 37. No. 10. 2023.\n    \"\"\"\n    all_dags: list[DAG] = []\n    self._meek_mec_enumeration(pdag=self, dag_list=all_dags)\n    return all_dags\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.to_dag","title":"<code>to_dag()</code>","text":"<p>Algorithm as described in Chickering (2002).</p> <pre><code>1. From PDAG P create DAG G containing all directed edges from P\n2. Repeat the following: Select node v in P s.t.\n    i. v has no outgoing edges (children) i.e. \\\\(ch(v) = \\\\emptyset \\\\)\n\n    ii. \\\\(neigh(v) \\\\neq \\\\emptyset\\\\)\n        Then \\\\( (pa(v) \\\\cup (neigh(v) \\\\) form a clique.\n        For each v that is in a clique and is part of an undirected edge in P\n        i.e. w - v, insert a directed edge w -&gt; v in G.\n        Remove v and all incident edges from P and continue with next node.\n        Until all nodes have been deleted from P.\n</code></pre> <p>Returns:</p> Type Description <code>DiGraph</code> <p>nx.DiGraph: DAG that belongs to the MEC implied by the PDAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_dag(self) -&gt; nx.DiGraph:\n    r\"\"\"Algorithm as described in Chickering (2002).\n\n        1. From PDAG P create DAG G containing all directed edges from P\n        2. Repeat the following: Select node v in P s.t.\n            i. v has no outgoing edges (children) i.e. \\\\(ch(v) = \\\\emptyset \\\\)\n\n            ii. \\\\(neigh(v) \\\\neq \\\\emptyset\\\\)\n                Then \\\\( (pa(v) \\\\cup (neigh(v) \\\\) form a clique.\n                For each v that is in a clique and is part of an undirected edge in P\n                i.e. w - v, insert a directed edge w -&gt; v in G.\n                Remove v and all incident edges from P and continue with next node.\n                Until all nodes have been deleted from P.\n\n    Returns:\n        nx.DiGraph: DAG that belongs to the MEC implied by the PDAG\n    \"\"\"\n    pdag = self.copy()\n\n    dag = nx.DiGraph()\n    dag.add_nodes_from(pdag.nodes)\n    dag.add_edges_from(pdag.dir_edges)\n\n    if pdag.num_undir_edges == 0:\n        return dag\n    else:\n        while pdag.num_nodes &gt; 0:\n            # find node with (1) no directed outgoing edges and\n            #                (2) the set of undirected neighbors is either empty or\n            #                    undirected neighbors + parents of X are a clique\n            found = False\n            for node in pdag.nodes:\n                children = pdag.children(node)\n                neighbors = pdag.neighbors(node)\n                # pdag._undirected_neighbors[node]\n                parents = pdag.parents(node)\n                potential_clique_members = neighbors.union(parents)\n\n                is_clique = pdag.is_clique(potential_clique_members)\n\n                if not children and (not neighbors or is_clique):\n                    found = True\n                    # add all edges of node as outgoing edges to dag\n                    for edge in pdag.undir_edges:\n                        if node in edge:\n                            incident_node = set(edge) - {node}\n                            dag.add_edge(*incident_node, node)\n\n                    pdag.remove_node(node)\n                    break\n\n            if not found:\n                logger.warning(\"PDAG not extendible: Random DAG on skeleton drawn.\")\n\n                dag = nx.from_pandas_adjacency(self._amat_to_dag(), create_using=nx.DiGraph)\n\n                break\n\n        return dag\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Convert to networkx graph.</p> <p>Returns:</p> Type Description <code>MultiDiGraph</code> <p>nx.MultiDiGraph: Graph with directed and undirected edges.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_networkx(self) -&gt; nx.MultiDiGraph:\n    \"\"\"Convert to networkx graph.\n\n    Returns:\n        nx.MultiDiGraph: Graph with directed and undirected edges.\n    \"\"\"\n    nx_pdag = nx.MultiDiGraph()\n    nx_pdag.add_nodes_from(self.nodes)\n    nx_pdag.add_edges_from(self.dir_edges)\n    for edge in self.undir_edges:\n        nx_pdag.add_edge(*edge)\n        nx_pdag.add_edge(*edge[::-1])\n\n    return nx_pdag\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.to_random_dag","title":"<code>to_random_dag()</code>","text":"<p>Provides a random DAG residing in the MEC.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>nx.DiGraph: random DAG living in MEC</p> Source code in <code>gresit/graphs.py</code> <pre><code>def to_random_dag(self) -&gt; DAG:\n    \"\"\"Provides a random DAG residing in the MEC.\n\n    Returns:\n        nx.DiGraph: random DAG living in MEC\n    \"\"\"\n    to_dag_candidate = self.copy()\n\n    while to_dag_candidate.num_undir_edges &gt; 0:\n        chosen_edge = to_dag_candidate.undir_edges[\n            np.random.choice(to_dag_candidate.num_undir_edges)\n        ]\n        choose_orientation = [chosen_edge, chosen_edge[::-1]]\n        node_i, node_j = choose_orientation[np.random.choice(len(choose_orientation))]\n\n        to_dag_candidate.undir_to_dir_edge(tail=node_i, head=node_j)\n        to_dag_candidate = to_dag_candidate._apply_meek_rules(G=to_dag_candidate)\n\n    return DAG.from_pandas_adjacency(to_dag_candidate.adjacency_matrix)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.undir_neighbors","title":"<code>undir_neighbors(node)</code>","text":"<p>Gives all undirected neighbors of node <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>node in current PDAG.</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[str]</code> <p>set of undirected neighbors.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def undir_neighbors(self, node: str) -&gt; set[str]:\n    \"\"\"Gives all undirected neighbors of node `node`.\n\n    Args:\n        node (str): node in current PDAG.\n\n    Returns:\n        set: set of undirected neighbors.\n    \"\"\"\n    if node in self._undirected_neighbors.keys():\n        return self._undirected_neighbors[node]\n    else:\n        return set()\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.undir_to_dir_edge","title":"<code>undir_to_dir_edge(tail, head)</code>","text":"<p>Takes a undirected edge and turns it into a directed one.</p> <p>tail indicates the starting node of the edge and head the end node, i.e. tail -&gt; head.</p> <p>Parameters:</p> Name Type Description Default <code>tail</code> <code>str</code> <p>starting node</p> required <code>head</code> <code>str</code> <p>end node</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>if edge does not exist or is not undirected.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def undir_to_dir_edge(self, tail: str, head: str) -&gt; None:\n    \"\"\"Takes a undirected edge and turns it into a directed one.\n\n    tail indicates the starting node of the edge and head the end node, i.e.\n    tail -&gt; head.\n\n    Args:\n        tail (str): starting node\n        head (str): end node\n\n    Raises:\n        AssertionError: if edge does not exist or is not undirected.\n    \"\"\"\n    if (tail, head) not in self.undir_edges and (\n        head,\n        tail,\n    ) not in self.undir_edges:\n        raise AssertionError(\"Edge seems not to be undirected or even there at all.\")\n    self._undir_edges.discard((tail, head))\n    self._undir_edges.discard((head, tail))\n    self._neighbors[tail].discard(head)\n    self._neighbors[head].discard(tail)\n    self._undirected_neighbors[tail].discard(head)\n    self._undirected_neighbors[head].discard(tail)\n\n    self._add_dir_edge(i=tail, j=head)\n</code></pre>"},{"location":"api/#gresit.graphs.PDAG.vstructs","title":"<code>vstructs()</code>","text":"<p>Retrieve v-structures.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set[tuple[str, str]]</code> <p>set of all v-structures</p> Source code in <code>gresit/graphs.py</code> <pre><code>def vstructs(self) -&gt; set[tuple[str, str]]:\n    \"\"\"Retrieve v-structures.\n\n    Returns:\n        set: set of all v-structures\n    \"\"\"\n    vstructures = set()\n    for node in self._nodes:\n        for p1, p2 in combinations(self._parents[node], 2):\n            if p1 not in self._parents[p2] and p2 not in self._parents[p1]:\n                vstructures.add((p1, node))\n                vstructures.add((p2, node))\n    return vstructures\n</code></pre>"},{"location":"api/#gresit.graphs.dag2cpdag","title":"<code>dag2cpdag(dag)</code>","text":"<p>Convertes a DAG into its unique CPDAG.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DiGraph</code> <p>DAG the CPDAG corresponds to.</p> required <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>unique CPDAG</p> Source code in <code>gresit/graphs.py</code> <pre><code>def dag2cpdag(dag: nx.DiGraph) -&gt; PDAG:\n    \"\"\"Convertes a DAG into its unique CPDAG.\n\n    Args:\n        dag (nx.DiGraph): DAG the CPDAG corresponds to.\n\n    Returns:\n        PDAG: unique CPDAG\n    \"\"\"\n    copy_dag = dag.copy()\n    # Skeleton\n    skeleton = nx.to_pandas_adjacency(copy_dag.to_undirected())\n    # v-Structures\n    vstructures = vstructs(dag=copy_dag)\n\n    for edge in vstructures:  # orient v-structures\n        skeleton.loc[edge[::-1]] = 0\n\n    pdag_init = PDAG.from_pandas_adjacency(skeleton)\n\n    # Apply Meek Rules\n    cpdag = rule_1(pdag=pdag_init)\n    cpdag = rule_2(pdag=cpdag)\n    cpdag = rule_3(pdag=cpdag)\n    cpdag = rule_4(pdag=cpdag)\n\n    return cpdag\n</code></pre>"},{"location":"api/#gresit.graphs.rule_1","title":"<code>rule_1(pdag)</code>","text":"<p>Applies first Meek rule.</p> <p>Given the following pattern X -&gt; Y - Z. Orient Y - Z to Y -&gt; Z if X and Z are non-adjacent (otherwise a new v-structure arises).</p> <p>Parameters:</p> Name Type Description Default <code>pdag</code> <code>PDAG</code> <p>PDAG before application of rule.</p> required <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>PDAG after application of rule.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def rule_1(pdag: PDAG) -&gt; PDAG:\n    \"\"\"Applies first Meek rule.\n\n    Given the following pattern X -&gt; Y - Z. Orient Y - Z to Y -&gt; Z\n    if X and Z are non-adjacent (otherwise a new v-structure arises).\n\n    Args:\n        pdag (PDAG): PDAG before application of rule.\n\n    Returns:\n        PDAG: PDAG after application of rule.\n    \"\"\"\n    copy_pdag = pdag.copy()\n    for edge in copy_pdag.undir_edges:\n        reverse_edge = edge[::-1]\n        test_edges = [edge, reverse_edge]\n        for tail, head in test_edges:\n            orient = False\n            undir_parents = copy_pdag.parents(tail)\n            if undir_parents:\n                for parent in undir_parents:\n                    if not copy_pdag.is_adjacent(parent, head):\n                        orient = True\n            if orient:\n                copy_pdag.undir_to_dir_edge(tail=tail, head=head)\n                break\n    return copy_pdag\n</code></pre>"},{"location":"api/#gresit.graphs.rule_2","title":"<code>rule_2(pdag)</code>","text":"<p>Applies the second Meek rule.</p> <p>Given the following directed triple X -&gt; Y -&gt; Z where X - Z are indeed adjacent. Orient X - Z to X -&gt; Z otherwise a cycle arises.</p> <p>Parameters:</p> Name Type Description Default <code>pdag</code> <code>PDAG</code> <p>PDAG before application of rule.</p> required <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>PDAG after application of rule.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def rule_2(pdag: PDAG) -&gt; PDAG:\n    \"\"\"Applies the second Meek rule.\n\n    Given the following directed triple\n    X -&gt; Y -&gt; Z where X - Z are indeed adjacent.\n    Orient X - Z to X -&gt; Z otherwise a cycle arises.\n\n    Args:\n        pdag (PDAG): PDAG before application of rule.\n\n    Returns:\n        PDAG: PDAG after application of rule.\n    \"\"\"\n    copy_pdag = pdag.copy()\n    for edge in copy_pdag.undir_edges:\n        reverse_edge = edge[::-1]\n        test_edges = [edge, reverse_edge]\n        for tail, head in test_edges:\n            orient = False\n            undir_children = copy_pdag.children(tail)\n            if undir_children:\n                for child in undir_children:\n                    if head in copy_pdag.children(child):\n                        orient = True\n            if orient:\n                copy_pdag.undir_to_dir_edge(tail=tail, head=head)\n                break\n    return copy_pdag\n</code></pre>"},{"location":"api/#gresit.graphs.rule_3","title":"<code>rule_3(pdag)</code>","text":"<p>Apply 3rd Meek rule.</p> <p>Orient X - Z to X -&gt; Z, whenever there are two triples X - Y1 -&gt; Z and X - Y2 -&gt; Z such that Y1 and Y2 are non-adjacent.</p> <p>Parameters:</p> Name Type Description Default <code>pdag</code> <code>PDAG</code> <p>PDAG before application of rule.</p> required <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>PDAG after application of rule.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def rule_3(pdag: PDAG) -&gt; PDAG:\n    \"\"\"Apply 3rd Meek rule.\n\n    Orient X - Z to X -&gt; Z, whenever there are two triples\n    X - Y1 -&gt; Z and X - Y2 -&gt; Z such that Y1 and Y2 are non-adjacent.\n\n    Args:\n        pdag (PDAG): PDAG before application of rule.\n\n    Returns:\n        PDAG: PDAG after application of rule.\n    \"\"\"\n    copy_pdag = pdag.copy()\n    for edge in copy_pdag.undir_edges:\n        reverse_edge = edge[::-1]\n        test_edges = [edge, reverse_edge]\n        for tail, head in test_edges:\n            # if true that tail - node1 -&gt; head and tail - node2 -&gt; head\n            # while {node1 U node2} = 0 then orient tail -&gt; head\n            orient = False\n            num_neighbors = 2\n            if len(copy_pdag.undir_neighbors(tail)) &gt;= num_neighbors:\n                undir_n = copy_pdag.undir_neighbors(tail)\n                selection = [\n                    (node1, node2)\n                    for node1, node2 in combinations(undir_n, 2)\n                    if not copy_pdag.is_adjacent(node1, node2)\n                ]\n                if selection:\n                    for node1, node2 in selection:\n                        if head in copy_pdag.parents(node1).intersection(copy_pdag.parents(node2)):\n                            orient = True\n            if orient:\n                copy_pdag.undir_to_dir_edge(tail=tail, head=head)\n                break\n    return pdag\n</code></pre>"},{"location":"api/#gresit.graphs.rule_4","title":"<code>rule_4(pdag)</code>","text":"<p>Apply 4th Meek rule.</p> <p>Orient X - Y1 to X -&gt; Y1, whenever there are two triples with X - Z and X - Y1 &lt;- Z and X - Y2 -&gt; Z such that Y1 and Y2 are non-adjacent.</p> <p>Parameters:</p> Name Type Description Default <code>pdag</code> <code>PDAG</code> <p>PDAG before application of rule.</p> required <p>Returns:</p> Name Type Description <code>PDAG</code> <code>PDAG</code> <p>PDAG after application of rule.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def rule_4(pdag: PDAG) -&gt; PDAG:\n    \"\"\"Apply 4th Meek rule.\n\n    Orient X - Y1 to X -&gt; Y1, whenever there are\n    two triples with X - Z and X - Y1 &lt;- Z and X - Y2 -&gt; Z\n    such that Y1 and Y2 are non-adjacent.\n\n    Args:\n        pdag (PDAG): PDAG before application of rule.\n\n    Returns:\n        PDAG: PDAG after application of rule.\n    \"\"\"\n    copy_pdag = pdag.copy()\n    for edge in copy_pdag.undir_edges:\n        reverse_edge = edge[::-1]\n        test_edges = [edge, reverse_edge]\n        for tail, head in test_edges:\n            orient = False\n            if len(copy_pdag.undir_neighbors(tail)) &gt; 0:\n                undirected_n = copy_pdag.undir_neighbors(tail)\n                for undir_n in undirected_n:\n                    if tail in copy_pdag.children(undir_n):\n                        children_select = list(copy_pdag.children(undir_n))\n                        if children_select:\n                            for parent in children_select:\n                                if head in copy_pdag.children(parent):\n                                    orient = True\n            if orient:\n                copy_pdag.undir_to_dir_edge(tail=tail, head=head)\n                break\n    return pdag\n</code></pre>"},{"location":"api/#gresit.graphs.vstructs","title":"<code>vstructs(dag)</code>","text":"<p>Retrieve all v-structures in a DAG.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DiGraph</code> <p>DAG in question</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[tuple[str, str]]</code> <p>Set of all v-structures.</p> Source code in <code>gresit/graphs.py</code> <pre><code>def vstructs(dag: nx.DiGraph) -&gt; set[tuple[str, str]]:\n    \"\"\"Retrieve all v-structures in a DAG.\n\n    Args:\n        dag (nx.DiGraph): DAG in question\n\n    Returns:\n        set: Set of all v-structures.\n    \"\"\"\n    vstructures = set()\n    for node in dag.nodes():\n        for p1, p2 in combinations(list(dag.predecessors(node)), 2):  # get all parents of node\n            if not dag.has_edge(p1, p2) and not dag.has_edge(p2, p1):\n                vstructures.add((p1, node))\n                vstructures.add((p2, node))\n    return vstructures\n</code></pre>"},{"location":"data/","title":"Data and <code>gRESIT</code> algorithm","text":"<p>The data required by <code>gRESIT</code> should be structured in a way that each key in the <code>data_dict</code> corresponds to a group, and the value is a <code>numpy.ndarray</code> containing the samples for that group. We do not require the <code>numpy.ndarray</code>'s to be of the same shape, but they should contain the same number of samples.</p> <pre><code>import numpy as np\n\nrng = np.random.default_rng(42)  # Set seed for reproducibility\n\nX_1 = rng.multivariate_normal(mean=np.zeros(2), cov=np.eye(2), size=2000)\nX_2 = rng.multivariate_normal(mean=np.zeros(3), cov=np.eye(3), size=2000)\n\nX_3 = np.column_stack(\n    [X_1[:, 0] * X_2[:, 0] + X_1[:, 1] * X_2[:, 1], X_1[:, 1] * X_2[:, 2]]\n) + 0.1 * rng.multivariate_normal(mean=np.zeros(2), cov=np.eye(2), size=2000)\n\ndata_dict = {\n    \"X_1\": X_1,\n    \"X_2\": X_2,\n    \"X_3\": X_3,\n}\n</code></pre> Key Shape Dtype Example Values <code>X_1</code> (2000, 2) float64 <code>[[0.305, -1.04], [0.75, 0.941]]</code> <code>X_2</code> (2000, 3) float64 <code>[[0.253, 0.895, 0.273], [2.239, 1.43, -0.308]]</code> <code>X_3</code> (2000, 2) float64 <code>[[-0.836, -0.194], [2.878, -0.318]]</code> <p>Given this data, we can run the <code>gRESIT</code> algorithm as follows:</p> <pre><code>from gresit.group_resit import GroupResit\nfrom gresit.independence_tests import HSIC\nfrom gresit.torch_models import Multioutcome_MLP\n\ngresit = GroupResit(regressor=Multioutcome_MLP(), test=HSIC)\ngresit.learn_graph(data_dict)\ngresit.show_interactive()\n</code></pre> <p>Which produces the following interactive graph:</p> <p>In the section gRESIT you will find details on all arguments and hyperparameters for <code>gRESIT</code>.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This example shows how to generate synthetic data and learn a causal graph from group data alone using <code>gRESIT</code>.</p>"},{"location":"getting_started/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>We first generate synthetic data using an Erd\u0151s\u2013R\u00e9nyi random graph model. Each group of variables is defined with a specified size and edge density.</p> <pre><code>from gresit.synthetic_data import GenERData\n\ndata_gen = GenERData(\n    number_of_nodes=10,\n    group_size=2,\n    edge_density=0.2,\n)\n\ndata_dict, _ = data_gen.generate_data(num_samples=1000)\n</code></pre> <p>The output data_dict is a dictionary where each key corresponds to a group, and the values are the observed samples. See data for more details on the expected data format.</p>"},{"location":"getting_started/#fitting-a-graph-model","title":"Fitting a Graph Model","text":"<p>We now fit a group RESIT model using a <code>Multioutcome_MLP</code> as the regressor and <code>HSIC</code> as the independence test.</p> <pre><code>from gresit.group_resit import GroupResit\nfrom gresit.independence_tests import HSIC\nfrom gresit.torch_models import Multioutcome_MLP\n\nmodel = GroupResit(\n    regressor=Multioutcome_MLP(),\n    test=HSIC,\n    pruning_method=\"murgs\",\n)\nlearned_dag = model.learn_graph(data_dict=data_dict)\n\n# Show the learned graph:\nlearned_dag.show()\n# or show interactive mode:\nmodel.show_interactive()\n</code></pre>"},{"location":"getting_started/#accessing-the-learned-graph","title":"Accessing the Learned Graph","text":"<p>The learned adjacency matrix representing the estimated group-level graph and a causal ordering can be accessed via:</p> <pre><code>model.adjacency_matrix\nmodel.causal_ordering\n</code></pre>"},{"location":"gresit/","title":"gRESIT arguments and hyperparameters","text":"<pre><code>from gresit.group_resit import GroupResit\nfrom gresit.torch_models import Multioutcome_MLP\nfrom gresit.independence_tests import HSIC\n\nalg = GroupResit(\n    pruning_method='murgs',\n    test=HSIC,\n    regressor=Multioutcome_MLP(),\n)\ngraph = alg.learn_graph(data_dict=data_dict)\n</code></pre>"},{"location":"gresit/#pruning-method","title":"Pruning method","text":"<p>Here, the valid arguments for <code>pruning_method</code> are <code>murgs</code> and <code>independence</code>.</p> <p><code>murgs</code> is the default pruning method, which uses multiresponse group sparse additive models (MURGS) to prune away extraneous edges in the super-graph implied by the causal ordering. <code>independence</code> on the other hand uses independence tests as proposed by Peters et al. (2014) to prune the super-graph. Note that while <code>murgs</code> is order independent, <code>independence</code> strongly depends on the order of which the independence tests are applied.</p>"},{"location":"gresit/#murgs-arguments","title":"<code>murgs</code> arguments","text":"<ul> <li><code>local_regression_method</code>:  Type of local linear smoother to use. Options are currently <code>loess</code>, <code>kernel</code>. Defaults to <code>kernel</code>.</li> </ul>"},{"location":"gresit/#independence-arguments","title":"<code>independence</code> arguments","text":"<ul> <li><code>alpha</code>: The significance level for the independence tests. Default is <code>0.01</code>.</li> </ul>"},{"location":"gresit/#regressor","title":"Regressor","text":"<p>The <code>regressor</code> argument specifies the regression technique used in <code>gRESIT</code>. It should be an instance of the <code>MultiRegressor</code> base class. Currently, the following regression techniques are available:</p> <ul> <li> <p><code>Multioutcome_MLP</code>: A multi-output MLP regressor.</p> <ul> <li><code>rng</code>: Random number generator to control seed for data splitting. Defaults to<code>np.random.default_rng(seed=2024)</code>.</li> <li> <p><code>loss</code>: Standard <code>mse</code> loss is default. Other options are <code>hsic</code> and <code>disco</code>.</p> </li> <li> <p><code>dropout_proba</code>: Dropout probability. Defaults to 0.6.</p> </li> <li><code>n_epochs</code>: Number of times the data gets passed through the MLP. Defaults to 6.</li> <li><code>patience</code>: Minimal number of epochs to train before early stopping applies. Defaults to 60</li> <li><code>learning_rate</code>: Defaults to 1e-3.</li> <li><code>val_size</code>: Relative size of the validation dataset. Defaults to 0.2.</li> <li><code>batch_size</code>: Batch size for training. Defaults to 200.</li> <li><code>es</code>: Early stopping. Defaults to true.</li> </ul> </li> <li> <p>Simultaneous Linear Regression: A simultaneous linear regression model.</p> <ul> <li><code>alpha</code>: Regularization parameter for the ridge regression. Defaults to 0.1.</li> </ul> </li> <li> <p>Reduced Rank Regression: Kernel Reduced Rank Ridge Regression.</p> <ul> <li><code>alpha</code>: Regularization parameter for the ridge type regularization. Defaults to 1.0.</li> </ul> </li> <li> <p>CurdsWhey: Breiman and Friedman's curds and whey multivariate regression model.</p> </li> <li>BoostedRegressionTrees: A boosted regression tree model.</li> </ul>"},{"location":"gresit/#independence-tests","title":"Independence tests","text":"<p>The following independence tests are implemented in <code>gresit.independence_tests</code>:</p> <ul> <li><code>KernelCI</code><ul> <li>Kernel HSIC for conditional independence testing wrapper  around the <code>causal-learn</code> functionality.</li> </ul> </li> <li><code>FisherZVEC</code><ul> <li>Fisher Z test for conditional independence.</li> </ul> </li> <li><code>HSIC</code><ul> <li>Hilbert-Schmidt Independence Criterion for unconditional independence testing.</li> </ul> </li> <li><code>DISCO</code><ul> <li>Distance Correlation (DISCO) which equals zero if and only if the vectors  considered are unconditionally independent.</li> </ul> </li> </ul> <p>In <code>gRESIT</code>, we only require unconditional independence tests, so the <code>test</code> argument should be one of the following:</p> <ul> <li><code>HSIC</code></li> <li><code>DISCO</code></li> </ul>"},{"location":"gresit/#additional-arguments","title":"Additional arguments","text":"<ul> <li><code>test_size</code>: The relative size of the test set. If chosen to be larger than     zero, the regression model is trained on the training set and the subsequent     independence test is performed on the test set only. Defaults to 0.2.</li> </ul>"},{"location":"simulations/","title":"Simulations","text":"<p>All code necessary to reproduce the simulation results from our paper</p> <pre><code>@inproceedings{\n    goebler2025gresit,\n    title={Nonlinear Causal Discovery for Grouped Data},\n    author={Goebler, K., Windisch, T., Drton, M.},\n    booktitle={The 41st Conference on Uncertainty in Artificial Intelligence},\n    year={2025},\n}\n</code></pre> <p>is contained in the folder <code>simulation</code>.</p> <p>Essentially, executing <code>./run_simulations.sh</code> executes the benchmarks for different numbers of nodes, groups, and samples, sequentially. In our experiments, we use <code>slurm</code> to massively parallelize the individual runs.</p>"}]}